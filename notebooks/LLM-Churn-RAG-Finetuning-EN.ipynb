{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c7e9343",
   "metadata": {},
   "source": "# LLM Customer Churn Insight Tool\n## Customer Churn Insight Analysis System Based on LLM + RAG\n\n---\n\n# Introduction\n\n## 1.1 Project Background\n\nIn the telecom industry, **Customer Churn** is a critical business problem. Acquiring new customers typically costs 5-7 times more than retaining existing ones, making it commercially valuable to understand churn causes and take preventive measures.\n\nTraditional churn analysis mainly relies on **structured data** (such as tenure, charges, service types) for statistical analysis or machine learning modeling. However, this approach overlooks an important information source: **customer feedback text**. The dissatisfaction, complaints, and suggestions expressed in customer feedback often reveal the deeper causes of churn.\n\nThis project combines **Large Language Models (LLM)** and **Retrieval-Augmented Generation (RAG)** technology to build an intelligent analysis system that can understand customer feedback and answer business questions.\n\n## 1.2 Project Objectives\n\nBuild an **LLM-powered Customer Insight Tool** capable of:\n\n1. **Retrieving Relevant Information**: Retrieve the most relevant customer data from the feedback database based on user queries\n2. **Generating Root Cause Analysis**: Identify the main reasons for customer churn (Root Cause Analysis)\n3. **Assessing Risk Levels**: Evaluate the severity of issues (Churn Risk Assessment)\n4. **Providing Action Recommendations**: Offer specific, actionable improvement measures (Actionable Recommendations)\n5. **Supporting Citation Traceability**: All conclusions are backed by specific customer IDs as evidence (Citations)\n\n## 1.3 Technical Approach\n\nThis project adopts a **RAG (Retrieval-Augmented Generation)** architecture:\n\n```mermaid\n%%{init: {'theme': 'dark', 'themeVariables': { 'fontSize': '14px', 'fontFamily': 'arial', 'primaryColor': '#1a1a2e', 'primaryTextColor': '#fff', 'primaryBorderColor': '#4a9eff', 'lineColor': '#4a9eff'}}}%%\nflowchart LR\n    subgraph Input[\" \"]\n        A[/\"üîç User Query\"/]\n    end\n\n    subgraph Retrieval[\"Hybrid Search\"]\n        direction TB\n        B1[\"üìä Vector Search<br/>Semantic Similarity\"]\n        B2[\"üî§ BM25 Search<br/>Keyword Matching\"]\n    end\n\n    subgraph Process[\" \"]\n        C[(\"üìÑ Top-K<br/>Relevant Docs\")]\n        D[\"ü§ñ LLM Analysis<br/>Qwen2.5-7B\"]\n    end\n\n    subgraph Output[\" \"]\n        E[\"üìä Analysis Report<br/>Summary+Actions+Citations\"]\n    end\n\n    A --> B1 & B2\n    B1 & B2 --> C\n    C --> D\n    D --> E\n\n    style Input fill:#0f3460,stroke:#ffd700,stroke-width:2px\n    style Retrieval fill:#16213e,stroke:#00d9ff,stroke-width:2px\n    style Process fill:#1a1a2e,stroke:#00ff88,stroke-width:2px\n    style Output fill:#3d1a4a,stroke:#bf7fff,stroke-width:2px\n    style A fill:#0f3460,stroke:#ffd700,color:#fff\n    style B1 fill:#1e3a5f,stroke:#00ff88,color:#fff\n    style B2 fill:#1e3a5f,stroke:#00ff88,color:#fff\n    style C fill:#2d4a3e,stroke:#00ff88,color:#fff\n    style D fill:#4a1942,stroke:#ff6b9d,color:#fff\n    style E fill:#3d1a4a,stroke:#bf7fff,color:#fff\n```\n\n### Why RAG?\n\n| Approach | Pros | Cons |\n|------|------|------|\n| **Ask LLM Directly** | Simple and fast | Cannot access private data, may hallucinate |\n| **Fine-tuning** | Model internalizes domain knowledge | High cost, data updates require retraining |\n| **RAG** | Based on real data, traceable, easy to update | Requires building a retrieval system |\n\nRAG is currently the mainstream approach for enterprise LLM applications, especially suitable for scenarios that require **analysis based on private data**.\n\n---\n\n# 2. Data Introduction\n\n## 2.1 Data Source\n\nThis project uses the **Telco Customer Churn with Realistic Customer Feedback** dataset from Kaggle.\n\n- **Data Link**: [https://www.kaggle.com/datasets/beatafaron/telco-customer-churn-realistic-customer-feedback](https://www.kaggle.com/datasets/beatafaron/telco-customer-churn-realistic-customer-feedback)\n- **License**: CC BY 4.0 (Attribution 4.0 International)\n- **Data Scale**: 7,043 customer records\n\nThis dataset is based on the classic IBM Telco Customer Churn dataset, with **LLM-generated simulated feedback text for each customer**, making the dataset contain both structured data and unstructured text.\n\n## 2.2 Data Files Description\n\nThe downloaded data contains the following files:\n\n| File Name | Size | Rows | Description |\n|--------|------|------|------|\n| `telco_churn_with_all_feedback.csv` | 5.6 MB | 7,043 | **Main data file**, contains complete customer info and feedback |\n| `telco_prep.csv` | 5.8 MB | 7,032 | Preprocessed version, values converted to lowercase, added `feedback_length` and `sentiment` columns |\n| `telco_noisy_feedback_prep.csv` | 1.7 MB | 7,032 | Version with some feedback missing, for testing system robustness |\n| `model_with_feedback.pkl` | 27 KB | - | Pre-trained sentiment analysis model (pickle format) |\n\n**This project uses `telco_churn_with_all_feedback.csv` as the main data source** because it contains complete feedback for all customers.\n\n## 2.3 Variable Details\n\n### 2.3.1 Customer Identifier\n\n| Variable | Type | Description | Example |\n|--------|------|------|------|\n| `customerID` | String | Unique customer identifier | \"7590-VHVEG\" |\n\n### 2.3.2 Demographics\n\n| Variable | Type | Values | Description |\n|--------|------|------|------|\n| `gender` | String | Female, Male | Customer gender |\n| `SeniorCitizen` | Integer | 0, 1 | Whether the customer is a senior citizen (65+)<br>0 = No, 1 = Yes |\n| `Partner` | String | Yes, No | Whether the customer has a partner/spouse |\n| `Dependents` | String | Yes, No | Whether the customer has dependents (e.g., children) |\n\n### 2.3.3 Account Information\n\n| Variable | Type | Values/Range | Description |\n|--------|------|-----------|------|\n| `tenure` | Integer | 0 - 72 | Tenure in months<br>0 indicates a new account opened this month |\n| `Contract` | String | Month-to-month,<br>One year,<br>Two year | Contract type |\n| `PaperlessBilling` | String | Yes, No | Whether the customer uses paperless billing |\n| `PaymentMethod` | String | Electronic check,<br>Mailed check,<br>Bank transfer (automatic),<br>Credit card (automatic) | Payment method |\n\n### 2.3.4 Services\n\n| Variable | Type | Values | Description |\n|--------|------|------|------|\n| `PhoneService` | String | Yes, No | Whether the customer subscribes to phone service |\n| `MultipleLines` | String | Yes, No,<br>No phone service | Whether the customer has multiple phone lines |\n| `InternetService` | String | DSL,<br>Fiber optic,<br>No | Internet service type |\n| `OnlineSecurity` | String | Yes, No,<br>No internet service | Online security service |\n| `OnlineBackup` | String | Yes, No,<br>No internet service | Online backup service |\n| `DeviceProtection` | String | Yes, No,<br>No internet service | Device protection service |\n| `TechSupport` | String | Yes, No,<br>No internet service | Tech support service |\n| `StreamingTV` | String | Yes, No,<br>No internet service | Streaming TV service |\n| `StreamingMovies` | String | Yes, No,<br>No internet service | Streaming movies service |\n\n### 2.3.5 Charges\n\n| Variable | Type | Range | Description |\n|--------|------|------|------|\n| `MonthlyCharges` | Float | $18.25 - $118.75 | Monthly charges (USD) |\n| `TotalCharges` | String* | - | Total charges (USD)<br>*Note: Original data is string type, needs conversion |\n\n### 2.3.6 Target Variable\n\n| Variable | Type | Values | Description |\n|--------|------|------|------|\n| `Churn` | String | Yes, No | **Whether churned**<br>Yes = Churned (left)<br>No = Retained (active) |\n\n### 2.3.7 Text Data\n\n| Variable | Type | Description |\n|--------|------|------|\n| `PromptInput` | String | LLM prompt used to generate feedback (not used in this project) |\n| `CustomerFeedback` | String | **Customer feedback text** (core data for this project)<br>Length approximately 277-840 characters |\n\n## 2.4 Data Characteristics\n\n1. **High Completeness**: All 7,043 records have customer feedback text\n2. **Clear Labels**: Churn labels are clear, facilitating analysis\n3. **Information-Rich**: Combination of structured data + unstructured text\n4. **Class Imbalance**: Churn rate is approximately 26.5% (1,869 churned vs 5,174 retained)\n\n---\n\n# 3. Project Structure\n\n```\nLLM-Project/\n‚îú‚îÄ‚îÄ data/                                    # Data directory\n‚îÇ   ‚îú‚îÄ‚îÄ telco_churn_with_all_feedback.csv   # Main data file\n‚îÇ   ‚îú‚îÄ‚îÄ telco_prep.csv                      # Preprocessed data\n‚îÇ   ‚îú‚îÄ‚îÄ telco_noisy_feedback_prep.csv       # Noisy feedback data\n‚îÇ   ‚îú‚îÄ‚îÄ model_with_feedback.pkl             # Pre-trained model\n‚îÇ   ‚îú‚îÄ‚îÄ faiss_index.bin                     # FAISS vector index (generated after running)\n‚îÇ   ‚îî‚îÄ‚îÄ index_data.pkl                      # Index metadata (generated after running)\n‚îú‚îÄ‚îÄ venv/                                    # Python virtual environment\n‚îú‚îÄ‚îÄ LLM-Churn-RAG-Fintuning.ipynb           # Main notebook (this file)\n‚îú‚îÄ‚îÄ Using_tool_required_for_customer_service.ipynb  # Reference example code\n‚îî‚îÄ‚îÄ README.md                               # Project documentation\n```\n\n---\n\n# 4. Key Terminology\n\n| Term | Full Name | Explanation |\n|------|----------|------|\n| **RAG** | Retrieval-Augmented Generation | First retrieves relevant information from a knowledge base, then has the LLM generate answers based on that information, avoiding \"hallucinations\" |\n| **Embedding** | Vector Embedding | Converts text into high-dimensional vectors; semantically similar texts are closer in vector space |\n| **FAISS** | Facebook AI Similarity Search | Efficient vector similarity search library developed by Meta, supporting fast search over billions of vectors |\n| **BM25** | Best Matching 25 | Classic keyword retrieval algorithm that calculates relevance based on term frequency and document frequency |\n| **RRF** | Reciprocal Rank Fusion | Algorithm for merging multiple ranked result lists |\n| **Chunk** | Text Chunk | Splitting long documents into smaller segments for easier retrieval and processing |\n| **Cosine Similarity** | - | Measures the directional similarity between two vectors, range [-1, 1] |\n| **Prompt Engineering** | - | Designing effective prompts to guide LLM to produce desired outputs |\n| **Hallucination** | - | LLM generating information that appears plausible but is actually non-existent or incorrect |\n| **Citation** | - | Reference pointing to the specific data source supporting a conclusion |\n\n---\n\n# 5. Runtime Environment\n\n## 5.1 Requirements\n\n- Python 3.9+\n- Approximately 2GB disk space (for models and indexes)\n- Google Colab (free T4 GPU)\n\n## 5.2 Dependencies\n\n```\npandas>=2.0\nnumpy>=1.24\nsentence-transformers>=2.2\nfaiss-cpu>=1.7\nrank_bm25>=0.2\ntransformers>=4.36\npeft>=0.7\nbitsandbytes>=0.41\naccelerate>=0.25\n```\n\n## 5.3 Getting Started\n\n```bash\n# 1. Navigate to the project directory\ncd /Users/ricky/Desktop/LLM-Project\n\n# 2. Activate virtual environment\nsource venv/bin/activate\n\n# 3. Start Jupyter\njupyter notebook\n\n# 4. In Jupyter, select Kernel: \"LLM Project (venv)\"\n```\n\n---\n\n# 6. Project Phase Overview\n\n| Phase | Name | Content | Key Technologies |\n|-------|------|------|----------|\n| **Phase 1** | Data Preparation | Load, clean, build documents | Pandas, Data Preprocessing |\n| **Phase 2** | Indexing & Retrieval | Vectorize, build indexes, hybrid search | Sentence-Transformers, FAISS, BM25, RRF |\n| **Phase 3** | LLM Integration | Prompt design, RAG Pipeline | Qwen2.5-7B-Instruct, Prompt Engineering |\n| **Phase 4** | Output & Evaluation | Formatted output, citation validation | JSON Parsing, Quality Assessment |"
  },
  {
   "cell_type": "markdown",
   "id": "c6faede0",
   "metadata": {},
   "source": "---\n\n# Phase 1: Data Ingestion\n\nIn a RAG system, **data quality directly determines the quality of the final output**. The goals of Phase 1 are:\n\n1. Load raw data (structured + unstructured)\n2. Data exploration and quality checks\n3. Data cleaning and preprocessing\n4. Build document formats suitable for retrieval\n\n## 1.0 Environment Setup\n\nFirst, import the necessary Python libraries and configure the runtime environment."
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "538dfc31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÁéØÂ¢ÉÈÖçÁΩÆÂÆåÊàê ‚úì\n"
     ]
    }
   ],
   "source": "# ============================================\n# LLM Customer Churn Insight Tool\n# LLM + RAG + Customer Feedback Analysis\n# ============================================\n\n# Phase 1: Data Ingestion\n# Phase 2: Indexing & Retrieval\n# Phase 3: LLM Integration\n# Phase 4: Output & Evaluation\n\n# --------------------------------------------\n# 1.0 Environment Setup & Dependency Imports\n# --------------------------------------------\n\nimport os\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set display options\npd.set_option('display.max_columns', None)\npd.set_option('display.max_colwidth', 200)\n\nprint(\"Environment setup complete ‚úì\")"
  },
  {
   "cell_type": "markdown",
   "id": "c51f4a3c",
   "metadata": {},
   "source": "## 1.1 Load Data\n\nWe are using the **Telco Customer Churn** dataset, which contains:\n\n- **Structured Data**: Customer demographics, subscribed services, billing info, churn labels\n- **Unstructured Data**: Customer feedback text (CustomerFeedback)\n\nData source: [Kaggle - Telco Customer Churn with Realistic Feedback](https://www.kaggle.com/datasets/beatafaron/telco-customer-churn-realistic-customer-feedback)"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1007e44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Êï∞ÊçÆÈõÜÂ§ßÂ∞è: 7043 Ë°å, 23 Âàó\n",
      "\n",
      "ÂàóÂêç:\n",
      "['customerID', 'gender', 'SeniorCitizen', 'Partner', 'Dependents', 'tenure', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod', 'MonthlyCharges', 'TotalCharges', 'Churn', 'PromptInput', 'CustomerFeedback']\n"
     ]
    }
   ],
   "source": "# --------------------------------------------\n# 1.1 Load Data\n# --------------------------------------------\n\n# Data path\nDATA_DIR = './data'\n\n# Load main data file (contains customer info + feedback)\ndf_main = pd.read_csv(f'{DATA_DIR}/telco_churn_with_all_feedback.csv')\n\nprint(f\"Dataset size: {df_main.shape[0]} rows, {df_main.shape[1]} columns\")\nprint(f\"\\nColumns:\\n{df_main.columns.tolist()}\")"
  },
  {
   "cell_type": "markdown",
   "id": "550d874a",
   "metadata": {},
   "source": "### Output Analysis\n\nThe dataset contains **7,043 customer records** and **23 feature columns**:\n\n| Category | Fields |\n|------|------|\n| **Customer ID** | customerID |\n| **Demographics** | gender, SeniorCitizen, Partner, Dependents |\n| **Account Info** | tenure, Contract, PaperlessBilling, PaymentMethod |\n| **Service Subscriptions** | PhoneService, MultipleLines, InternetService, OnlineSecurity, OnlineBackup, DeviceProtection, TechSupport, StreamingTV, StreamingMovies |\n| **Charges** | MonthlyCharges, TotalCharges |\n| **Target Variable** | Churn |\n| **Text Data** | CustomerFeedback |\n\n---\n\n## 1.2 Data Exploration\n\nView the first few rows of data to understand the basic structure."
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8064d72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerID</th>\n",
       "      <th>gender</th>\n",
       "      <th>SeniorCitizen</th>\n",
       "      <th>Partner</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>tenure</th>\n",
       "      <th>PhoneService</th>\n",
       "      <th>MultipleLines</th>\n",
       "      <th>InternetService</th>\n",
       "      <th>OnlineSecurity</th>\n",
       "      <th>OnlineBackup</th>\n",
       "      <th>DeviceProtection</th>\n",
       "      <th>TechSupport</th>\n",
       "      <th>StreamingTV</th>\n",
       "      <th>StreamingMovies</th>\n",
       "      <th>Contract</th>\n",
       "      <th>PaperlessBilling</th>\n",
       "      <th>PaymentMethod</th>\n",
       "      <th>MonthlyCharges</th>\n",
       "      <th>TotalCharges</th>\n",
       "      <th>Churn</th>\n",
       "      <th>PromptInput</th>\n",
       "      <th>CustomerFeedback</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7590-VHVEG</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>No phone service</td>\n",
       "      <td>DSL</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Electronic check</td>\n",
       "      <td>29.85</td>\n",
       "      <td>29.85</td>\n",
       "      <td>No</td>\n",
       "      <td>Write a realistic customer feedback based on this profile:\\nChurn: No\\nTenure: 1 months\\nContract type: Month-to-month\\nMonthly Charges: $29.85\\nInternet Service: DSL\\nPayment Method: Electronic c...</td>\n",
       "      <td>I have been using the DSL internet service from this provider for the past month and so far, I am satisfied with the service. The connection has been reliable and the speed is sufficient for my ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5575-GNVDE</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>34</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>One year</td>\n",
       "      <td>No</td>\n",
       "      <td>Mailed check</td>\n",
       "      <td>56.95</td>\n",
       "      <td>1889.5</td>\n",
       "      <td>No</td>\n",
       "      <td>Write a realistic customer feedback based on this profile:\\nChurn: No\\nTenure: 34 months\\nContract type: One year\\nMonthly Charges: $56.95\\nInternet Service: DSL\\nPayment Method: Mailed check</td>\n",
       "      <td>I have been a customer with this company for over two and a half years now and I have been very satisfied with their service. The DSL internet has been reliable and the monthly charges are reasona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3668-QPYBK</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Mailed check</td>\n",
       "      <td>53.85</td>\n",
       "      <td>108.15</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Write a realistic customer feedback based on this profile:\\nChurn: Yes\\nTenure: 2 months\\nContract type: Month-to-month\\nMonthly Charges: $53.85\\nInternet Service: DSL\\nPayment Method: Mailed check</td>\n",
       "      <td>I recently signed up for DSL internet service with this provider two months ago on a month-to-month contract. Unfortunately, I have already decided to churn and switch to a different provider. The...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customerID  gender  SeniorCitizen Partner Dependents  tenure PhoneService  \\\n",
       "0  7590-VHVEG  Female              0     Yes         No       1           No   \n",
       "1  5575-GNVDE    Male              0      No         No      34          Yes   \n",
       "2  3668-QPYBK    Male              0      No         No       2          Yes   \n",
       "\n",
       "      MultipleLines InternetService OnlineSecurity OnlineBackup  \\\n",
       "0  No phone service             DSL             No          Yes   \n",
       "1                No             DSL            Yes           No   \n",
       "2                No             DSL            Yes          Yes   \n",
       "\n",
       "  DeviceProtection TechSupport StreamingTV StreamingMovies        Contract  \\\n",
       "0               No          No          No              No  Month-to-month   \n",
       "1              Yes          No          No              No        One year   \n",
       "2               No          No          No              No  Month-to-month   \n",
       "\n",
       "  PaperlessBilling     PaymentMethod  MonthlyCharges TotalCharges Churn  \\\n",
       "0              Yes  Electronic check           29.85        29.85    No   \n",
       "1               No      Mailed check           56.95       1889.5    No   \n",
       "2              Yes      Mailed check           53.85       108.15   Yes   \n",
       "\n",
       "                                                                                                                                                                                               PromptInput  \\\n",
       "0  Write a realistic customer feedback based on this profile:\\nChurn: No\\nTenure: 1 months\\nContract type: Month-to-month\\nMonthly Charges: $29.85\\nInternet Service: DSL\\nPayment Method: Electronic c...   \n",
       "1          Write a realistic customer feedback based on this profile:\\nChurn: No\\nTenure: 34 months\\nContract type: One year\\nMonthly Charges: $56.95\\nInternet Service: DSL\\nPayment Method: Mailed check   \n",
       "2    Write a realistic customer feedback based on this profile:\\nChurn: Yes\\nTenure: 2 months\\nContract type: Month-to-month\\nMonthly Charges: $53.85\\nInternet Service: DSL\\nPayment Method: Mailed check   \n",
       "\n",
       "                                                                                                                                                                                          CustomerFeedback  \n",
       "0  I have been using the DSL internet service from this provider for the past month and so far, I am satisfied with the service. The connection has been reliable and the speed is sufficient for my ne...  \n",
       "1  I have been a customer with this company for over two and a half years now and I have been very satisfied with their service. The DSL internet has been reliable and the monthly charges are reasona...  \n",
       "2  I recently signed up for DSL internet service with this provider two months ago on a month-to-month contract. Unfortunately, I have already decided to churn and switch to a different provider. The...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "# --------------------------------------------\n# 1.2 Data Exploration - Basic Info\n# --------------------------------------------\n\n# View first few rows\ndf_main.head(3)"
  },
  {
   "cell_type": "markdown",
   "id": "efd11e4c",
   "metadata": {},
   "source": "### Data Type Check\n\nCheck the data type and missing values for each column. This step is important because:\n- Numeric fields may be incorrectly identified as strings (e.g., TotalCharges)\n- Missing values need to be filled or removed in subsequent processing"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b804a87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Êï∞ÊçÆÁ±ªÂûã:\n",
      "==================================================\n",
      "customerID           object\n",
      "gender               object\n",
      "SeniorCitizen         int64\n",
      "Partner              object\n",
      "Dependents           object\n",
      "tenure                int64\n",
      "PhoneService         object\n",
      "MultipleLines        object\n",
      "InternetService      object\n",
      "OnlineSecurity       object\n",
      "OnlineBackup         object\n",
      "DeviceProtection     object\n",
      "TechSupport          object\n",
      "StreamingTV          object\n",
      "StreamingMovies      object\n",
      "Contract             object\n",
      "PaperlessBilling     object\n",
      "PaymentMethod        object\n",
      "MonthlyCharges      float64\n",
      "TotalCharges         object\n",
      "Churn                object\n",
      "PromptInput          object\n",
      "CustomerFeedback     object\n",
      "dtype: object\n",
      "\n",
      "==================================================\n",
      "Áº∫Â§±ÂÄºÁªüËÆ°:\n",
      "==================================================\n",
      "customerID          0\n",
      "gender              0\n",
      "SeniorCitizen       0\n",
      "Partner             0\n",
      "Dependents          0\n",
      "tenure              0\n",
      "PhoneService        0\n",
      "MultipleLines       0\n",
      "InternetService     0\n",
      "OnlineSecurity      0\n",
      "OnlineBackup        0\n",
      "DeviceProtection    0\n",
      "TechSupport         0\n",
      "StreamingTV         0\n",
      "StreamingMovies     0\n",
      "Contract            0\n",
      "PaperlessBilling    0\n",
      "PaymentMethod       0\n",
      "MonthlyCharges      0\n",
      "TotalCharges        0\n",
      "Churn               0\n",
      "PromptInput         0\n",
      "CustomerFeedback    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": "# Data type and missing value check\nprint(\"=\" * 50)\nprint(\"Data Types:\")\nprint(\"=\" * 50)\nprint(df_main.dtypes)\nprint(\"\\n\" + \"=\" * 50)\nprint(\"Missing Value Statistics:\")\nprint(\"=\" * 50)\nprint(df_main.isnull().sum())"
  },
  {
   "cell_type": "markdown",
   "id": "b9595768",
   "metadata": {},
   "source": "### Output Analysis\n\n**Issues Found**:\n- `TotalCharges` is `object` type instead of `float64`, indicating non-numeric characters (such as empty strings) that need conversion\n- There appear to be no missing values (`isnull().sum()` is all 0), but empty strings are not detected as NaN\n\n---\n\n## 1.3 Churn Analysis\n\nAnalyze the distribution of the target variable (Churn)."
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1fc7f5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÊµÅÂ§±ÁªüËÆ°:\n",
      "Churn\n",
      "No     5174\n",
      "Yes    1869\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ÊµÅÂ§±Áéá: 26.54%\n"
     ]
    }
   ],
   "source": "# --------------------------------------------\n# 1.3 Data Exploration - Churn Analysis\n# --------------------------------------------\n\n# Churn rate statistics\nchurn_counts = df_main['Churn'].value_counts()\nprint(\"Churn Statistics:\")\nprint(churn_counts)\nprint(f\"\\nChurn rate: {(df_main['Churn'] == 'Yes').mean() * 100:.2f}%\")"
  },
  {
   "cell_type": "markdown",
   "id": "5da58263",
   "metadata": {},
   "source": "### Output Analysis\n\n- **Churned Customers**: 1,869 (26.54%)\n- **Retained Customers**: 5,174 (73.46%)\n\nThis is a typical **Imbalanced Dataset**, with churned customers comprising about 1/4. In actual business, this churn rate is quite high and requires focused analysis of churn causes.\n\n---\n\n## 1.4 Customer Feedback Analysis\n\nView feedback text examples from churned and retained customers to understand text characteristics and differences."
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99abf347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÂÆ¢Êà∑ÂèçÈ¶àÁ§∫‰æã (ÊµÅÂ§±ÂÆ¢Êà∑):\n",
      "--------------------------------------------------\n",
      "I recently signed up for DSL internet service with this provider two months ago on a month-to-month contract. Unfortunately, I have already decided to churn and switch to a different provider. The monthly charges of $53.85 were reasonable, but I found the internet service to be unreliable and slow. Additionally, having to mail in a check for payment was inconvenient and outdated. I would not recommend this provider to others looking for reliable and convenient internet service.\n",
      "\n",
      "==================================================\n",
      "\n",
      "ÂÆ¢Êà∑ÂèçÈ¶àÁ§∫‰æã (ÈùûÊµÅÂ§±ÂÆ¢Êà∑):\n",
      "--------------------------------------------------\n",
      "I have been using the DSL internet service from this provider for the past month and so far, I am satisfied with the service. The connection has been reliable and the speed is sufficient for my needs. The monthly charges are reasonable at $29.85 and I appreciate the convenience of paying through electronic check. Overall, I have had a positive experience and would recommend this provider to others.\n"
     ]
    }
   ],
   "source": "# --------------------------------------------\n# 1.4 Data Exploration - Customer Feedback Analysis\n# --------------------------------------------\n\n# Check customer feedback column\nprint(\"Customer Feedback Example (Churned):\")\nprint(\"-\" * 50)\nchurned_feedback = df_main[df_main['Churn'] == 'Yes']['CustomerFeedback'].iloc[0]\nprint(churned_feedback[:500] if len(churned_feedback) > 500 else churned_feedback)\n\nprint(\"\\n\" + \"=\" * 50)\nprint(\"\\nCustomer Feedback Example (Retained):\")\nprint(\"-\" * 50)\nretained_feedback = df_main[df_main['Churn'] == 'No']['CustomerFeedback'].iloc[0]\nprint(retained_feedback[:500] if len(retained_feedback) > 500 else retained_feedback)"
  },
  {
   "cell_type": "markdown",
   "id": "dc291b2d",
   "metadata": {},
   "source": "### Output Analysis\n\nBy comparing feedback from both customer types, clear **sentiment differences** can be observed:\n\n| Customer Type | Feedback Characteristics |\n|----------|----------|\n| **Churned** | Uses negative words (unreliable, slow, inconvenient), clearly expresses dissatisfaction and intent to leave |\n| **Retained** | Uses positive words (satisfied, reliable, reasonable), expresses willingness to recommend |\n\nThese feedback texts will be the **core knowledge base** of the RAG system, and the LLM will analyze based on these real feedbacks.\n\n---\n\n## 1.5 Data Cleaning & Preprocessing\n\nPerform necessary data cleaning:\n1. Drop unnecessary columns (PromptInput is the prompt used to generate feedback, not needed)\n2. Convert data types (TotalCharges to numeric)\n3. Handle missing values\n4. Create numeric target variable (for easier analysis)"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e932550b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Â∑≤Âà†Èô§ PromptInput Âàó\n",
      "TotalCharges Áº∫Â§±ÂÄº: 11\n",
      "\n",
      "Ê∏ÖÊ¥óÂêéÊï∞ÊçÆÈõÜÂ§ßÂ∞è: (7043, 23)\n",
      "Êï∞ÊçÆÊ∏ÖÊ¥óÂÆåÊàê ‚úì\n"
     ]
    }
   ],
   "source": "# --------------------------------------------\n# 1.5 Data Cleaning & Preprocessing\n# --------------------------------------------\n\n# Create working copy\ndf = df_main.copy()\n\n# 1. Drop unnecessary columns (PromptInput was used to generate feedback, not needed)\nif 'PromptInput' in df.columns:\n    df = df.drop(columns=['PromptInput'])\n    print(\"Dropped PromptInput column\")\n\n# 2. Handle empty values in TotalCharges (convert to numeric)\ndf['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\nprint(f\"TotalCharges missing values: {df['TotalCharges'].isnull().sum()}\")\n\n# 3. Fill missing values (use MonthlyCharges since new customers may have empty TotalCharges)\ndf['TotalCharges'] = df['TotalCharges'].fillna(df['MonthlyCharges'])\n\n# 4. Convert Churn to numeric (for easier analysis)\ndf['Churn_Binary'] = (df['Churn'] == 'Yes').astype(int)\n\nprint(f\"\\nDataset size after cleaning: {df.shape}\")\nprint(\"Data cleaning complete ‚úì\")"
  },
  {
   "cell_type": "markdown",
   "id": "29457acf",
   "metadata": {},
   "source": "### Output Analysis\n\n- After successfully converting `TotalCharges` to numeric, **11 missing values** were found\n- These missing values correspond to new customers (tenure=0), filling with `MonthlyCharges` is reasonable\n- Created `Churn_Binary` column (0=Retained, 1=Churned) for numeric calculations\n\n## 1.6 Feedback Text Processing\n\nCheck the validity of feedback text to ensure all records have usable feedback content."
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "184546d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÊúâÊïàÂèçÈ¶àÊï∞Èáè: 7043 / 7043\n",
      "ÊúâÊïàÂèçÈ¶àÊØî‰æã: 100.00%\n",
      "\n",
      "ÂèçÈ¶àÈïøÂ∫¶ÁªüËÆ°:\n",
      "count    7043.000000\n",
      "mean      460.571205\n",
      "std        68.197917\n",
      "min       277.000000\n",
      "25%       414.000000\n",
      "50%       454.000000\n",
      "75%       499.000000\n",
      "max       840.000000\n",
      "Name: feedback_length, dtype: float64\n"
     ]
    }
   ],
   "source": "# --------------------------------------------\n# 1.6 Feedback Text Processing\n# --------------------------------------------\n\n# Check feedback text validity\ndf['feedback_length'] = df['CustomerFeedback'].fillna('').apply(len)\ndf['has_feedback'] = df['feedback_length'] > 10  # At least 10 characters counts as valid feedback\n\nprint(f\"Valid feedback count: {df['has_feedback'].sum()} / {len(df)}\")\nprint(f\"Valid feedback ratio: {df['has_feedback'].mean() * 100:.2f}%\")\n\n# Feedback length statistics\nprint(f\"\\nFeedback Length Statistics:\")\nprint(df[df['has_feedback']]['feedback_length'].describe())"
  },
  {
   "cell_type": "markdown",
   "id": "c9822ce7",
   "metadata": {},
   "source": "### Output Analysis\n\n- **100% of customers have valid feedback** (length > 10 characters)\n- Feedback length statistics: average 461 characters, range 277-840 characters\n- This means we can build documents for all 7,043 customers\n\n---\n\n## 1.7 Build RAG Documents\n\n**This is the most critical step in Phase 1**. We need to integrate structured data and unstructured feedback into a unified document format.\n\n### Why Build Documents?\n\nThe retrieval unit in a RAG system is a \"document\". A good document should:\n1. **Self-contained**: Contains sufficient context for the LLM to understand without additional information\n2. **Structured**: Information is clearly organized for easy extraction by the LLM\n3. **Identifiable**: Contains a unique identifier (Customer ID) for citation traceability"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ffbceecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Â∑≤ÂàõÂª∫ 7043 ‰∏™ÂÆ¢Êà∑ÊñáÊ°£\n",
      "\n",
      "Á§∫‰æãÊñáÊ°£:\n",
      "==================================================\n",
      "Customer ID: 7590-VHVEG\n",
      "Churn Status: No\n",
      "\n",
      "Customer Profile:\n",
      "- Gender: Female\n",
      "- Senior Citizen: No\n",
      "- Partner: Yes\n",
      "- Dependents: No\n",
      "- Tenure: 1 months\n",
      "\n",
      "Services:\n",
      "- Phone Service: No\n",
      "- Internet Service: DSL\n",
      "- Online Security: No\n",
      "- Tech Support: No\n",
      "- Streaming TV: No\n",
      "- Streaming Movies: No\n",
      "\n",
      "Contract & Billing:\n",
      "- Contract: Month-to-month\n",
      "- Monthly Charges: $29.85\n",
      "- Total Charges: $29.85\n",
      "- Payment Method: Electronic check\n",
      "\n",
      "Customer Feedback:\n",
      "I have been using the DSL internet service from this provider for the past month and so far, I am satisfied with the service. The connection has been reliable and the speed is sufficient for my needs. The monthly charges are reasonable at $29.85 and I appreciate the convenience of paying through electronic check. Overall, I have had a positive experience and\n"
     ]
    }
   ],
   "source": "# --------------------------------------------\n# 1.7 Prepare RAG Documents\n# --------------------------------------------\n\n# Create a comprehensive document for each customer (for RAG retrieval)\ndef create_customer_document(row):\n    \"\"\"\n    Combine customer structured data and feedback into a single document\n    \"\"\"\n    doc = f\"\"\"Customer ID: {row['customerID']}\nChurn Status: {row['Churn']}\n\nCustomer Profile:\n- Gender: {row['gender']}\n- Senior Citizen: {'Yes' if row['SeniorCitizen'] == 1 else 'No'}\n- Partner: {row['Partner']}\n- Dependents: {row['Dependents']}\n- Tenure: {row['tenure']} months\n\nServices:\n- Phone Service: {row['PhoneService']}\n- Internet Service: {row['InternetService']}\n- Online Security: {row['OnlineSecurity']}\n- Tech Support: {row['TechSupport']}\n- Streaming TV: {row['StreamingTV']}\n- Streaming Movies: {row['StreamingMovies']}\n\nContract & Billing:\n- Contract: {row['Contract']}\n- Monthly Charges: ${row['MonthlyCharges']}\n- Total Charges: ${row['TotalCharges']:.2f}\n- Payment Method: {row['PaymentMethod']}\n\nCustomer Feedback:\n{row['CustomerFeedback']}\n\"\"\"\n    return doc\n\n# Only create documents for customers with feedback\ndf_with_feedback = df[df['has_feedback']].copy()\ndf_with_feedback['document'] = df_with_feedback.apply(create_customer_document, axis=1)\n\nprint(f\"Created {len(df_with_feedback)} customer documents\")\nprint(\"\\nExample Document:\")\nprint(\"=\" * 50)\nprint(df_with_feedback['document'].iloc[0][:800])"
  },
  {
   "cell_type": "markdown",
   "id": "39798b18",
   "metadata": {},
   "source": "### Output Analysis\n\nSuccessfully created **7,043 customer documents**. Each document contains:\n- Customer ID and churn status (for citation)\n- Customer profile (demographics)\n- Service subscriptions\n- Charges and contract information\n- Original customer feedback\n\nThis format ensures the LLM can see both **quantitative data** (charges, tenure) and **qualitative data** (feedback text) during analysis.\n\n## Phase 1 Summary"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c8d1268a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Phase 1: Êï∞ÊçÆÂáÜÂ§áÂÆåÊàê\n",
      "==================================================\n",
      "ÊÄªÂÆ¢Êà∑Êï∞: 7043\n",
      "ÊúâÂèçÈ¶àÁöÑÂÆ¢Êà∑Êï∞: 7043\n",
      "ÊµÅÂ§±ÂÆ¢Êà∑Êï∞: 1869\n",
      "ÊµÅÂ§±Áéá: 26.54%\n",
      "\n",
      "ÊµÅÂ§±ÂÆ¢Êà∑‰∏≠ÊúâÂèçÈ¶àÁöÑÊØî‰æã: 26.54%\n"
     ]
    }
   ],
   "source": "# --------------------------------------------\n# Phase 1 Complete - Data Summary\n# --------------------------------------------\n\nprint(\"=\" * 50)\nprint(\"Phase 1: Data Preparation Complete\")\nprint(\"=\" * 50)\nprint(f\"Total customers: {len(df)}\")\nprint(f\"Customers with feedback: {len(df_with_feedback)}\")\nprint(f\"Churned customers: {df['Churn_Binary'].sum()}\")\nprint(f\"Churn rate: {df['Churn_Binary'].mean() * 100:.2f}%\")\nprint(f\"\\nFeedback ratio among churned customers: {df_with_feedback['Churn_Binary'].mean() * 100:.2f}%\")"
  },
  {
   "cell_type": "markdown",
   "id": "5afd217a",
   "metadata": {},
   "source": "### Phase 1 Complete\n\nKey achievements of the data preparation phase:\n- Loaded and cleaned 7,043 customer records\n- Churn rate is 26.54% (1,869 churned customers)\n- Built 7,043 structured documents, ready for indexing\n\n---\n\n# Phase 2: Indexing & Retrieval\n\nPhase 2 is the **core technical component** of the RAG system, aiming to achieve efficient and accurate document retrieval.\n\n## Why Is Retrieval Needed?\n\nThe LLM's context window is limited (e.g., Qwen2.5-7B has ~32K tokens) and cannot process all 7,043 documents at once. We need to:\n1. Quickly find the most relevant documents (e.g., Top 5) based on user queries\n2. Only feed these relevant documents to the LLM for analysis\n\n## Retrieval Strategy: Hybrid Retrieval\n\nWe adopt a **vector search + keyword search** hybrid strategy:\n\n| Method | Advantage | Disadvantage |\n|------|------|------|\n| **Vector Search** | Understands semantic similarity (e.g., \"cancel service\" ~ \"unsubscribe\") | May miss exact keyword matches |\n| **BM25 Keyword Search** | Exact keyword matching, high interpretability | Cannot understand synonyms and semantics |\n| **Hybrid Search** | Combines advantages of both | Requires designing a fusion strategy |\n\n## 2.0 Load Dependencies\n\nImport libraries needed for Phase 2:\n- `sentence-transformers`: Generate text vector embeddings\n- `faiss`: Efficient vector similarity search\n- `rank_bm25`: BM25 keyword retrieval algorithm"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f3379f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 2 ‰æùËµñÂä†ËΩΩÂÆåÊàê ‚úì\n"
     ]
    }
   ],
   "source": "# ============================================\n# Phase 2: Indexing & Retrieval\n# ============================================\n\n# Install required dependencies (if not already installed)\n# !pip install sentence-transformers faiss-cpu rank_bm25 -q\n\nfrom sentence_transformers import SentenceTransformer\nimport faiss\nfrom rank_bm25 import BM25Okapi\nimport re\n\nprint(\"Phase 2 dependencies loaded ‚úì\")"
  },
  {
   "cell_type": "markdown",
   "id": "d8e6b3bd",
   "metadata": {},
   "source": "## 2.1 Text Preprocessing\n\nBefore creating vector embeddings, the text needs to be standardized:\n- Merge extra whitespace characters\n- Trim leading and trailing spaces\n\nThis step ensures consistent text formatting, avoiding meaningless differences that affect retrieval performance."
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ffc8ece4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "È¢ÑÂ§ÑÁêÜÂÆåÊàê: 7043 ‰∏™ÊñáÊ°£\n",
      "Âπ≥ÂùáÊñáÊ°£ÈïøÂ∫¶: 918 Â≠óÁ¨¶\n"
     ]
    }
   ],
   "source": "# --------------------------------------------\n# 2.1 Text Preprocessing\n# --------------------------------------------\n\ndef preprocess_text(text):\n    \"\"\"\n    Clean text: remove extra whitespace\n    \"\"\"\n    text = re.sub(r'\\s+', ' ', text)  # Merge extra whitespace\n    text = text.strip()\n    return text\n\n# Preprocess all documents\ndocuments = df_with_feedback['document'].apply(preprocess_text).tolist()\ncustomer_ids = df_with_feedback['customerID'].tolist()\n\nprint(f\"Preprocessing complete: {len(documents)} documents\")\nprint(f\"Average document length: {np.mean([len(d) for d in documents]):.0f} characters\")"
  },
  {
   "cell_type": "markdown",
   "id": "1e0c8dff",
   "metadata": {},
   "source": "## 2.2 Create Vector Embeddings\n\n### What Are Vector Embeddings?\n\nVector embeddings are a technique for mapping text to high-dimensional vector space. In this space:\n- **Semantically similar texts** are closer together\n- **Semantically different texts** are farther apart\n\nFor example:\n- \"I want to cancel my service\" and \"I'm leaving this company\" would have very close vectors\n- \"I love this service\" and \"I want to cancel\" would have distant vectors\n\n### Embedding Model Selection\n\nWe use the **BGE (BAAI General Embedding)** model:\n- Model: `BAAI/bge-base-en-v1.5`\n- Dimensions: 768\n- Features: Excellent performance on multiple retrieval benchmarks, fully open-source and free\n\n### Mathematical Principle\n\nGiven a text $t$, the embedding model $f$ maps it to a vector:\n\n$$\\vec{v} = f(t) \\in \\mathbb{R}^{768}$$\n\nThe similarity between two texts is calculated using **cosine similarity**:\n\n$$\\text{similarity}(t_1, t_2) = \\cos(\\vec{v_1}, \\vec{v_2}) = \\frac{\\vec{v_1} \\cdot \\vec{v_2}}{||\\vec{v_1}|| \\cdot ||\\vec{v_2}||}$$\n\nWhen vectors are normalized ($||\\vec{v}|| = 1$), cosine similarity equals the dot product."
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c481015d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Âä†ËΩΩÂµåÂÖ•Ê®°Âûã...\n",
      "ÁîüÊàêÊñáÊ°£ÂµåÂÖ• (ÂèØËÉΩÈúÄË¶ÅÂá†ÂàÜÈíü)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 221/221 [01:47<00:00,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ÂµåÂÖ•Áª¥Â∫¶: (7043, 768)\n",
      "ÂêëÈáèÂµåÂÖ•ÂàõÂª∫ÂÆåÊàê ‚úì\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": "# --------------------------------------------\n# 2.2 Create Vector Embeddings\n# --------------------------------------------\n\n# Load embedding model (using BGE model, high performance and free)\nprint(\"Loading embedding model...\")\nembedding_model = SentenceTransformer('BAAI/bge-base-en-v1.5')\n\n# Generate document embeddings\nprint(\"Generating document embeddings (may take a few minutes)...\")\ndocument_embeddings = embedding_model.encode(\n    documents, \n    show_progress_bar=True,\n    normalize_embeddings=True  # Normalize for cosine similarity\n)\n\nprint(f\"\\nEmbedding dimensions: {document_embeddings.shape}\")\nprint(\"Vector embeddings created ‚úì\")"
  },
  {
   "cell_type": "markdown",
   "id": "6f4e9092",
   "metadata": {},
   "source": "### Output Analysis\n\n- Successfully generated vector embeddings for **7,043 documents**\n- Each vector has **768** dimensions\n- Used `normalize_embeddings=True` for L2 normalization, allowing inner product to replace cosine similarity\n\n## 2.3 Build Vector Store (FAISS Index)\n\n### What Is FAISS?\n\n**FAISS (Facebook AI Similarity Search)** is an efficient vector similarity search library developed by Meta, capable of:\n- Completing similarity search over millions of vectors in milliseconds\n- Supporting GPU acceleration\n- Providing multiple index types (exact search, approximate search)\n\n### Index Type Selection\n\nWe use `IndexFlatIP` (Flat Index with Inner Product):\n- **Flat**: Exact search, no approximation\n- **IP**: Inner Product, since vectors are normalized, equivalent to cosine similarity\n\nFor 7,043 documents, exact search performance is already sufficient. For million-scale documents, consider using approximate indexes like `IndexIVFFlat`."
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d6d8acac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS Á¥¢ÂºïÂ∑≤ÂàõÂª∫\n",
      "Á¥¢Âºï‰∏≠ÁöÑÂêëÈáèÊï∞: 7043\n"
     ]
    }
   ],
   "source": "# --------------------------------------------\n# 2.3 Build Vector Store (FAISS)\n# --------------------------------------------\n\n# Create FAISS index\ndimension = document_embeddings.shape[1]\nfaiss_index = faiss.IndexFlatIP(dimension)  # Inner product (equivalent to cosine similarity since vectors are normalized)\nfaiss_index.add(document_embeddings.astype('float32'))\n\nprint(f\"FAISS index created\")\nprint(f\"Vectors in index: {faiss_index.ntotal}\")"
  },
  {
   "cell_type": "markdown",
   "id": "24f878c4",
   "metadata": {},
   "source": "## 2.4 Build BM25 Index\n\n### What Is BM25?\n\n**BM25 (Best Matching 25)** is a classic keyword retrieval algorithm widely used in search engines. It is based on a bag-of-words model and considers:\n- **Term Frequency (TF)**: The number of times a term appears in a document\n- **Inverse Document Frequency (IDF)**: How rare a term is across the entire corpus\n- **Document Length Normalization**: Prevents unfair advantage for longer documents\n\n### BM25 Formula\n\nFor query $Q$ and document $D$, the BM25 score is:\n\n$$\\text{BM25}(D, Q) = \\sum_{i=1}^{n} \\text{IDF}(q_i) \\cdot \\frac{f(q_i, D) \\cdot (k_1 + 1)}{f(q_i, D) + k_1 \\cdot (1 - b + b \\cdot \\frac{|D|}{\\text{avgdl}})}$$\n\nWhere:\n- $f(q_i, D)$: Term frequency of $q_i$ in document $D$\n- $|D|$: Document length\n- $\\text{avgdl}$: Average document length\n- $k_1$, $b$: Tuning parameters (typically $k_1 = 1.5$, $b = 0.75$)\n- $\\text{IDF}(q_i)$: Inverse document frequency of term $q_i$\n\n### Advantages of BM25\n\n- Excellent for **exact keyword matching**\n- Computationally efficient, no GPU required\n- Results are interpretable (can identify which terms matched)"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f560494b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 Á¥¢ÂºïÂ∑≤ÂàõÂª∫\n",
      "ËØçÊ±áË°®Â§ßÂ∞è: 17770\n"
     ]
    }
   ],
   "source": "# --------------------------------------------\n# 2.4 Build BM25 Index (Keyword Search)\n# --------------------------------------------\n\n# Tokenization\ntokenized_docs = [doc.lower().split() for doc in documents]\n\n# Create BM25 index\nbm25_index = BM25Okapi(tokenized_docs)\n\nprint(f\"BM25 index created\")\nprint(f\"Vocabulary size: {len(bm25_index.idf)}\")"
  },
  {
   "cell_type": "markdown",
   "id": "92d73985",
   "metadata": {},
   "source": "### Output Analysis\n\n- BM25 index contains **17,770 unique terms**\n- Tokenization uses simple whitespace splitting (suitable for English); Chinese would require tools like jieba\n\n## 2.5 Implement Hybrid Search\n\n### Why Hybrid?\n\nEach retrieval method has limitations:\n\n| Query Example | Vector Search | BM25 |\n|----------|----------|------|\n| \"customers unhappy with service\" | Can understand \"unhappy\" semantics | May miss \"dissatisfied\" documents |\n| \"DSL internet\" | May return other types of network services | Precisely matches \"DSL\" |\n\nHybrid search combines both advantages, improving **recall** and **precision**.\n\n### Fusion Algorithm: RRF (Reciprocal Rank Fusion)\n\n**RRF** is a simple and effective rank fusion algorithm with the formula:\n\n$$\\text{RRF}(d) = \\sum_{r \\in R} \\frac{1}{k + r(d)}$$\n\nWhere:\n- $R$: All retrieval result lists (vector search and BM25 in this project)\n- $r(d)$: The rank of document $d$ in a result list (starting from 1)\n- $k$: Smoothing constant (typically 60)\n\n**RRF Characteristics**:\n- No need to normalize scores from different retrieval methods\n- Gives higher weight to top-ranked documents\n- Parameter $k$ controls the impact of rank differences\n\n### Weight Parameter Œ±\n\nFor document d, its RRF fusion score is defined as:\n\n$$\n\\mathrm{RRF}(d)\n=\n\\alpha \\cdot \\frac{1}{k + r_{\\text{faiss}}(d)}\n+\n(1 - \\alpha) \\cdot \\frac{1}{k + r_{\\text{bm25}}(d)}\n$$\n\nWhere:\n\n* $d$: Candidate document\n* $r_{\\text{faiss}}(d)$: Rank of document $d$ in vector search (FAISS) results (starting from 1)\n* $r_{\\text{bm25}}(d)$: Rank of document $d$ in BM25 keyword search results (starting from 1)\n* $k$: RRF smoothing constant, used to reduce the impact of rank differences, typically $k = 60$\n* $\\alpha \\in [0, 1]$: Weight parameter controlling the relative importance of vector search vs BM25 search\n\n  * $\\alpha = 0.5$: Equal weight for both methods\n  * $\\alpha > 0.5$: More emphasis on vector search\n  * $\\alpha < 0.5$: More emphasis on BM25 search\n"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f2ba6f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ê∑∑ÂêàÊ£ÄÁ¥¢ÂáΩÊï∞ÂÆö‰πâÂÆåÊàê ‚úì\n"
     ]
    }
   ],
   "source": "# --------------------------------------------\n# 2.5 Implement Hybrid Search\n# --------------------------------------------\n\ndef vector_search(query, k=10):\n    \"\"\"\n    Vector search\n    \"\"\"\n    query_embedding = embedding_model.encode([query], normalize_embeddings=True)\n    scores, indices = faiss_index.search(query_embedding.astype('float32'), k)\n    return list(zip(indices[0], scores[0]))\n\ndef bm25_search(query, k=10):\n    \"\"\"\n    BM25 keyword search\n    \"\"\"\n    tokenized_query = query.lower().split()\n    scores = bm25_index.get_scores(tokenized_query)\n    top_indices = np.argsort(scores)[::-1][:k]\n    return [(idx, scores[idx]) for idx in top_indices]\n\ndef hybrid_search(query, k=10, alpha=0.5):\n    \"\"\"\n    Hybrid search (RRF - Reciprocal Rank Fusion)\n    \n    Args:\n        query: Query text\n        k: Number of results to return\n        alpha: Vector search weight (1-alpha is the BM25 weight)\n    \"\"\"\n    # Get results from both search methods\n    vector_results = vector_search(query, k=k*2)\n    bm25_results = bm25_search(query, k=k*2)\n    \n    # RRF fusion\n    rrf_scores = {}\n    rrf_k = 60  # RRF constant\n    \n    for rank, (idx, _) in enumerate(vector_results):\n        rrf_scores[idx] = rrf_scores.get(idx, 0) + alpha / (rrf_k + rank + 1)\n    \n    for rank, (idx, _) in enumerate(bm25_results):\n        rrf_scores[idx] = rrf_scores.get(idx, 0) + (1 - alpha) / (rrf_k + rank + 1)\n    \n    # Sort and return\n    sorted_results = sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)[:k]\n    \n    return sorted_results\n\nprint(\"Hybrid search functions defined ‚úì\")"
  },
  {
   "cell_type": "markdown",
   "id": "7ec0d975",
   "metadata": {},
   "source": "## 2.6 Test Retrieval\n\nUse a sample query to test the hybrid search system and verify the relevance of retrieval results."
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f8b13f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÊµãËØïÊü•ËØ¢: customers who are unhappy with the internet service and want to cancel\n",
      "==================================================\n",
      "\n",
      "Ê£ÄÁ¥¢ÁªìÊûú:\n",
      "\n",
      "--- ÁªìÊûú 1 (Score: 0.0082) ---\n",
      "Customer ID: 6598-RFFVI\n",
      "Churn: Yes\n",
      "Feedback: I am extremely dissatisfied with my experience with this service provider. Despite being promised internet service, I have not received any connection in the two months since signing up. The fact that I am locked into a one-year contract with monthly charges of $19.3 is unacceptable. The automatic credit card payment method adds insult to injury, as I am essentially paying for a service I am not receiving. I will be looking to cancel my contract and find a more reliable provider as soon as possible.\n",
      "\n",
      "--- ÁªìÊûú 2 (Score: 0.0082) ---\n",
      "Customer ID: 7321-ZNSLA\n",
      "Churn: No\n",
      "Feedback: I have been a customer of this DSL internet service provider for the past 13 months and I am pleased to say that I have not experienced any issues that would make me want to churn. The monthly charges of $40.55 are reasonable and the service provided has been reliable. \n",
      "\n",
      "I appreciate the option to pay by mailed check as it is convenient for me. Overall, I am satisfied with the service and would recommend it to others who are looking for a reliable internet service provider.\n",
      "\n",
      "--- ÁªìÊûú 3 (Score: 0.0081) ---\n",
      "Customer ID: 4871-JTKJF\n",
      "Churn: Yes\n",
      "Feedback: I decided to cancel my service with this company after only 1 month of being a customer. The monthly charges of $69.65 for fiber optic internet were reasonable, but the service itself was not up to par. I experienced frequent outages and slow speeds, which was frustrating. Additionally, the process of cancelling my service was not smooth and took longer than expected. I will be looking for a more reliable internet provider in the future.\n"
     ]
    }
   ],
   "source": "# --------------------------------------------\n# 2.6 Test Retrieval\n# --------------------------------------------\n\n# Test query\ntest_query = \"customers who are unhappy with the internet service and want to cancel\"\n\nprint(f\"Test query: {test_query}\")\nprint(\"=\" * 50)\n\n# Execute hybrid search\nresults = hybrid_search(test_query, k=3)\n\nprint(\"\\nSearch Results:\")\nfor rank, (idx, score) in enumerate(results, 1):\n    print(f\"\\n--- Result {rank} (Score: {score:.4f}) ---\")\n    print(f\"Customer ID: {customer_ids[idx]}\")\n    print(f\"Churn: {df_with_feedback.iloc[idx]['Churn']}\")\n    # Show first 200 characters of feedback\n    feedback = df_with_feedback.iloc[idx]['CustomerFeedback']\n    print(f\"Feedback: {feedback}\")"
  },
  {
   "cell_type": "markdown",
   "id": "8adc4993",
   "metadata": {},
   "source": "### Output Analysis\n\nThe retrieval test results show the system is working properly:\n\n| Rank | Customer ID | Churn Status | Analysis |\n|------|---------|----------|------|\n| 1 | 6598-RFFVI | Yes | Retrieved a churned customer dissatisfied with internet service |\n| 2 | 7321-ZNSLA | No | Provides a contrasting perspective (satisfied customer) |\n| 3 | 4871-JTKJF | Yes | Retrieved another customer who churned due to service quality |\n\nThe retrieval system successfully found customer feedback relevant to the query, including both churned and retained customers, which is valuable for comprehensive analysis.\n\n---\n\n# Phase 3: LLM Integration\n\nPhase 3 connects the retrieval system with the large language model to build a complete **RAG Pipeline**.\n\n## RAG Workflow\n\n```mermaid\n%%{init: {'theme': 'dark', 'themeVariables': { 'fontSize': '14px'}}}%%\nflowchart LR\n    A[\"üîç User Query\"] --> B[\"‚ö° Hybrid Search\"]\n    B --> C[\"üìÑ Top-K Docs\"]\n    C --> D[\"üìù Build Prompt\"]\n    D --> E[\"ü§ñ Qwen2.5-7B\"]\n    E --> F[\"üìä Structured Output\"]\n\n    D -.-> G[\"System Prompt<br/>+ Context<br/>+ Query\"]\n\n    style A fill:#0f3460,stroke:#ffd700,color:#fff\n    style B fill:#1e3a5f,stroke:#00ff88,color:#fff\n    style C fill:#2d4a3e,stroke:#00ff88,color:#fff\n    style D fill:#16213e,stroke:#00d9ff,color:#fff\n    style E fill:#4a1942,stroke:#ff6b9d,color:#fff\n    style F fill:#3d1a4a,stroke:#bf7fff,color:#fff\n    style G fill:#1a1a2e,stroke:#888,color:#aaa\n```\n\n## Why RAG Instead of Asking the LLM Directly?\n\n| Method | Pros | Cons |\n|------|------|------|\n| **Ask LLM Directly** | Simple | May hallucinate, cannot cite specific data |\n| **RAG** | Based on real data, traceable citations | Requires building a retrieval system |\n\nRAG makes LLM answers **verifiable**, which is crucial in business analysis scenarios.\n\n## 3.0 Load Open-Source LLM Model\n\nLoad Qwen2.5-7B-Instruct (4-bit quantized) as the generation end of the RAG Pipeline. Using BitsAndBytes 4-bit quantization, VRAM usage is approximately 4.5 GB, fully supported by T4 GPU.\n\n> **Warning: Starting from Phase 3, code needs to run on Google Colab (T4 GPU)**"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "24f4a578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI ÂÆ¢Êà∑Á´ØÂàùÂßãÂåñÂÆåÊàê ‚úì\n"
     ]
    }
   ],
   "source": "# ============================================\n# Phase 3: LLM Integration\n# ‚ö†Ô∏è Run on Google Colab (T4 GPU)\n# ============================================\n\n# Install fine-tuning dependencies (needed for Phase 3-8)\n# !pip install -q transformers peft bitsandbytes trl datasets accelerate\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nimport torch\nimport json\n\n# 4-bit quantization config (NF4 + double quantization, maximize compression)\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",             # NF4 quantization: information-theoretically optimal 4-bit data type\n    bnb_4bit_compute_dtype=torch.bfloat16,  # Use bfloat16 precision for computation\n    bnb_4bit_use_double_quant=True,         # Double quantization: further compress quantization parameters\n)\n\nMODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n\nprint(f\"Loading model: {MODEL_NAME} (4-bit quantization)...\")\nprint(\"Estimated time: 2-3 minutes...\")\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\nmodel.eval()\n\nprint(f\"\\nModel loaded ‚úì\")\nprint(f\"GPU memory usage: {torch.cuda.memory_allocated() / 1024**3:.1f} GB\")\nprint(f\"GPU total memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
  },
  {
   "cell_type": "markdown",
   "id": "25001d4c",
   "metadata": {},
   "source": "## 3.1 Define Prompt Templates\n\n### Prompt Engineering\n\nPrompt design is a key factor affecting LLM output quality. A good prompt should:\n\n1. **Define the Role**: Tell the LLM what expert it is\n2. **Define the Task**: Clearly describe the work to be completed\n3. **Specify the Format**: Require structured output (e.g., JSON)\n4. **Provide Examples**: Show the expected output format\n5. **Set Constraints**: Clearly define limitations and requirements\n\n### Our Prompt Structure\n\n```mermaid\n%%{init: {'theme': 'dark', 'themeVariables': { 'fontSize': '14px'}}}%%\nflowchart TB\n    subgraph SP[\"üîß System Prompt\"]\n        direction LR\n        S1[\"üë§ Role Definition<br/>Churn Analyst\"]\n        S2[\"üìã Output Format<br/>JSON Schema\"]\n        S3[\"üìè Analysis Guide<br/>Constraints\"]\n    end\n\n    PLUS((\"+\"))\n\n    subgraph UP[\"üí¨ User Prompt\"]\n        direction LR\n        U1[\"‚ùì User Query\"]\n        U2[\"üìä Retrieved<br/>Customer Data\"]\n    end\n\n    SP --> PLUS --> UP\n\n    style SP fill:#1a1a2e,stroke:#4a9eff,stroke-width:2px\n    style UP fill:#16213e,stroke:#00d9ff,stroke-width:2px\n    style PLUS fill:#ffd700,stroke:#ffd700,color:#000\n    style S1 fill:#0f3460,stroke:#ffd700,color:#fff\n    style S2 fill:#0f3460,stroke:#ffd700,color:#fff\n    style S3 fill:#0f3460,stroke:#ffd700,color:#fff\n    style U1 fill:#1e3a5f,stroke:#00ff88,color:#fff\n    style U2 fill:#1e3a5f,stroke:#00ff88,color:#fff\n```\n\n### Importance of Structured Output\n\nWe require the LLM to return JSON format so that:\n- It can be easily parsed programmatically for downstream processing\n- The output contains all necessary fields\n- Automated validation is possible (e.g., checking if citations are valid)"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e7491476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Ê®°ÊùøÂÆö‰πâÂÆåÊàê ‚úì\n"
     ]
    }
   ],
   "source": "# --------------------------------------------\n# 3.1 Define Prompt Templates\n# --------------------------------------------\n\nSYSTEM_PROMPT = \"\"\"You are a customer churn analysis expert. Based on the provided customer feedback and profile data, analyze the root causes of churn, assess risk levels, and provide actionable recommendations.\n\nYou must respond in the following JSON format:\n{\n    \"summary\": \"Brief overall summary of the analysis (2-3 sentences)\",\n    \"top_reasons\": [\"reason 1\", \"reason 2\", \"reason 3\"],\n    \"risk_level\": \"high/medium/low\",\n    \"actions\": [\"recommended action 1\", \"recommended action 2\", \"recommended action 3\"],\n    \"citations\": [\"7590-VHVEG\", \"5575-GNVDE\"]\n}\n\nGuidelines:\n- Base your analysis ONLY on the provided customer data\n- IMPORTANT: In citations, use ONLY the exact Customer ID format shown in the data (e.g., \"7590-VHVEG\"), NOT \"customerID_xxx\"\n- Risk level should be based on the proportion of churned customers and severity of issues\n- Actions should be specific and actionable\n- Always respond in valid JSON format\"\"\"\n\nUSER_TEMPLATE = \"\"\"Query: {query}\n\nRelevant Customer Data:\n{context}\n\nPlease analyze the above customer feedback and provide insights in the specified JSON format.\"\"\"\n\nprint(\"Prompt templates defined ‚úì\")"
  },
  {
   "cell_type": "markdown",
   "id": "4793c940",
   "metadata": {},
   "source": "## 3.2 Implement RAG Pipeline\n\nThe RAG Pipeline is the core of the entire system, chaining the retrieval and generation steps together.\n\n### Pipeline Steps\n\n> Uses local Qwen2.5-7B-Instruct model, no API Key required, completely free\n\n1. **Retrieve**: Use hybrid search to find Top-K relevant documents\n2. **Build Context**: Format retrieved documents into LLM-readable text\n3. **Generate**: Call the LLM API, passing in System Prompt, context, and user query\n4. **Parse**: Parse the JSON response returned by the LLM\n\n### Model Selection\n\nWe use **Qwen2.5-7B-Instruct** (4-bit quantized):\n- Completely free, no API Key required\n- Strong JSON structured output capability\n- 128K context window, sufficient for processing 5 customer documents\n\n### Temperature Parameter\n\n`temperature=0.7` controls the randomness of output:\n- `0`: Fully deterministic, same output every time\n- `1`: More creative, but may be unstable\n- `0.7`: Balance between accuracy and diversity"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "530cf2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Pipeline ÂÆö‰πâÂÆåÊàê ‚úì\n"
     ]
    }
   ],
   "source": "# --------------------------------------------\n# 3.2 Implement RAG Pipeline\n# ‚ö†Ô∏è Run on Google Colab (using local open-source model)\n# --------------------------------------------\n\ndef build_context(search_results, max_docs=5):\n    \"\"\"\n    Build LLM context\n    \"\"\"\n    context_parts = []\n    for idx, score in search_results[:max_docs]:\n        doc = documents[idx]\n        context_parts.append(f\"--- Document (Relevance: {score:.4f}) ---\\n{doc}\\n\")\n    return \"\\n\".join(context_parts)\n\ndef rag_query(user_query, k=5, max_new_tokens=1024):\n    \"\"\"\n    RAG Query Pipeline (using local Qwen2.5-7B-Instruct)\n\n    Args:\n        user_query: User query\n        k: Number of documents to retrieve\n        max_new_tokens: Maximum tokens to generate\n\n    Returns:\n        dict: Parsed JSON result\n    \"\"\"\n    # Step 1: Retrieve relevant documents\n    search_results = hybrid_search(user_query, k=k)\n\n    # Step 2: Build context\n    context = build_context(search_results, max_docs=k)\n\n    # Step 3: Build ChatML prompt and generate with local model\n    messages = [\n        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n        {\"role\": \"user\", \"content\": USER_TEMPLATE.format(\n            query=user_query,\n            context=context\n        )},\n    ]\n\n    text = tokenizer.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )\n    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            temperature=0.7,\n            top_p=0.9,\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id,\n        )\n\n    # Decode only the newly generated part\n    generated_ids = outputs[0][inputs['input_ids'].shape[1]:]\n    response_text = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n\n    # Step 4: JSON parsing + regex fallback (open-source models may add text around JSON)\n    try:\n        result = json.loads(response_text)\n    except json.JSONDecodeError:\n        import re as _re\n        json_match = _re.search(r'\\{[\\s\\S]*\\}', response_text)\n        if json_match:\n            try:\n                result = json.loads(json_match.group())\n            except json.JSONDecodeError:\n                result = {\"error\": \"Failed to parse JSON\", \"raw_response\": response_text}\n        else:\n            result = {\"error\": \"Failed to parse JSON\", \"raw_response\": response_text}\n\n    # Add retrieval metadata\n    result[\"retrieved_docs\"] = len(search_results)\n    result[\"retrieved_customer_ids\"] = [customer_ids[idx] for idx, _ in search_results]\n\n    return result\n\nprint(\"RAG Pipeline defined ‚úì\")\nprint(\"  Using model: Qwen2.5-7B-Instruct (local, 4-bit)\")"
  },
  {
   "cell_type": "markdown",
   "id": "0063add9",
   "metadata": {},
   "source": "## 3.3 Test RAG Pipeline\n\nUse actual queries to test the complete RAG system and verify the end-to-end workflow."
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "69bdc5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÊµãËØï RAG Pipeline\n",
      "==================================================\n",
      "Query: What are the main reasons customers are leaving due to internet service issues?\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mQuery: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_queries[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m50\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m result = \u001b[43mrag_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_queries\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mÂàÜÊûêÁªìÊûú:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(json.dumps(result, indent=\u001b[32m2\u001b[39m, ensure_ascii=\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mrag_query\u001b[39m\u001b[34m(user_query, k, model)\u001b[39m\n\u001b[32m     31\u001b[39m context = build_context(search_results, max_docs=k)\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Step 3: Ë∞ÉÁî® LLM\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mSYSTEM_PROMPT\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mUSER_TEMPLATE\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m            \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mjson_object\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.7\u001b[39;49m\n\u001b[32m     45\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Step 4: Ëß£ÊûêÁªìÊûú\u001b[39;00m\n\u001b[32m     48\u001b[39m result_text = response.choices[\u001b[32m0\u001b[39m].message.content\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM-Project/venv/lib/python3.12/site-packages/openai/_utils/_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM-Project/venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:1192\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1145\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1146\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1147\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1189\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   1190\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1191\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1201\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1203\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1204\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1205\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1206\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1207\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1208\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1209\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1210\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1211\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1212\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1213\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_retention\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_retention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1214\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1215\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1216\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1217\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1218\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1219\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1220\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1221\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1225\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1227\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1229\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1230\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1231\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1232\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1237\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1238\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1242\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM-Project/venv/lib/python3.12/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM-Project/venv/lib/python3.12/site-packages/openai/_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": "# --------------------------------------------\n# 3.3 Test RAG Pipeline\n# --------------------------------------------\n\n# Test query\ntest_queries = [\n    \"What are the main reasons customers are leaving due to internet service issues?\",\n    \"Why are customers on month-to-month contracts churning?\",\n    \"What do customers say about pricing and value for money?\"\n]\n\n# Execute first test query\nprint(\"Testing RAG Pipeline\")\nprint(\"=\" * 50)\nprint(f\"Query: {test_queries[0]}\")\nprint(\"=\" * 50)\n\nresult = rag_query(test_queries[0], k=5)\n\nprint(\"\\nAnalysis Results:\")\nprint(json.dumps(result, indent=2, ensure_ascii=False))"
  },
  {
   "cell_type": "markdown",
   "id": "524d07a8",
   "metadata": {},
   "source": "### Output Analysis\n\nThe RAG Pipeline successfully returned structured analysis results:\n\n| Field | Content | Description |\n|------|------|------|\n| **summary** | Overall analysis summary | Summary generated by LLM based on retrieved documents |\n| **top_reasons** | List of churn reasons | Key issues extracted from customer feedback |\n| **risk_level** | high | Risk level determined based on data |\n| **actions** | Recommended actions | Actionable improvement measures |\n| **citations** | List of customer IDs | Data sources supporting the conclusions |\n\n**Key Observations**:\n- The LLM accurately extracted core reasons such as \"unreliable internet service\", \"high prices\", and \"customer service issues\" from the feedback\n- All cited customer IDs come from the retrieval results, indicating the LLM followed the \"analyze based on data\" instruction\n- The recommended actions are actionable\n\n---\n\n# Phase 4: Output & Evaluation\n\nPhase 4 focuses on how to **present results** and **verify quality**.\n\n## Importance of Evaluation\n\nRAG system outputs need verification:\n1. **Format Correctness**: Whether JSON is parseable\n2. **Citation Accuracy**: Whether customer IDs cited by the LLM come from retrieval results\n3. **Content Relevance**: Whether the analysis answers the user's question\n\n## 4.1 Formatted Output\n\nConvert JSON results into a human-readable report format."
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ec1851b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìä CUSTOMER CHURN ANALYSIS REPORT\n",
      "============================================================\n",
      "\n",
      "üìù Summary:\n",
      "   The analysis of customer feedback reveals that the primary reasons for churn are related to inadequate internet service availability, high pricing, and poor customer service experiences. Customers express dissatisfaction with billing issues and payment methods, which further exacerbate their frustrations leading to their decision to leave.\n",
      "\n",
      "üîç Top Reasons for Churn:\n",
      "   1. Lack of internet service availability in certain areas\n",
      "   2. High pricing for internet services\n",
      "   3. Poor customer service and billing issues\n",
      "\n",
      "‚ö†Ô∏è Risk Level: HIGH\n",
      "\n",
      "üí° Recommended Actions:\n",
      "   1. Expand internet service availability in underserved areas to attract and retain customers.\n",
      "   2. Review and adjust pricing strategies to ensure competitiveness and perceived value.\n",
      "   3. Enhance customer service training and streamline billing processes to improve customer satisfaction.\n",
      "\n",
      "üìé Citations (Customer IDs):\n",
      "   4877-EVATK, 1871-MOWRM, 5687-DKDTV, 6892-EZDTG, 8065-YKXKD\n",
      "\n",
      "üìö Retrieved Documents: 5\n",
      "============================================================\n"
     ]
    }
   ],
   "source": "# ============================================\n# Phase 4: Output & Evaluation\n# ============================================\n\n# --------------------------------------------\n# 4.1 Format Output Function\n# --------------------------------------------\n\ndef display_analysis(result):\n    \"\"\"\n    Display analysis results in formatted style\n    \"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(\"üìä CUSTOMER CHURN ANALYSIS REPORT\")\n    print(\"=\" * 60)\n    \n    if \"error\" in result:\n        print(f\"‚ùå Error: {result['error']}\")\n        return\n    \n    print(f\"\\nüìù Summary:\")\n    print(f\"   {result.get('summary', 'N/A')}\")\n    \n    print(f\"\\nüîç Top Reasons for Churn:\")\n    for i, reason in enumerate(result.get('top_reasons', []), 1):\n        print(f\"   {i}. {reason}\")\n    \n    print(f\"\\n‚ö†Ô∏è Risk Level: {result.get('risk_level', 'N/A').upper()}\")\n    \n    print(f\"\\nüí° Recommended Actions:\")\n    for i, action in enumerate(result.get('actions', []), 1):\n        print(f\"   {i}. {action}\")\n    \n    print(f\"\\nüìé Citations (Customer IDs):\")\n    print(f\"   {', '.join(result.get('citations', []))}\")\n    \n    print(f\"\\nüìö Retrieved Documents: {result.get('retrieved_docs', 0)}\")\n    print(\"=\" * 60)\n\n# Test formatted output\ndisplay_analysis(result)"
  },
  {
   "cell_type": "markdown",
   "id": "d6863d3d",
   "metadata": {},
   "source": "## 4.2 Citation Validation\n\n### What Is Citation Validation?\n\nCitation validation checks whether the customer IDs cited by the LLM **actually exist in the retrieval results**.\n\nThis is an important measure to prevent **LLM Hallucination**. Hallucination refers to the LLM generating information that appears plausible but actually does not exist.\n\n### Validation Logic\n\n```mermaid\n%%{init: {'theme': 'dark', 'themeVariables': { 'fontSize': '13px'}}}%%\nflowchart LR\n    A[\"LLM Output<br/>citations\"] --> B{\"Validation\"}\n    C[\"Retrieved<br/>customer_ids\"] --> B\n    B --> D[\"Calculate<br/>Accuracy\"]\n\n    style A fill:#4a1942,stroke:#ff6b9d,color:#fff\n    style B fill:#1a1a2e,stroke:#ffd700,color:#fff\n    style C fill:#2d4a3e,stroke:#00ff88,color:#fff\n    style D fill:#3d1a4a,stroke:#bf7fff,color:#fff\n```\n\n### Why Is ID Normalization Needed?\n\nThe LLM may return IDs in different formats:\n- `7590-VHVEG` (correct)\n- `customerID_7590-VHVEG` (added prefix)\n- `7590-vhveg` (different case)\n\nOur `normalize_customer_id` function handles these variants to ensure validation robustness."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79623258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÂºïÁî®È™åËØÅÁªìÊûú:\n",
      "  ÊÄªÂºïÁî®Êï∞: 5\n",
      "  ÊúâÊïàÂºïÁî®: ['4877-EVATK', '1871-MOWRM', '5687-DKDTV', '6892-EZDTG', '8065-YKXKD']\n",
      "  Êó†ÊïàÂºïÁî®: []\n",
      "  ÂºïÁî®ÂáÜÁ°ÆÁéá: 100.0%\n"
     ]
    }
   ],
   "source": "# --------------------------------------------\n# 4.2 Citation Validation\n# --------------------------------------------\n\ndef normalize_customer_id(cid):\n    \"\"\"\n    Normalize customer ID, handle various formats\n    \"\"\"\n    cid = cid.strip().lower()\n    # Remove possible prefixes\n    prefixes = ['customerid_', 'customer_id_', 'cid_', 'id_']\n    for prefix in prefixes:\n        if cid.startswith(prefix):\n            cid = cid[len(prefix):]\n    return cid\n\ndef validate_citations(result):\n    \"\"\"\n    Validate whether cited customer IDs are in retrieved results\n    \"\"\"\n    citations = result.get('citations', [])\n    retrieved_ids = result.get('retrieved_customer_ids', [])\n    \n    # Normalize all IDs\n    retrieved_normalized = {normalize_customer_id(rid): rid for rid in retrieved_ids}\n    \n    valid_citations = []\n    invalid_citations = []\n    \n    for cid in citations:\n        normalized = normalize_customer_id(cid)\n        if normalized in retrieved_normalized:\n            valid_citations.append(cid)\n        else:\n            invalid_citations.append(cid)\n    \n    accuracy = len(valid_citations) / len(citations) if citations else 0\n    \n    return {\n        \"total_citations\": len(citations),\n        \"valid_citations\": valid_citations,\n        \"invalid_citations\": invalid_citations,\n        \"accuracy\": accuracy\n    }\n\n# Validation result\nvalidation = validate_citations(result)\nprint(\"Citation Validation Results:\")\nprint(f\"  Total citations: {validation['total_citations']}\")\nprint(f\"  Valid citations: {validation['valid_citations']}\")\nprint(f\"  Invalid citations: {validation['invalid_citations']}\")\nprint(f\"  Citation accuracy: {validation['accuracy']*100:.1f}%\")"
  },
  {
   "cell_type": "markdown",
   "id": "41cef1aa",
   "metadata": {},
   "source": "### Output Analysis\n\nCitation validation results show **100% accuracy**, all citations are valid. This indicates:\n1. Prompt design is effective, the LLM followed the \"only cite provided data\" instruction\n2. The system did not produce hallucinations\n\n**If accuracy is below 100%**, you may need to:\n- Adjust the Prompt to emphasize citation constraints\n- Check retrieval quality to ensure relevant documents are retrieved\n- Consider using a larger open-source model (e.g., Qwen2.5-14B)\n\n## 4.3 Interactive Analysis Interface\n\nEncapsulate the complete analysis workflow and provide a concise calling interface."
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "80c118fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üéâ Customer Churn Insight Tool Ready!\n",
      "============================================================\n",
      "\n",
      "‰ΩøÁî®ÊñπÊ≥ï: result = analyze_churn('your query here')\n",
      "\n",
      "Á§∫‰æãÊü•ËØ¢:\n",
      "  1. 'Why are customers with fiber optic internet churning?'\n",
      "  2. 'What issues do senior citizens have with our service?'\n",
      "  3. 'What are the common complaints about customer support?'\n"
     ]
    }
   ],
   "source": "# --------------------------------------------\n# 4.3 Interactive Analysis Interface\n# --------------------------------------------\n\ndef analyze_churn(query):\n    \"\"\"\n    Complete churn analysis function\n    \n    Args:\n        query: Analysis query\n    \n    Returns:\n        Analysis result\n    \"\"\"\n    print(f\"üîé Analyzing: {query}\")\n    print(\"-\" * 50)\n    \n    # Execute RAG query\n    result = rag_query(query, k=5)\n    \n    # Display results\n    display_analysis(result)\n    \n    # Validate citations\n    validation = validate_citations(result)\n    print(f\"\\n‚úÖ Citation Accuracy: {validation['accuracy']*100:.1f}%\")\n    \n    return result\n\n# Example queries\nprint(\"=\" * 60)\nprint(\"üéâ Customer Churn Insight Tool Ready!\")\nprint(\"=\" * 60)\nprint(\"\\nUsage: result = analyze_churn('your query here')\")\nprint(\"\\nExample queries:\")\nprint(\"  1. 'Why are customers with fiber optic internet churning?'\")\nprint(\"  2. 'What issues do senior citizens have with our service?'\")\nprint(\"  3. 'What are the common complaints about customer support?'\")"
  },
  {
   "cell_type": "markdown",
   "id": "75cd9bfa",
   "metadata": {},
   "source": "## 4.4 Full Example\n\nRun a complete analysis query to demonstrate the system's end-to-end capability."
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "543422ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Analyzing: Why are customers with fiber optic internet churning?\n",
      "--------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "üìä CUSTOMER CHURN ANALYSIS REPORT\n",
      "============================================================\n",
      "\n",
      "üìù Summary:\n",
      "   The analysis indicates that customers with fiber optic internet are primarily churning due to service disruptions and inconsistent speeds. While some customers appreciate the initial pricing, the overall dissatisfaction with reliability and customer support significantly contributes to their decision to leave.\n",
      "\n",
      "üîç Top Reasons for Churn:\n",
      "   1. Frequent service disruptions\n",
      "   2. Inconsistent internet speeds\n",
      "   3. Poor customer support\n",
      "\n",
      "‚ö†Ô∏è Risk Level: HIGH\n",
      "\n",
      "üí° Recommended Actions:\n",
      "   1. Enhance service reliability through infrastructure improvements\n",
      "   2. Implement a proactive customer support system with quicker response times\n",
      "   3. Gather customer feedback regularly to address service issues promptly\n",
      "\n",
      "üìé Citations (Customer IDs):\n",
      "   6680-WKXRZ, 8065-YKXKD\n",
      "\n",
      "üìö Retrieved Documents: 5\n",
      "============================================================\n",
      "\n",
      "‚úÖ Citation Accuracy: 100.0%\n"
     ]
    }
   ],
   "source": "# --------------------------------------------\n# Example: Run full analysis\n# --------------------------------------------\n\n# Uncomment to run analysis\nresult = analyze_churn(\"Why are customers with fiber optic internet churning?\")"
  },
  {
   "cell_type": "markdown",
   "id": "2db1b5fa",
   "metadata": {},
   "source": "### Output Analysis\n\nThe complete analysis workflow executed successfully:\n1. Retrieved relevant customer data\n2. LLM generated a structured analysis report\n3. Citation validation passed (100% accuracy)\n\nThe analysis revealed the main reasons for fiber optic customer churn: frequent service interruptions, unstable speeds, and insufficient customer support.\n\n---\n\n# Conclusion\n\n## Technology Stack Review\n\n| Component | Technology | Purpose |\n|------|------|------|\n| Data Processing | Pandas | Load, clean, transform data |\n| Text Embedding | Sentence-Transformers (BGE) | Convert text to 768-dimensional vectors |\n| Vector Search | FAISS | Efficient similarity search |\n| Keyword Search | BM25 | Exact keyword matching |\n| Result Fusion | RRF | Merge hybrid search rankings |\n| Text Generation | Qwen2.5-7B-Instruct (4-bit) | Analysis and reasoning |\n| Output Format | JSON | Structured, parseable output |\n\n## System Features\n\n1. **Traceability**: All analytical conclusions are supported by customer ID citations\n2. **Accuracy**: Hybrid search + citation validation ensures data accuracy\n3. **Scalability**: Easily scalable to larger datasets (using approximate indexes)\n4. **Interpretability**: Structured output, easy to understand and use\n\n## Future Improvements\n\n1. **Retrieval Optimization**:\n   - Add a Reranker (e.g., cross-encoder) to further improve precision\n   - Try different embedding models (e.g., bge-large-en-v1.5)\n\n2. **LLM Optimization**:\n   - Use QLoRA fine-tuning to improve output quality (see Phase 5-8)\n   - Implement multi-turn conversation, support follow-up questions\n\n3. **Evaluation Framework**:\n   - Build annotated datasets for quantitative evaluation\n   - Implement A/B testing framework\n\n4. **Production Deployment**:\n   - Persist vector indexes (save/load FAISS indexes)\n   - Add API interface (FastAPI/Flask)\n   - Implement caching mechanism\n\n## Usage Examples\n\n```python\n# Analyze customer churn causes\nresult = analyze_churn(\"Why are customers with fiber optic internet churning?\")\n\n# Analyze specific groups\nresult = analyze_churn(\"What issues do senior citizens have with our service?\")\n\n# Analyze service issues\nresult = analyze_churn(\"What are the common complaints about customer support?\")\n\n# Analyze price sensitivity\nresult = analyze_churn(\"Are customers leaving because of high prices?\")\n```"
  },
  {
   "cell_type": "markdown",
   "id": "df9c19b9",
   "metadata": {},
   "source": "## Phase 3-4 Model Cleanup\n\nBefore starting Phase 5 (loading the 14B teacher model), we need to release the GPU memory used by the 7B model from Phase 3-4.\n\n- 7B model uses ~4.5 GB VRAM\n- 14B teacher model needs ~8 GB VRAM\n- T4 has 15 GB total, need to release 7B before loading 14B"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a829cc",
   "metadata": {},
   "outputs": [],
   "source": "# --------------------------------------------\n# Release GPU memory from Phase 3-4 model\n# (Free up space for Phase 5's 14B teacher model)\n# ‚ö†Ô∏è Run on Google Colab\n# --------------------------------------------\n\nimport gc\n\nprint(f\"GPU memory before release: {torch.cuda.memory_allocated() / 1024**3:.1f} GB\")\n\n# Release the 7B model loaded in Phase 3\nif 'model' in dir() and model is not None:\n    del model\nif 'tokenizer' in dir() and tokenizer is not None:\n    del tokenizer\n\ngc.collect()\ntorch.cuda.empty_cache()\n\nprint(f\"GPU memory after release: {torch.cuda.memory_allocated() / 1024**3:.1f} GB\")\nprint(\"\\n7B model released, ready to load 14B teacher model ‚úì\")"
  },
  {
   "cell_type": "markdown",
   "id": "bf5f684d",
   "metadata": {},
   "source": "---\n\n# Phase 5: Training Data Generation\n\n> **Warning: Starting from Phase 5, all code runs on Google Colab (free T4 GPU)**\n\n## Objective\n\nUse **Qwen2.5-14B-Instruct** (4-bit quantized) as the teacher model to generate high-quality JSON-formatted training data for each query, which will be used to fine-tune the 7B student model.\n\n## Strategy\n\n| Step | Description |\n|------|------|\n| 5.0 | Colab environment setup + rebuild retrieval system |\n| 5.1 | Design 8 query template categories, generate ~60-80 unique queries |\n| 5.2 | Use 14B teacher model to generate Gold Standard training data |\n| 5.3 | Data augmentation (k=3,5,7 multi-round generation) |\n| 5.4 | Train/validation split (85%/15%) |\n| 5.5 | Export ChatML format JSONL |\n| 5.6 | Release teacher model memory |\n\n### VRAM Estimate\n\n- BGE embedding model: ~0.5 GB\n- Qwen2.5-14B-Instruct (4-bit): ~8 GB\n- **Total: ~8.5 GB - Safe for T4 (15 GB limit)**"
  },
  {
   "cell_type": "markdown",
   "id": "5866c38a",
   "metadata": {},
   "source": "## 5.0 Colab Environment Setup\n\nInstall all dependencies on Colab and rebuild the retrieval system. Since Colab is a fresh environment, data needs to be reloaded and indexes rebuilt.\n\n**New dependencies installed**:\n- `peft`: LoRA adapter framework\n- `bitsandbytes`: 4-bit / 8-bit quantization support\n- `trl`: Transformer Reinforcement Learning, provides `SFTTrainer`\n- `datasets`: Hugging Face dataset loading\n- `accelerate`: Distributed training and mixed precision support"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0765ab7",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# Phase 5: Training Data Generation\n# ‚ö†Ô∏è Run on Google Colab (T4 GPU)\n# ============================================\n\n# --------------------------------------------\n# 5.0 Colab Environment Setup\n# --------------------------------------------\n\n# Install dependencies\n!pip install -q transformers peft bitsandbytes trl datasets accelerate\n!pip install -q sentence-transformers faiss-cpu rank_bm25\n\n# Mount Google Drive (for saving model adapter later)\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nimport os\nimport torch\n\n# Check GPU availability\nprint(f\"GPU available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU model: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n\nprint(\"\\nEnvironment setup complete ‚úì\")"
  },
  {
   "cell_type": "markdown",
   "id": "b69d7c0c",
   "metadata": {},
   "source": "### Upload Data and Rebuild Retrieval System\n\nReuse Phase 1-2 logic: Load CSV -> Create customer documents -> Build FAISS + BM25 indexes.\n\n**Note**: Please upload `telco_churn_with_all_feedback.csv` to Colab first, or place it in Google Drive."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f6afef",
   "metadata": {},
   "outputs": [],
   "source": "# --------------------------------------------\n# 5.0b Load Data + Rebuild Retrieval System\n# [Optional] Skip if Phase 1-2 variables are still in memory\n# (Reuse Phase 1-2 logic)\n# ‚ö†Ô∏è Run on Google Colab\n# --------------------------------------------\n\nimport pandas as pd\nimport numpy as np\nimport re\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# === Load Data ===\n# Method 1: Upload from Colab\n# from google.colab import files\n# uploaded = files.upload()\n\n# Method 2: Read from Google Drive (recommended)\nDATA_PATH = '/content/drive/MyDrive/telco_churn_with_all_feedback.csv'\n\n# If not in Drive, try current directory\nif not os.path.exists(DATA_PATH):\n    DATA_PATH = '/content/telco_churn_with_all_feedback.csv'\n    if not os.path.exists(DATA_PATH):\n        from google.colab import files\n        print(\"Please upload telco_churn_with_all_feedback.csv:\")\n        uploaded = files.upload()\n        DATA_PATH = list(uploaded.keys())[0]\n\ndf_main = pd.read_csv(DATA_PATH)\nprint(f\"Data loaded: {df_main.shape}\")\n\n# === Data Preprocessing (same as Phase 1) ===\ndf = df_main.copy()\nif 'PromptInput' in df.columns:\n    df = df.drop(columns=['PromptInput'])\ndf['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\ndf['TotalCharges'] = df['TotalCharges'].fillna(df['MonthlyCharges'])\ndf['Churn_Binary'] = (df['Churn'] == 'Yes').astype(int)\n\n# Check feedback field\ndf['has_feedback'] = df['CustomerFeedback'].notna() & (df['CustomerFeedback'].str.len() > 10)\n\n# === Create Customer Documents (same as Phase 1.7) ===\ndef create_customer_document(row):\n    doc = f\"\"\"Customer ID: {row['customerID']}\nChurn Status: {row['Churn']}\n\nCustomer Profile:\n- Gender: {row['gender']}\n- Senior Citizen: {'Yes' if row['SeniorCitizen'] == 1 else 'No'}\n- Partner: {row['Partner']}\n- Dependents: {row['Dependents']}\n- Tenure: {row['tenure']} months\n\nServices:\n- Phone Service: {row['PhoneService']}\n- Internet Service: {row['InternetService']}\n- Online Security: {row['OnlineSecurity']}\n- Tech Support: {row['TechSupport']}\n- Streaming TV: {row['StreamingTV']}\n- Streaming Movies: {row['StreamingMovies']}\n\nContract & Billing:\n- Contract: {row['Contract']}\n- Monthly Charges: ${row['MonthlyCharges']}\n- Total Charges: ${row['TotalCharges']:.2f}\n- Payment Method: {row['PaymentMethod']}\n\nCustomer Feedback:\n{row['CustomerFeedback']}\n\"\"\"\n    return doc\n\ndf_with_feedback = df[df['has_feedback']].copy()\ndf_with_feedback['document'] = df_with_feedback.apply(create_customer_document, axis=1)\n\ndef preprocess_text(text):\n    text = re.sub(r'\\s+', ' ', text)\n    text = text.strip()\n    return text\n\ndocuments = df_with_feedback['document'].apply(preprocess_text).tolist()\ncustomer_ids = df_with_feedback['customerID'].tolist()\n\n# === Build Retrieval System (same as Phase 2) ===\nfrom sentence_transformers import SentenceTransformer\nimport faiss\nfrom rank_bm25 import BM25Okapi\n\nprint(\"Loading BGE embedding model...\")\nembedding_model = SentenceTransformer('BAAI/bge-base-en-v1.5')\n\nprint(\"Generating document embeddings...\")\ndocument_embeddings = embedding_model.encode(\n    documents, show_progress_bar=True, normalize_embeddings=True\n)\n\n# FAISS index\ndimension = document_embeddings.shape[1]\nfaiss_index = faiss.IndexFlatIP(dimension)\nfaiss_index.add(document_embeddings.astype('float32'))\n\n# BM25 index\ntokenized_docs = [doc.lower().split() for doc in documents]\nbm25_index = BM25Okapi(tokenized_docs)\n\n# Retrieval functions (same as Phase 2.5)\ndef vector_search(query, k=10):\n    query_embedding = embedding_model.encode([query], normalize_embeddings=True)\n    scores, indices = faiss_index.search(query_embedding.astype('float32'), k)\n    return list(zip(indices[0], scores[0]))\n\ndef bm25_search(query, k=10):\n    tokenized_query = query.lower().split()\n    scores = bm25_index.get_scores(tokenized_query)\n    top_indices = np.argsort(scores)[::-1][:k]\n    return [(idx, scores[idx]) for idx in top_indices]\n\ndef hybrid_search(query, k=10, alpha=0.5):\n    vector_results = vector_search(query, k=k*2)\n    bm25_results = bm25_search(query, k=k*2)\n    rrf_scores = {}\n    rrf_k = 60\n    for rank, (idx, _) in enumerate(vector_results):\n        rrf_scores[idx] = rrf_scores.get(idx, 0) + alpha / (rrf_k + rank + 1)\n    for rank, (idx, _) in enumerate(bm25_results):\n        rrf_scores[idx] = rrf_scores.get(idx, 0) + (1 - alpha) / (rrf_k + rank + 1)\n    sorted_results = sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)[:k]\n    return sorted_results\n\ndef build_context(search_results, max_docs=5):\n    context_parts = []\n    for idx, score in search_results[:max_docs]:\n        doc = documents[idx]\n        context_parts.append(f\"--- Document (Relevance: {score:.4f}) ---\\n{doc}\\n\")\n    return \"\\n\".join(context_parts)\n\n# Prompt template (same as Phase 3.1)\nSYSTEM_PROMPT = \"\"\"You are a customer churn analysis expert. Based on the provided customer feedback and profile data, analyze the root causes of churn, assess risk levels, and provide actionable recommendations.\n\nYou must respond in the following JSON format:\n{\n    \"summary\": \"Brief overall summary of the analysis (2-3 sentences)\",\n    \"top_reasons\": [\"reason 1\", \"reason 2\", \"reason 3\"],\n    \"risk_level\": \"high/medium/low\",\n    \"actions\": [\"recommended action 1\", \"recommended action 2\", \"recommended action 3\"],\n    \"citations\": [\"7590-VHVEG\", \"5575-GNVDE\"]\n}\n\nGuidelines:\n- Base your analysis ONLY on the provided customer data\n- IMPORTANT: In citations, use ONLY the exact Customer ID format shown in the data (e.g., \"7590-VHVEG\"), NOT \"customerID_xxx\"\n- Risk level should be based on the proportion of churned customers and severity of issues\n- Actions should be specific and actionable\n- Always respond in valid JSON format\"\"\"\n\nUSER_TEMPLATE = \"\"\"Query: {query}\n\nRelevant Customer Data:\n{context}\n\nPlease analyze the above customer feedback and provide insights in the specified JSON format.\"\"\"\n\nprint(f\"\\nRetrieval system rebuilt:\")\nprint(f\"  Documents: {len(documents)}\")\nprint(f\"  FAISS index: {faiss_index.ntotal} vectors ({dimension}D)\")\nprint(f\"  BM25 vocabulary: {len(bm25_index.idf)} words\")\nprint(\"‚úì Phase 5.0 complete\")"
  },
  {
   "cell_type": "markdown",
   "id": "c8721737",
   "metadata": {},
   "source": "## 5.1 Design Query Templates\n\nWe design **8 categories of query templates** covering different analysis dimensions:\n\n| Category | Description | Example |\n|------|------|------|\n| Service Issues | Churn analysis related to specific services | \"Why are fiber optic customers churning?\" |\n| Demographics | Characteristics of different customer groups | \"What issues do senior citizens face?\" |\n| Contract Type | Relationship between contracts and churn | \"Why do month-to-month customers leave?\" |\n| Tenure | Churn patterns by tenure | \"What are the concerns of new customers?\" |\n| Sentiment | Customer emotion-related queries | \"What are the most negative feedback themes?\" |\n| Pricing | Price sensitivity analysis | \"How does pricing affect customer retention?\" |\n| Comparison | Comparisons between groups | \"Compare churn between DSL and fiber optic users\" |\n| Action | Decision-oriented queries | \"What should we do to reduce churn?\" |\n\nThrough parameter substitution (service names, customer groups, etc.), each category generates 8-10 queries, totaling ~60-80 unique queries."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daf8cdd",
   "metadata": {},
   "outputs": [],
   "source": "# --------------------------------------------\n# 5.1 Design Query Templates\n# ‚ö†Ô∏è Run on Google Colab\n# --------------------------------------------\n\nimport random\nrandom.seed(42)\n\n# 8 categories of query templates\nquery_templates = {\n    \"service\": [\n        \"Why are customers with {service} churning?\",\n        \"What are the main complaints about {service}?\",\n        \"How does {service} quality affect customer retention?\",\n        \"What feedback do churned customers give about {service}?\",\n    ],\n    \"demographics\": [\n        \"What issues do {group} face with our services?\",\n        \"Why are {group} more likely to churn?\",\n        \"What are the common complaints from {group}?\",\n        \"How can we improve retention for {group}?\",\n    ],\n    \"contract\": [\n        \"Why do {contract} contract customers leave?\",\n        \"What are the churn patterns for {contract} contracts?\",\n        \"How does {contract} contract type affect customer satisfaction?\",\n    ],\n    \"tenure\": [\n        \"What are the concerns of customers with {tenure} tenure?\",\n        \"Why do customers with {tenure} tenure churn?\",\n        \"What feedback patterns exist for {tenure} tenure customers?\",\n    ],\n    \"sentiment\": [\n        \"What are the most negative feedback themes?\",\n        \"What do unhappy customers complain about most?\",\n        \"What are the key drivers of customer dissatisfaction?\",\n        \"What emotional patterns appear in churned customer feedback?\",\n        \"Which service issues generate the strongest negative reactions?\",\n    ],\n    \"pricing\": [\n        \"How does pricing affect customer retention?\",\n        \"What do customers say about value for money?\",\n        \"Are high-paying customers more likely to churn?\",\n        \"What pricing-related complaints lead to churn?\",\n        \"How do monthly charges relate to customer satisfaction?\",\n    ],\n    \"comparison\": [\n        \"Compare churn between {comp_a} and {comp_b} customers\",\n        \"What differences exist between {comp_a} and {comp_b} customer feedback?\",\n        \"Which group has higher churn risk: {comp_a} or {comp_b}?\",\n    ],\n    \"action\": [\n        \"What should we do to reduce churn among {target} customers?\",\n        \"What retention strategies would work for {target} customers?\",\n        \"What immediate actions can prevent {target} customers from leaving?\",\n        \"How can we improve the experience for {target} customers?\",\n    ],\n}\n\n# Parameter sets\nparams = {\n    \"service\": [\"fiber optic internet\", \"DSL internet\", \"phone service\",\n                 \"online security\", \"tech support\", \"streaming TV\", \"streaming movies\"],\n    \"group\": [\"senior citizens\", \"customers without partners\",\n              \"customers with dependents\", \"young customers\", \"female customers\"],\n    \"contract\": [\"month-to-month\", \"one year\", \"two year\"],\n    \"tenure\": [\"less than 6 months\", \"6-12 months\", \"1-2 years\",\n               \"2-4 years\", \"over 5 years\"],\n    \"comp_a\": [\"DSL\", \"month-to-month\", \"senior citizen\", \"male\"],\n    \"comp_b\": [\"fiber optic\", \"two-year contract\", \"non-senior\", \"female\"],\n    \"target\": [\"high-value\", \"month-to-month\", \"fiber optic\", \"new\",\n               \"senior citizen\", \"long-tenure\", \"price-sensitive\"],\n}\n\n# Generate all queries\nall_queries = []\n\nfor category, templates in query_templates.items():\n    for template in templates:\n        if \"{service}\" in template:\n            for svc in params[\"service\"]:\n                all_queries.append({\"query\": template.format(service=svc), \"category\": category})\n        elif \"{group}\" in template:\n            for grp in params[\"group\"]:\n                all_queries.append({\"query\": template.format(group=grp), \"category\": category})\n        elif \"{contract}\" in template:\n            for ct in params[\"contract\"]:\n                all_queries.append({\"query\": template.format(contract=ct), \"category\": category})\n        elif \"{tenure}\" in template:\n            for tn in params[\"tenure\"]:\n                all_queries.append({\"query\": template.format(tenure=tn), \"category\": category})\n        elif \"{comp_a}\" in template:\n            for a, b in zip(params[\"comp_a\"], params[\"comp_b\"]):\n                all_queries.append({\"query\": template.format(comp_a=a, comp_b=b), \"category\": category})\n        elif \"{target}\" in template:\n            for tgt in params[\"target\"]:\n                all_queries.append({\"query\": template.format(target=tgt), \"category\": category})\n        else:\n            all_queries.append({\"query\": template, \"category\": category})\n\n# Deduplicate\nseen = set()\nunique_queries = []\nfor q in all_queries:\n    if q[\"query\"] not in seen:\n        seen.add(q[\"query\"])\n        unique_queries.append(q)\n\nprint(f\"Total queries generated: {len(unique_queries)}\")\nprint(f\"\\nQueries per category:\")\nfrom collections import Counter\ncat_counts = Counter(q[\"category\"] for q in unique_queries)\nfor cat, count in sorted(cat_counts.items()):\n    print(f\"  {cat}: {count}\")\nprint(f\"\\nExample queries:\")\nfor q in unique_queries[:5]:\n    print(f\"  [{q['category']}] {q['query']}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cf0ed4b5",
   "metadata": {},
   "source": "## 5.2 Load Teacher Model to Generate Gold Standard Training Data\n\n### Teacher Model Selection: Qwen2.5-14B-Instruct\n\n| Attribute | Description |\n|------|------|\n| Model | `Qwen/Qwen2.5-14B-Instruct` |\n| Quantization | 4-bit (NF4 + double quantization) |\n| VRAM | ~8 GB |\n| Advantage | Strong JSON structured output capability, excellent in both Chinese and English |\n| License | Apache 2.0 |\n\n### Generation Process\n\nFor each query:\n1. Call `hybrid_search()` to get Top-K documents\n2. Build prompt with `SYSTEM_PROMPT` + `USER_TEMPLATE`\n3. Use 14B teacher model for inference to generate JSON response\n4. Validate JSON contains all 5 required fields (summary, top_reasons, risk_level, actions, citations)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c751dc39",
   "metadata": {},
   "outputs": [],
   "source": "# --------------------------------------------\n# 5.2 Load Teacher Model (Qwen2.5-14B-Instruct 4-bit)\n# ‚ö†Ô∏è Run on Google Colab\n# --------------------------------------------\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nimport json\nimport gc\n\n# 4-bit quantization config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",             # NF4 quantization (information-theoretically optimal)\n    bnb_4bit_compute_dtype=torch.bfloat16,  # Use bfloat16 for computation\n    bnb_4bit_use_double_quant=True,         # Double quantization, further compression\n)\n\nTEACHER_MODEL_NAME = \"Qwen/Qwen2.5-14B-Instruct\"\n\nprint(f\"Loading teacher model: {TEACHER_MODEL_NAME} (4-bit)...\")\nprint(\"Estimated time: 3-5 minutes...\")\n\nteacher_tokenizer = AutoTokenizer.from_pretrained(\n    TEACHER_MODEL_NAME,\n    trust_remote_code=True\n)\n\nteacher_model = AutoModelForCausalLM.from_pretrained(\n    TEACHER_MODEL_NAME,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\nteacher_model.eval()\n\n# Check memory usage\nprint(f\"\\nTeacher model loaded ‚úì\")\nprint(f\"GPU memory usage: {torch.cuda.memory_allocated() / 1024**3:.1f} GB\")\nprint(f\"GPU total memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
  },
  {
   "cell_type": "markdown",
   "id": "bef39cff",
   "metadata": {},
   "source": "### Generate Training Data with Teacher Model\n\nFor each query, use the teacher model to generate structured JSON responses with strict format validation.\n\n**Validation Rules**:\n- Must be valid JSON\n- Must contain 5 fields: `summary`, `top_reasons`, `risk_level`, `actions`, `citations`\n- `risk_level` must be one of `high`/`medium`/`low`\n- All list fields must be non-empty"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5cdfad",
   "metadata": {},
   "outputs": [],
   "source": "# --------------------------------------------\n# 5.2b Generate Training Data with Teacher Model\n# ‚ö†Ô∏è Run on Google Colab\n# --------------------------------------------\n\ndef generate_with_teacher(query, k=5, max_new_tokens=1024):\n    \"\"\"\n    Generate RAG response using teacher model\n\n    Args:\n        query: User query\n        k: Number of documents to retrieve\n        max_new_tokens: Maximum tokens to generate\n\n    Returns:\n        dict: Parsed JSON result, returns None on failure\n    \"\"\"\n    # Step 1-2: Retrieve + build context (same as original rag_query)\n    search_results = hybrid_search(query, k=k)\n    context = build_context(search_results, max_docs=k)\n\n    # Step 3: Build ChatML format messages\n    messages = [\n        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n        {\"role\": \"user\", \"content\": USER_TEMPLATE.format(query=query, context=context)},\n    ]\n\n    # Use tokenizer's chat template\n    text = teacher_tokenizer.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )\n    inputs = teacher_tokenizer(text, return_tensors=\"pt\").to(teacher_model.device)\n\n    # Generate\n    with torch.no_grad():\n        outputs = teacher_model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            temperature=0.7,\n            top_p=0.9,\n            do_sample=True,\n            pad_token_id=teacher_tokenizer.eos_token_id,\n        )\n\n    # Decode (only the newly generated part)\n    generated_ids = outputs[0][inputs['input_ids'].shape[1]:]\n    response_text = teacher_tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n\n    # Step 4: Parse JSON\n    try:\n        # Try direct parsing\n        result = json.loads(response_text)\n    except json.JSONDecodeError:\n        # Try extracting JSON block (model may add text around JSON)\n        import re\n        json_match = re.search(r'\\{[\\s\\S]*\\}', response_text)\n        if json_match:\n            try:\n                result = json.loads(json_match.group())\n            except json.JSONDecodeError:\n                return None\n        else:\n            return None\n\n    # Validate required fields\n    required_fields = ['summary', 'top_reasons', 'risk_level', 'actions', 'citations']\n    if not all(field in result for field in required_fields):\n        return None\n\n    # Validate field types and values\n    if not isinstance(result['top_reasons'], list) or len(result['top_reasons']) == 0:\n        return None\n    if result['risk_level'] not in ['high', 'medium', 'low']:\n        return None\n    if not isinstance(result['actions'], list) or len(result['actions']) == 0:\n        return None\n    if not isinstance(result['citations'], list):\n        return None\n\n    # Add retrieval metadata\n    result['retrieved_customer_ids'] = [customer_ids[idx] for idx, _ in search_results]\n\n    return result\n\n# Test teacher model generation\nprint(\"Testing teacher model generation...\")\ntest_result = generate_with_teacher(\"Why are fiber optic customers churning?\", k=5)\nif test_result:\n    print(\"‚úì Teacher model generation test passed\")\n    print(f\"  summary: {test_result['summary'][:80]}...\")\n    print(f\"  risk_level: {test_result['risk_level']}\")\n    print(f\"  top_reasons: {len(test_result['top_reasons'])} items\")\n    print(f\"  actions: {len(test_result['actions'])} items\")\n    print(f\"  citations: {test_result['citations']}\")\nelse:\n    print(\"‚úó Teacher model generation test failed, please check model output\")"
  },
  {
   "cell_type": "markdown",
   "id": "260ca403",
   "metadata": {},
   "source": "## 5.3 Data Augmentation\n\nFor each query, generate training data using **k=3, 5, 7** (number of retrieved documents), totaling approximately **180-240 samples**.\n\nSignificance of different k values:\n- **k=3**: Few highly relevant documents -> Model learns precise analysis\n- **k=5**: Standard retrieval amount -> Consistent with inference time\n- **k=7**: More context -> Model learns to handle noisy information\n\n## 5.4 Train/Validation Split\n\nUse **85%/15%** stratified split, stratified by query category to ensure uniform distribution across categories.\n\n## 5.5 Export ChatML Format\n\nUse Qwen2.5's ChatML template format, each data sample contains a complete system + user + assistant three-turn conversation."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e684dd",
   "metadata": {},
   "outputs": [],
   "source": "# --------------------------------------------\n# 5.3 Data Augmentation + 5.4 Split + 5.5 Export\n# ‚ö†Ô∏è Run on Google Colab\n# --------------------------------------------\n\nfrom tqdm import tqdm\n\n# === 5.3 Data Augmentation: Multi-k Generation ===\ntraining_samples = []\nfailed_count = 0\nk_values = [3, 5, 7]\n\nprint(\"Starting training data generation...\")\nprint(f\"Number of queries: {len(unique_queries)}, k values: {k_values}\")\nprint(f\"Expected to generate: {len(unique_queries) * len(k_values)} samples\")\nprint(\"=\" * 50)\n\nfor k in k_values:\n    print(f\"\\n--- Generating with k={k} ---\")\n    for i, q_info in enumerate(tqdm(unique_queries, desc=f\"k={k}\")):\n        query = q_info[\"query\"]\n        category = q_info[\"category\"]\n\n        result = generate_with_teacher(query, k=k)\n\n        if result is not None:\n            # Build context (same as during generation)\n            search_results = hybrid_search(query, k=k)\n            context = build_context(search_results, max_docs=k)\n\n            # Build training sample\n            sample = {\n                \"query\": query,\n                \"category\": category,\n                \"k\": k,\n                \"context\": context,\n                \"response\": json.dumps(result, ensure_ascii=False),\n            }\n            training_samples.append(sample)\n        else:\n            failed_count += 1\n\n        # Clear GPU cache every 20 samples\n        if (i + 1) % 20 == 0:\n            torch.cuda.empty_cache()\n\nprint(f\"\\nGeneration complete:\")\nprint(f\"  Succeeded: {len(training_samples)} items\")\nprint(f\"  Failed: {failed_count} items\")\nprint(f\"  Success rate: {len(training_samples)/(len(training_samples)+failed_count)*100:.1f}%\")\n\n# === 5.4 Train/Validation Split (85%/15%, stratified by category) ===\nfrom sklearn.model_selection import train_test_split\n\n# Stratified split by category\ncategories = [s[\"category\"] for s in training_samples]\ntrain_samples, val_samples = train_test_split(\n    training_samples, test_size=0.15, random_state=42, stratify=categories\n)\n\nprint(f\"\\nData split:\")\nprint(f\"  Training set: {len(train_samples)} items\")\nprint(f\"  Validation set: {len(val_samples)} items\")\n\n# === 5.5 Export ChatML Format JSONL ===\ndef to_chatml(sample):\n    \"\"\"Convert sample to Qwen2.5 ChatML format\"\"\"\n    user_content = USER_TEMPLATE.format(\n        query=sample[\"query\"],\n        context=sample[\"context\"]\n    )\n    messages = [\n        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n        {\"role\": \"user\", \"content\": user_content},\n        {\"role\": \"assistant\", \"content\": sample[\"response\"]},\n    ]\n\n    # Use tokenizer's chat template to generate full text\n    text = teacher_tokenizer.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=False\n    )\n\n    return {\"text\": text, \"messages\": messages}\n\n# Export to JSONL\nos.makedirs(\"data\", exist_ok=True)\n\ntrain_path = \"data/finetune_train_hf.jsonl\"\nval_path = \"data/finetune_val_hf.jsonl\"\n\nwith open(train_path, 'w', encoding='utf-8') as f:\n    for sample in train_samples:\n        chatml = to_chatml(sample)\n        f.write(json.dumps(chatml, ensure_ascii=False) + '\\n')\n\nwith open(val_path, 'w', encoding='utf-8') as f:\n    for sample in val_samples:\n        chatml = to_chatml(sample)\n        f.write(json.dumps(chatml, ensure_ascii=False) + '\\n')\n\nprint(f\"\\nExport complete:\")\nprint(f\"  Training set: {train_path} ({len(train_samples)} items)\")\nprint(f\"  Validation set: {val_path} ({len(val_samples)} items)\")\n\n# Verify exported file\nwith open(train_path, 'r') as f:\n    first_line = json.loads(f.readline())\n    print(f\"\\nSample verification:\")\n    print(f\"  Fields: {list(first_line.keys())}\")\n    print(f\"  text length: {len(first_line['text'])} characters\")\n    print(f\"  messages count: {len(first_line['messages'])} turns\")\n\n# Also save to Google Drive\ndrive_data_dir = '/content/drive/MyDrive/lora_finetune_data'\nos.makedirs(drive_data_dir, exist_ok=True)\nimport shutil\nshutil.copy(train_path, os.path.join(drive_data_dir, 'finetune_train_hf.jsonl'))\nshutil.copy(val_path, os.path.join(drive_data_dir, 'finetune_val_hf.jsonl'))\nprint(f\"\\nBacked up to Google Drive: {drive_data_dir}\")\nprint(\"\\n‚úì Phase 5.3-5.5 complete\")"
  },
  {
   "cell_type": "markdown",
   "id": "14c1b434",
   "metadata": {},
   "source": "## 5.6 Release Teacher Model Memory\n\nThe 14B teacher model uses approximately 8GB of VRAM, which needs to be released before loading the 7B student model for fine-tuning.\n\nUse `del model` + `torch.cuda.empty_cache()` + `gc.collect()` triple cleanup to ensure complete memory release."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5906f214",
   "metadata": {},
   "outputs": [],
   "source": "# --------------------------------------------\n# 5.6 Release Teacher Model Memory\n# ‚ö†Ô∏è Run on Google Colab\n# --------------------------------------------\n\nprint(f\"GPU memory before release: {torch.cuda.memory_allocated() / 1024**3:.1f} GB\")\n\n# Delete teacher model and tokenizer\ndel teacher_model\ndel teacher_tokenizer\ngc.collect()\ntorch.cuda.empty_cache()\n\nprint(f\"GPU memory after release: {torch.cuda.memory_allocated() / 1024**3:.1f} GB\")\nprint(\"\\n‚úì Phase 5 complete - Teacher model memory released\")\nprint(\"  Training data saved, ready to start Phase 6 fine-tuning\")"
  },
  {
   "cell_type": "markdown",
   "id": "e5f660cc",
   "metadata": {},
   "source": "---\n\n# Phase 6: QLoRA Fine-tuning\n\n> **Warning: Run on Google Colab (free T4 GPU)**\n\n## Objective\n\nUse **QLoRA** to fine-tune **Qwen2.5-7B-Instruct** to generate high-quality structured JSON responses for the customer churn analysis task.\n\n## Base Model: Qwen2.5-7B-Instruct\n\n| Attribute | Description |\n|------|------|\n| Parameters | 7.6B |\n| Size after quantization | ~4.5 GB (4-bit) |\n| Training peak VRAM | ~12 GB |\n| T4 safe | Yes (within 15 GB limit) |\n| License | Apache 2.0 |\n\n## What Is QLoRA?\n\n**QLoRA (Quantized Low-Rank Adaptation)** combines two key techniques:\n\n1. **4-bit Quantization**: Compresses model weights from FP16 (2 bytes) to 4-bit (0.5 bytes), reducing memory usage by 75%\n2. **LoRA Low-Rank Adaptation**: Freezes original weights, only trains a small number of low-rank matrices (~20M parameters, 0.26% of total)\n\n### LoRA Parameter Explanation\n\n| Parameter | Value | Description |\n|------|-----|------|\n| `r` | 16 | Rank of low-rank matrices; higher means more expressive but uses more memory |\n| `lora_alpha` | 32 | Scaling factor, typically set to 2r |\n| `lora_dropout` | 0.05 | Dropout to prevent overfitting |\n| Target modules | q/k/v/o/gate/up/down_proj | Covers all attention + FFN layers |\n\n### Training Parameter Explanation\n\n| Parameter | Value | Description |\n|------|-----|------|\n| Epochs | 3 | Train more rounds for small datasets |\n| Batch size | 1 | Limited by VRAM |\n| Gradient accumulation | 8 | Effective batch size = 8 |\n| Learning rate | 2e-4 | QLoRA standard learning rate |\n| Optimizer | paged_adamw_8bit | 8-bit optimizer to save memory |\n| Scheduler | cosine | Cosine annealing for stable training |\n| Max sequence length | 2048 | Covers RAG context + response |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98ec931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Phase 6: QLoRA Fine-tuning\n",
    "# ‚ö†Ô∏è Run on Google Colab (T4 GPU)\n",
    "# ============================================\n",
    "\n",
    "# --------------------------------------------\n",
    "# 6.1 Load Base Model + Configure QLoRA\n",
    "# --------------------------------------------\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import torch\n",
    "\n",
    "# 4-bit quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # A100 natively supports bfloat16\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "BASE_MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "print(f\"Loading base model: {BASE_MODEL_NAME} (4-bit)...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.model_max_length = 2048       # Limit max sequence length\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Configure LoRA adapter\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                           # Rank of low-rank matrices\n",
    "    lora_alpha=32,                  # Scaling factor (usually = 2r)\n",
    "    lora_dropout=0.05,              # Dropout to prevent overfitting\n",
    "    bias=\"none\",                    # Do not train bias\n",
    "    task_type=\"CAUSAL_LM\",         # Causal language model task\n",
    "    target_modules=[                # Target modules: cover all linear layers\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(f\"\\nGPU memory usage: {torch.cuda.memory_allocated() / 1024**3:.1f} GB\")\n",
    "print(\"\\n‚úì Model loading and LoRA configuration complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a36c041",
   "metadata": {},
   "source": "### Load Training Data and Start Training\n\nUse `SFTTrainer` (Supervised Fine-Tuning Trainer) for training. It wraps the Hugging Face Trainer with optimizations for instruction fine-tuning scenarios.\n\n**Training Estimate**:\n- ~180-240 samples, 3 epochs\n- Batch size 1 x Gradient accumulation 8 = effective batch 8\n- Estimated training time: **~30-50 minutes** (T4 GPU)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57992d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# 6.2 Load Training Data + SFTTrainer Training\n",
    "# ‚ö†Ô∏è Run on Google Colab\n",
    "# --------------------------------------------\n",
    "\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# Load training data\n",
    "train_dataset = load_dataset('json', data_files='data/finetune_train_hf.jsonl', split='train')\n",
    "val_dataset = load_dataset('json', data_files='data/finetune_val_hf.jsonl', split='train')\n",
    "\n",
    "print(f\"Training set: {len(train_dataset)} items\")\n",
    "print(f\"Validation set: {len(val_dataset)} items\")\n",
    "\n",
    "# Training config\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=\"./qwen2.5-7b-churn-lora\",      # Output directory\n",
    "    num_train_epochs=3,                          # Number of epochs\n",
    "    per_device_train_batch_size=1,               # Per GPU batch size\n",
    "    per_device_eval_batch_size=1,                # Evaluation batch size\n",
    "    gradient_accumulation_steps=8,               # Gradient accumulation (effective batch = 8)\n",
    "    optim=\"paged_adamw_8bit\",                    # 8-bit optimizer to save memory\n",
    "    learning_rate=2e-4,                          # QLoRA standard learning rate\n",
    "    lr_scheduler_type=\"cosine\",                  # Cosine annealing\n",
    "    warmup_steps=6,                           # Warmup ratio\n",
    "    weight_decay=0.01,                           # Weight decay\n",
    "    fp16=False,                                   # A100 uses BF16 (more stable, no GradScaler needed)\n",
    "    bf16=True,\n",
    "    logging_steps=5,                             # Log every 5 steps\n",
    "    eval_strategy=\"steps\",                       # Evaluate by steps\n",
    "    eval_steps=20,                               # Evaluate every 20 steps\n",
    "    save_strategy=\"steps\",                       # Save by steps\n",
    "    save_steps=50,                               # Save every 50 steps\n",
    "    save_total_limit=3,                          # Keep at most 3 checkpoints\n",
    "    gradient_checkpointing=True,                 # Gradient checkpointing to save memory\n",
    "    dataset_text_field=\"text\",                   # Text field name in dataset\n",
    "    report_to=\"none\",                            # Do not upload to wandb\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining config:\")\n",
    "print(f\"  Effective batch size: {sft_config.per_device_train_batch_size * sft_config.gradient_accumulation_steps}\")\n",
    "print(f\"  Total training steps: {len(train_dataset) // (sft_config.per_device_train_batch_size * sft_config.gradient_accumulation_steps) * sft_config.num_train_epochs}\")\n",
    "\n",
    "# Start training\n",
    "print(\"\\nStarting training...\")\n",
    "print(\"=\" * 50)\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\nTraining complete ‚úì\")\n",
    "print(f\"  Training loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"  Training time: {train_result.metrics['train_runtime']:.0f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410c2b7b",
   "metadata": {},
   "source": "### Save LoRA Adapter\n\nThe fine-tuned LoRA adapter is only approximately **40 MB** (compared to the full model at 14 GB), making it easy to store and share.\n\nSave locations:\n1. Colab local: `./qwen2.5-7b-churn-lora/final`\n2. Google Drive: `/content/drive/MyDrive/qwen2.5-7b-churn-lora` (persistent)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9571ae5",
   "metadata": {},
   "outputs": [],
   "source": "# --------------------------------------------\n# 6.3 Save LoRA Adapter\n# ‚ö†Ô∏è Run on Google Colab\n# --------------------------------------------\n\n# Save locally\nadapter_local_path = \"./qwen2.5-7b-churn-lora/final\"\ntrainer.save_model(adapter_local_path)\ntokenizer.save_pretrained(adapter_local_path)\nprint(f\"Adapter saved to: {adapter_local_path}\")\n\n# Save to Google Drive (persistent)\nadapter_drive_path = \"/content/drive/MyDrive/qwen2.5-7b-churn-lora\"\nos.makedirs(adapter_drive_path, exist_ok=True)\ntrainer.save_model(adapter_drive_path)\ntokenizer.save_pretrained(adapter_drive_path)\nprint(f\"Adapter backed up to Google Drive: {adapter_drive_path}\")\n\n# Show adapter size\nimport subprocess\nresult = subprocess.run(['du', '-sh', adapter_local_path], capture_output=True, text=True)\nprint(f\"\\nAdapter size: {result.stdout.strip()}\")\n\n# Release training-related memory\ndel trainer\ngc.collect()\ntorch.cuda.empty_cache()\n\n# Optional: Push to Hugging Face Hub\n# from huggingface_hub import login\n# login(token=\"your_hf_token\")\n# model.push_to_hub(\"your-username/qwen2.5-7b-churn-lora\")\n# tokenizer.push_to_hub(\"your-username/qwen2.5-7b-churn-lora\")\n\nprint(f\"\\nGPU memory: {torch.cuda.memory_allocated() / 1024**3:.1f} GB\")\nprint(\"\\n‚úì Phase 6 complete - LoRA Adapter saved\")"
  },
  {
   "cell_type": "markdown",
   "id": "7a9d1780",
   "metadata": {},
   "source": "---\n\n# Phase 7: Open-Source Model Integration into RAG Pipeline\n\n> **Warning: Run on Google Colab (free T4 GPU)**\n\n## Objective\n\nReplace the Phase 3 base model with the **QLoRA fine-tuned Qwen2.5-7B-Instruct** to improve RAG output quality.\n\n## Architecture Change\n\n```\nOriginal Architecture (Phase 3-4):\n  hybrid_search() -> build_context() -> Qwen2.5-7B base (zero-shot) -> JSON parsing\n\nNew Architecture (Phase 7):\n  hybrid_search() -> build_context() -> Qwen2.5-7B + LoRA (local) -> JSON parsing\n```\n\n## VRAM Estimate\n\n| Component | VRAM |\n|------|------|\n| BGE Embedding Model | ~0.5 GB |\n| Qwen2.5-7B + LoRA (4-bit) | ~4.5 GB |\n| **Total** | **~5 GB** |\n\nVRAM is sufficient in inference mode, T4 is fully capable."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7542214",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# Phase 7: Open-Source Model Integration into RAG Pipeline\n# ‚ö†Ô∏è Run on Google Colab (T4 GPU)\n# ============================================\n\n# --------------------------------------------\n# 7.1 Load Fine-tuned Model\n# --------------------------------------------\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import PeftModel\nimport torch\nimport json\nimport re\nimport gc\n\n# Clean up if previous model is still in memory\ngc.collect()\ntorch.cuda.empty_cache()\n\n# 4-bit quantization config (for inference)\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True,\n)\n\nBASE_MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n# Load adapter from Google Drive (persistent path)\nADAPTER_PATH = \"/content/drive/MyDrive/qwen2.5-7b-churn-lora\"\n# Alternative: load from local\n# ADAPTER_PATH = \"./qwen2.5-7b-churn-lora/final\"\n\nprint(f\"Loading base model: {BASE_MODEL_NAME} (4-bit)...\")\nft_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME, trust_remote_code=True)\n\nft_base_model = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL_NAME,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\n\n# Load LoRA adapter\nprint(f\"Loading LoRA adapter: {ADAPTER_PATH}...\")\nft_model = PeftModel.from_pretrained(ft_base_model, ADAPTER_PATH)\nft_model.eval()\n\nprint(f\"\\nFine-tuned model loaded ‚úì\")\nprint(f\"GPU memory: {torch.cuda.memory_allocated() / 1024**3:.1f} GB\")"
  },
  {
   "cell_type": "markdown",
   "id": "ec45ac9b",
   "metadata": {},
   "source": "## 7.2 Define Local RAG Query Function\n\n`rag_query_local()` has the same interface as the original `rag_query()`, but with LoRA fine-tuned weights loaded:\n\n| Step | Original Method (Phase 3) | New Method (Phase 7) |\n|------|------------------|------------------|\n| Retrieval | `hybrid_search()` | `hybrid_search()` (unchanged) |\n| Context | `build_context()` | `build_context()` (unchanged) |\n| Generation | `model.generate()` (base) | `model.generate()` (base + LoRA) |\n| Parsing | `json.loads()` | `json.loads()` + regex fallback |\n\n### Enhanced JSON Parsing\n\nOpen-source models may add extra text around the JSON (e.g., \"Here is the analysis:\"), so a **regex fallback** was added to extract the JSON block."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de3532f",
   "metadata": {},
   "outputs": [],
   "source": "# --------------------------------------------\n# 7.2 Define Local RAG Query Function\n# ‚ö†Ô∏è Run on Google Colab\n# --------------------------------------------\n\ndef rag_query_local(user_query, k=5, max_new_tokens=1024, use_finetuned=True):\n    \"\"\"\n    Fine-tuned model RAG Query Pipeline (base + LoRA adapter)\n\n    Args:\n        user_query: User query\n        k: Number of documents to retrieve\n        max_new_tokens: Maximum tokens to generate\n        use_finetuned: True for fine-tuned model, False for base model (for comparison)\n\n    Returns:\n        dict: Parsed JSON result\n    \"\"\"\n    # Step 1: Retrieve relevant documents (same as original rag_query)\n    search_results = hybrid_search(user_query, k=k)\n\n    # Step 2: Build context (same as original rag_query)\n    context = build_context(search_results, max_docs=k)\n\n    # Step 3: Build prompt and generate with local model\n    messages = [\n        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n        {\"role\": \"user\", \"content\": USER_TEMPLATE.format(query=user_query, context=context)},\n    ]\n\n    active_model = ft_model if use_finetuned else ft_base_model\n\n    text = ft_tokenizer.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )\n    inputs = ft_tokenizer(text, return_tensors=\"pt\").to(active_model.device)\n\n    with torch.no_grad():\n        outputs = active_model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            temperature=0.7,\n            top_p=0.9,\n            do_sample=True,\n            pad_token_id=ft_tokenizer.eos_token_id,\n        )\n\n    generated_ids = outputs[0][inputs['input_ids'].shape[1]:]\n    response_text = ft_tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n\n    # Step 4: JSON parsing + regex fallback\n    result = None\n    try:\n        result = json.loads(response_text)\n    except json.JSONDecodeError:\n        # Fallback: extract JSON block\n        json_match = re.search(r'\\{[\\s\\S]*\\}', response_text)\n        if json_match:\n            try:\n                result = json.loads(json_match.group())\n            except json.JSONDecodeError:\n                pass\n\n    if result is None:\n        result = {\"error\": \"Failed to parse JSON\", \"raw_response\": response_text}\n\n    # Add retrieval metadata\n    result[\"retrieved_docs\"] = len(search_results)\n    result[\"retrieved_customer_ids\"] = [customer_ids[idx] for idx, _ in search_results]\n\n    return result\n\nprint(\"rag_query_local() defined ‚úì\")"
  },
  {
   "cell_type": "markdown",
   "id": "59d387d4",
   "metadata": {},
   "source": "## 7.3 Define Complete Analysis Function\n\n`analyze_churn_local()` encapsulates the complete RAG workflow: Retrieval -> Generation -> Formatted Output -> Citation Validation.\n\nReuses existing `display_analysis()` and `validate_citations()` functions."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bcb697",
   "metadata": {},
   "outputs": [],
   "source": "# --------------------------------------------\n# 7.3 Define Complete Analysis Function\n# ‚ö†Ô∏è Run on Google Colab\n# --------------------------------------------\n\n# Reuse Phase 4 helper functions\ndef display_analysis(result):\n    \"\"\"Display analysis results in formatted style\"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(\"CUSTOMER CHURN ANALYSIS REPORT\")\n    print(\"=\" * 60)\n\n    if \"error\" in result:\n        print(f\"Error: {result['error']}\")\n        if \"raw_response\" in result:\n            print(f\"Raw response: {result['raw_response'][:500]}\")\n        return\n\n    print(f\"\\nSummary:\")\n    print(f\"   {result.get('summary', 'N/A')}\")\n\n    print(f\"\\nTop Reasons for Churn:\")\n    for i, reason in enumerate(result.get('top_reasons', []), 1):\n        print(f\"   {i}. {reason}\")\n\n    print(f\"\\nRisk Level: {result.get('risk_level', 'N/A').upper()}\")\n\n    print(f\"\\nRecommended Actions:\")\n    for i, action in enumerate(result.get('actions', []), 1):\n        print(f\"   {i}. {action}\")\n\n    print(f\"\\nCitations (Customer IDs):\")\n    print(f\"   {', '.join(result.get('citations', []))}\")\n\n    print(f\"\\nRetrieved Documents: {result.get('retrieved_docs', 0)}\")\n    print(\"=\" * 60)\n\ndef normalize_customer_id(cid):\n    \"\"\"Normalize customer ID\"\"\"\n    cid = cid.strip().lower()\n    prefixes = ['customerid_', 'customer_id_', 'cid_', 'id_']\n    for prefix in prefixes:\n        if cid.startswith(prefix):\n            cid = cid[len(prefix):]\n    return cid\n\ndef validate_citations(result):\n    \"\"\"Validate whether citations come from retrieved results\"\"\"\n    citations = result.get('citations', [])\n    retrieved_ids = result.get('retrieved_customer_ids', [])\n\n    retrieved_normalized = {normalize_customer_id(rid): rid for rid in retrieved_ids}\n\n    valid_citations = []\n    invalid_citations = []\n\n    for cid in citations:\n        normalized = normalize_customer_id(cid)\n        if normalized in retrieved_normalized:\n            valid_citations.append(cid)\n        else:\n            invalid_citations.append(cid)\n\n    accuracy = len(valid_citations) / len(citations) if citations else 0\n\n    return {\n        \"total_citations\": len(citations),\n        \"valid_citations\": valid_citations,\n        \"invalid_citations\": invalid_citations,\n        \"accuracy\": accuracy,\n    }\n\ndef analyze_churn_local(query, use_finetuned=True):\n    \"\"\"\n    Complete local churn analysis function\n\n    Args:\n        query: Analysis query\n        use_finetuned: True for fine-tuned model, False for base model\n\n    Returns:\n        Analysis result dict\n    \"\"\"\n    model_type = \"Fine-tuned\" if use_finetuned else \"Base\"\n    print(f\"Analyzing ({model_type}): {query}\")\n    print(\"-\" * 50)\n\n    result = rag_query_local(query, k=5, use_finetuned=use_finetuned)\n    display_analysis(result)\n\n    validation = validate_citations(result)\n    print(f\"\\nCitation Accuracy: {validation['accuracy']*100:.1f}%\")\n\n    return result\n\nprint(\"analyze_churn_local() defined ‚úì\")"
  },
  {
   "cell_type": "markdown",
   "id": "41ffe26a",
   "metadata": {},
   "source": "## 7.4 End-to-End Test\n\nUse 3 different types of queries to verify the complete RAG Pipeline with the local model."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89744595",
   "metadata": {},
   "outputs": [],
   "source": "# --------------------------------------------\n# 7.4 End-to-End Test\n# ‚ö†Ô∏è Run on Google Colab\n# --------------------------------------------\n\ntest_queries = [\n    \"Why are customers with fiber optic internet churning?\",\n    \"What issues do senior citizens face with our services?\",\n    \"What should we do to reduce churn among month-to-month customers?\",\n]\n\nprint(\"=\" * 60)\nprint(\"Phase 7: End-to-End Test - Fine-tuned Model RAG Pipeline\")\nprint(\"=\" * 60)\n\ntest_results = []\nfor i, query in enumerate(test_queries, 1):\n    print(f\"\\n{'='*60}\")\n    print(f\"Test {i}/{len(test_queries)}\")\n    print(f\"{'='*60}\")\n    result = analyze_churn_local(query, use_finetuned=True)\n    test_results.append(result)\n\n# Summarize test results\nprint(\"\\n\" + \"=\" * 60)\nprint(\"End-to-End Test Summary\")\nprint(\"=\" * 60)\nfor i, (query, result) in enumerate(zip(test_queries, test_results), 1):\n    has_error = \"error\" in result\n    json_valid = not has_error\n    fields_ok = all(f in result for f in ['summary', 'top_reasons', 'risk_level', 'actions', 'citations']) if not has_error else False\n    print(f\"  Test {i}: JSON={'OK' if json_valid else 'FAIL'}  Fields={'Complete' if fields_ok else 'Missing'}  Query={query[:50]}...\")\n\nprint(\"\\n‚úì Phase 7 complete - Open-source model RAG Pipeline verified\")"
  },
  {
   "cell_type": "markdown",
   "id": "cf5f9860",
   "metadata": {},
   "source": "---\n\n# Phase 8: Evaluation & Comparison\n\n> **Warning: Run on Google Colab (free T4 GPU)**\n\n## Objective\n\nCompare the performance of **Base model (zero-shot)** vs **QLoRA fine-tuned model** on the RAG task.\n\n## Dual-Model Comparison Plan\n\n| Model | Description |\n|------|------|\n| Qwen2.5-7B-Instruct (base) | Original model, zero-shot |\n| Qwen2.5-7B-Instruct + LoRA | Fine-tuned model |\n\n## Evaluation Metrics\n\n| Metric | Description |\n|------|------|\n| JSON Format Compliance | Whether the response is valid JSON |\n| Field Completeness | Whether all 5 required fields are present |\n| Type Correctness | Whether field values and types match the schema |\n| Citation Accuracy | Whether cited customer IDs come from retrieval results |\n| Risk Level Alignment | Whether risk_level aligns with actual churn ratio |\n\n## Test Set\n\n10 diverse queries not used in training, covering different analysis dimensions."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae98d9bd",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# Phase 8: Evaluation & Comparison\n# ‚ö†Ô∏è Run on Google Colab (T4 GPU)\n# ============================================\n\n# --------------------------------------------\n# 8.1 Define Test Set and Evaluation Functions\n# --------------------------------------------\n\n# 10 test queries not used in training\neval_queries = [\n    {\"query\": \"What are the top reasons for churn among customers with multiple services?\", \"category\": \"service\"},\n    {\"query\": \"How do payment methods influence customer retention?\", \"category\": \"billing\"},\n    {\"query\": \"What patterns exist in feedback from customers who stayed?\", \"category\": \"sentiment\"},\n    {\"query\": \"Why do customers without online security churn more?\", \"category\": \"service\"},\n    {\"query\": \"What is the relationship between tenure and customer satisfaction?\", \"category\": \"tenure\"},\n    {\"query\": \"How effective is tech support in preventing churn?\", \"category\": \"service\"},\n    {\"query\": \"What demographic factors contribute most to churn risk?\", \"category\": \"demographics\"},\n    {\"query\": \"Compare customer satisfaction between paperless and non-paperless billing\", \"category\": \"comparison\"},\n    {\"query\": \"What proactive measures can reduce churn for high-value customers?\", \"category\": \"action\"},\n    {\"query\": \"What role does contract length play in customer loyalty?\", \"category\": \"contract\"},\n]\n\ndef evaluate_response(result, query_info):\n    \"\"\"\n    Evaluate the quality of a single response\n\n    Returns scores per dimension (0 or 1)\n    \"\"\"\n    scores = {}\n\n    # 1. JSON format compliance\n    scores[\"json_valid\"] = 0 if \"error\" in result else 1\n\n    if scores[\"json_valid\"] == 0:\n        # JSON invalid, all other metrics are 0\n        scores[\"fields_complete\"] = 0\n        scores[\"types_correct\"] = 0\n        scores[\"citation_accuracy\"] = 0.0\n        scores[\"risk_aligned\"] = 0\n        scores[\"response_length\"] = 0\n        return scores\n\n    # 2. Field completeness\n    required_fields = ['summary', 'top_reasons', 'risk_level', 'actions', 'citations']\n    scores[\"fields_complete\"] = 1 if all(f in result for f in required_fields) else 0\n\n    # 3. Type correctness\n    type_checks = [\n        isinstance(result.get('summary'), str) and len(result.get('summary', '')) > 10,\n        isinstance(result.get('top_reasons'), list) and len(result.get('top_reasons', [])) > 0,\n        result.get('risk_level') in ['high', 'medium', 'low'],\n        isinstance(result.get('actions'), list) and len(result.get('actions', [])) > 0,\n        isinstance(result.get('citations'), list),\n    ]\n    scores[\"types_correct\"] = 1 if all(type_checks) else 0\n\n    # 4. Citation accuracy\n    validation = validate_citations(result)\n    scores[\"citation_accuracy\"] = validation[\"accuracy\"]\n\n    # 5. Risk level alignment (based on actual churn ratio of retrieved customers)\n    retrieved_ids = result.get(\"retrieved_customer_ids\", [])\n    if retrieved_ids:\n        churned = sum(1 for cid in retrieved_ids\n                      if cid in customer_ids and\n                      df_with_feedback[df_with_feedback['customerID'] == cid]['Churn'].values[0] == 'Yes'\n                      if len(df_with_feedback[df_with_feedback['customerID'] == cid]) > 0)\n        churn_rate = churned / len(retrieved_ids)\n        actual_risk = \"high\" if churn_rate > 0.5 else (\"medium\" if churn_rate > 0.25 else \"low\")\n        scores[\"risk_aligned\"] = 1 if result.get(\"risk_level\") == actual_risk else 0\n    else:\n        scores[\"risk_aligned\"] = 0\n\n    # 6. Response detail level (total characters of summary + reasons + actions)\n    detail_len = len(result.get('summary', ''))\n    detail_len += sum(len(r) for r in result.get('top_reasons', []))\n    detail_len += sum(len(a) for a in result.get('actions', []))\n    scores[\"response_length\"] = detail_len\n\n    return scores\n\nprint(f\"Test set: {len(eval_queries)} queries\")\nprint(\"Evaluation functions defined ‚úì\")"
  },
  {
   "cell_type": "markdown",
   "id": "7fc88fe8",
   "metadata": {},
   "source": "### Run Dual-Model Evaluation\n\nFor the same set of test queries, generate responses with both the **Base model** and **Fine-tuned model** and evaluate them."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5202bb8a",
   "metadata": {},
   "outputs": [],
   "source": "# --------------------------------------------\n# 8.2 Run Dual-Model Evaluation\n# ‚ö†Ô∏è Run on Google Colab\n# --------------------------------------------\n\nfrom tqdm import tqdm\n\n# Store evaluation results\neval_results = {\"base\": [], \"finetuned\": []}\neval_scores = {\"base\": [], \"finetuned\": []}\n\nfor model_type, use_ft in [(\"finetuned\", True), (\"base\", False)]:\n    print(f\"\\n{'='*60}\")\n    print(f\"Evaluating model: {'Fine-tuned (LoRA)' if use_ft else 'Base (Zero-shot)'}\")\n    print(f\"{'='*60}\")\n\n    for q_info in tqdm(eval_queries, desc=model_type):\n        result = rag_query_local(q_info[\"query\"], k=5, use_finetuned=use_ft)\n        scores = evaluate_response(result, q_info)\n\n        eval_results[model_type].append(result)\n        eval_scores[model_type].append(scores)\n\n        torch.cuda.empty_cache()\n\n# Summarize evaluation results\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Evaluation Results Summary\")\nprint(\"=\" * 60)\n\nmetrics = [\"json_valid\", \"fields_complete\", \"types_correct\", \"citation_accuracy\", \"risk_aligned\"]\nmetric_names = {\n    \"json_valid\": \"JSON Format Compliance\",\n    \"fields_complete\": \"Field Completeness\",\n    \"types_correct\": \"Type Correctness\",\n    \"citation_accuracy\": \"Citation Accuracy\",\n    \"risk_aligned\": \"Risk Level Alignment\",\n}\n\nprint(f\"\\n{'Metric':<20} {'Base Model':>12} {'Fine-tuned':>12} {'Improvement':>10}\")\nprint(\"-\" * 56)\n\nfor metric in metrics:\n    base_avg = np.mean([s[metric] for s in eval_scores[\"base\"]])\n    ft_avg = np.mean([s[metric] for s in eval_scores[\"finetuned\"]])\n    diff = ft_avg - base_avg\n    print(f\"{metric_names[metric]:<20} {base_avg:>11.1%} {ft_avg:>11.1%} {diff:>+9.1%}\")\n\n# Response detail level\nbase_len = np.mean([s[\"response_length\"] for s in eval_scores[\"base\"]])\nft_len = np.mean([s[\"response_length\"] for s in eval_scores[\"finetuned\"]])\nprint(f\"{'Avg Response Length':<20} {base_len:>11.0f} {ft_len:>11.0f} {ft_len-base_len:>+9.0f}\")"
  },
  {
   "cell_type": "markdown",
   "id": "3b89db19",
   "metadata": {},
   "source": "### Visualization Comparison\n\nGenerate two sets of visualizations:\n1. **Quality Metrics Bar Chart**: Compare 5 evaluation dimensions\n2. **Response Detail Box Plot**: Compare average response lengths\n3. **Per-Query Comparison Table**: Detailed scores for each query"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b04e7bc",
   "metadata": {},
   "outputs": [],
   "source": "# --------------------------------------------\n# 8.3 Visualization Comparison\n# ‚ö†Ô∏è Run on Google Colab\n# --------------------------------------------\n\nimport matplotlib.pyplot as plt\nimport matplotlib\nmatplotlib.rcParams['figure.dpi'] = 120\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# === Chart 1: Quality Metrics Comparison ===\nax1 = axes[0]\nx = np.arange(len(metrics))\nwidth = 0.35\n\nbase_scores_avg = [np.mean([s[m] for s in eval_scores[\"base\"]]) for m in metrics]\nft_scores_avg = [np.mean([s[m] for s in eval_scores[\"finetuned\"]]) for m in metrics]\n\nbars1 = ax1.bar(x - width/2, base_scores_avg, width, label='Base (Zero-shot)', color='#FF6B6B', alpha=0.8)\nbars2 = ax1.bar(x + width/2, ft_scores_avg, width, label='Fine-tuned (QLoRA)', color='#4ECDC4', alpha=0.8)\n\nax1.set_xlabel('Evaluation Metrics')\nax1.set_ylabel('Score')\nax1.set_title('Model Quality Comparison')\nax1.set_xticks(x)\nax1.set_xticklabels(['JSON\\nValid', 'Fields\\nComplete', 'Types\\nCorrect', 'Citation\\nAccuracy', 'Risk\\nAligned'],\n                     fontsize=8)\nax1.set_ylim(0, 1.15)\nax1.legend(loc='upper right', fontsize=8)\n\n# Add value labels\nfor bar in bars1:\n    height = bar.get_height()\n    ax1.annotate(f'{height:.0%}', xy=(bar.get_x() + bar.get_width()/2, height),\n                xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom', fontsize=7)\nfor bar in bars2:\n    height = bar.get_height()\n    ax1.annotate(f'{height:.0%}', xy=(bar.get_x() + bar.get_width()/2, height),\n                xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom', fontsize=7)\n\n# === Chart 2: Response Detail Comparison ===\nax2 = axes[1]\nbase_lengths = [s[\"response_length\"] for s in eval_scores[\"base\"]]\nft_lengths = [s[\"response_length\"] for s in eval_scores[\"finetuned\"]]\n\nbp = ax2.boxplot([base_lengths, ft_lengths],\n                  labels=['Base (Zero-shot)', 'Fine-tuned (QLoRA)'],\n                  patch_artist=True,\n                  boxprops=dict(alpha=0.8))\nbp['boxes'][0].set_facecolor('#FF6B6B')\nbp['boxes'][1].set_facecolor('#4ECDC4')\n\nax2.set_ylabel('Response Length (chars)')\nax2.set_title('Response Detail Comparison')\n\n# Add mean markers\nax2.scatter([1, 2], [np.mean(base_lengths), np.mean(ft_lengths)],\n            color='black', marker='D', s=50, zorder=5, label='Mean')\nax2.legend(fontsize=8)\n\nplt.tight_layout()\n\n# Save charts\nos.makedirs(\"data\", exist_ok=True)\nplt.savefig(\"data/model_comparison.png\", dpi=150, bbox_inches='tight')\n\n# Also save to Google Drive\ndrive_data_dir = '/content/drive/MyDrive/lora_finetune_data'\nos.makedirs(drive_data_dir, exist_ok=True)\nplt.savefig(os.path.join(drive_data_dir, 'model_comparison.png'), dpi=150, bbox_inches='tight')\n\nplt.show()\nprint(\"\\nCharts saved to data/model_comparison.png\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230cdc55",
   "metadata": {},
   "outputs": [],
   "source": "# --------------------------------------------\n# 8.4 Per-Query Comparison Table\n# ‚ö†Ô∏è Run on Google Colab\n# --------------------------------------------\n\nprint(\"=\" * 90)\nprint(\"Per-Query Comparison Table\")\nprint(\"=\" * 90)\nprint(f\"{'#':<3} {'Query':<50} {'Model':<12} {'JSON':>5} {'Fields':>5} {'Types':>5} {'Citations':>6} {'Risk':>5}\")\nprint(\"-\" * 90)\n\nfor i, q_info in enumerate(eval_queries):\n    query_short = q_info[\"query\"][:48] + \"..\" if len(q_info[\"query\"]) > 48 else q_info[\"query\"]\n\n    base_s = eval_scores[\"base\"][i]\n    ft_s = eval_scores[\"finetuned\"][i]\n\n    print(f\"{i+1:<3} {query_short:<50} {'Base':<12} \"\n          f\"{'OK' if base_s['json_valid'] else 'FAIL':>5} \"\n          f\"{'OK' if base_s['fields_complete'] else 'FAIL':>5} \"\n          f\"{'OK' if base_s['types_correct'] else 'FAIL':>5} \"\n          f\"{base_s['citation_accuracy']:>5.0%} \"\n          f\"{'OK' if base_s['risk_aligned'] else 'FAIL':>5}\")\n\n    print(f\"{'':3} {'':50} {'LoRA':<12} \"\n          f\"{'OK' if ft_s['json_valid'] else 'FAIL':>5} \"\n          f\"{'OK' if ft_s['fields_complete'] else 'FAIL':>5} \"\n          f\"{'OK' if ft_s['types_correct'] else 'FAIL':>5} \"\n          f\"{ft_s['citation_accuracy']:>5.0%} \"\n          f\"{'OK' if ft_s['risk_aligned'] else 'FAIL':>5}\")\n    print()\n\n# Final summary\nprint(\"=\" * 60)\nprint(\"Phase 8 Evaluation Complete\")\nprint(\"=\" * 60)\n\nbase_overall = np.mean([np.mean([s[m] for m in metrics]) for s in eval_scores[\"base\"]])\nft_overall = np.mean([np.mean([s[m] for m in metrics]) for s in eval_scores[\"finetuned\"]])\n\nprint(f\"\\n  Base model overall score:    {base_overall:.1%}\")\nprint(f\"  Fine-tuned model overall score:    {ft_overall:.1%}\")\nprint(f\"  Overall improvement:            {ft_overall - base_overall:+.1%}\")\n\nprint(f\"\\nProject complete:\")\nprint(f\"  Phase 1-4: RAG System (Qwen2.5-7B-Instruct, open-source)\")\nprint(f\"  Phase 5:   Training Data Preparation (Colab, free)\")\nprint(f\"  Phase 6:   QLoRA Fine-tuning (Colab, free)\")\nprint(f\"  Phase 7:   Open-source Model Integration (Colab, free)\")\nprint(f\"  Phase 8:   Evaluation & Comparison (Colab, free)\")\nprint(f\"\\nAll phases completed, entire pipeline uses open-source models, zero cost ‚úì\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
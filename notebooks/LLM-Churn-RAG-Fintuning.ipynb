{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbe558c5-75e4-40fa-bbd1-a227d701879e",
   "metadata": {},
   "source": [
    "# LLM Customer Churn Insight Tool\n",
    "## åŸºäº LLM + RAG çš„å®¢æˆ·æµå¤±æ´å¯Ÿåˆ†æç³»ç»Ÿ\n",
    "\n",
    "---\n",
    "\n",
    "# Introduction\n",
    "\n",
    "## 1.1 é¡¹ç›®èƒŒæ™¯\n",
    "\n",
    "åœ¨ç”µä¿¡è¡Œä¸šï¼Œ**å®¢æˆ·æµå¤± (Customer Churn)** æ˜¯ä¸€ä¸ªå…³é”®çš„ä¸šåŠ¡é—®é¢˜ã€‚è·å–æ–°å®¢æˆ·çš„æˆæœ¬é€šå¸¸æ˜¯ç•™ä½ç°æœ‰å®¢æˆ·çš„ 5-7 å€ï¼Œå› æ­¤ç†è§£å®¢æˆ·æµå¤±çš„åŸå› å¹¶é‡‡å–é¢„é˜²æªæ–½å…·æœ‰é‡è¦çš„å•†ä¸šä»·å€¼ã€‚\n",
    "\n",
    "ä¼ ç»Ÿçš„æµå¤±åˆ†æä¸»è¦ä¾èµ–**ç»“æ„åŒ–æ•°æ®**ï¼ˆå¦‚åœ¨ç½‘æ—¶é•¿ã€æ¶ˆè´¹é‡‘é¢ã€æœåŠ¡ç±»å‹ç­‰ï¼‰è¿›è¡Œç»Ÿè®¡åˆ†ææˆ–æœºå™¨å­¦ä¹ å»ºæ¨¡ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•å¿½ç•¥äº†ä¸€ä¸ªé‡è¦çš„ä¿¡æ¯æ¥æºï¼š**å®¢æˆ·åé¦ˆæ–‡æœ¬**ã€‚å®¢æˆ·åœ¨åé¦ˆä¸­è¡¨è¾¾çš„ä¸æ»¡ã€æŠ±æ€¨å’Œå»ºè®®ï¼Œå¾€å¾€èƒ½æ­ç¤ºæµå¤±çš„æ·±å±‚åŸå› ã€‚\n",
    "\n",
    "æœ¬é¡¹ç›®ç»“åˆ**å¤§è¯­è¨€æ¨¡å‹ (LLM)** å’Œ **æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG)** æŠ€æœ¯ï¼Œæ„å»ºä¸€ä¸ªèƒ½å¤Ÿç†è§£å®¢æˆ·åé¦ˆã€å›ç­”ä¸šåŠ¡é—®é¢˜çš„æ™ºèƒ½åˆ†æç³»ç»Ÿã€‚\n",
    "\n",
    "## 1.2 é¡¹ç›®ç›®æ ‡\n",
    "\n",
    "æ„å»ºä¸€ä¸ª **LLM é©±åŠ¨çš„å®¢æˆ·æ´å¯Ÿå·¥å…·**ï¼Œèƒ½å¤Ÿï¼š\n",
    "\n",
    "1. **æ£€ç´¢ç›¸å…³ä¿¡æ¯**ï¼šæ ¹æ®ç”¨æˆ·çš„æŸ¥è¯¢ï¼Œä»å®¢æˆ·åé¦ˆåº“ä¸­æ£€ç´¢æœ€ç›¸å…³çš„å®¢æˆ·æ•°æ®\n",
    "2. **ç”Ÿæˆæ ¹å› åˆ†æ**ï¼šè¯†åˆ«å®¢æˆ·æµå¤±çš„ä¸»è¦åŸå› ï¼ˆRoot Cause Analysisï¼‰\n",
    "3. **è¯„ä¼°é£é™©ç­‰çº§**ï¼šåˆ¤æ–­é—®é¢˜çš„ä¸¥é‡ç¨‹åº¦ï¼ˆChurn Risk Assessmentï¼‰\n",
    "4. **æä¾›è¡ŒåŠ¨å»ºè®®**ï¼šç»™å‡ºå…·ä½“ã€å¯æ‰§è¡Œçš„æ”¹è¿›æªæ–½ï¼ˆActionable Recommendationsï¼‰\n",
    "5. **æ”¯æŒå¼•ç”¨è¿½æº¯**ï¼šæ‰€æœ‰ç»“è®ºéƒ½æœ‰å…·ä½“çš„å®¢æˆ· ID ä½œä¸ºè¯æ®æ”¯æŒï¼ˆCitationsï¼‰\n",
    "\n",
    "## 1.3 æŠ€æœ¯æ–¹æ¡ˆ\n",
    "\n",
    "æœ¬é¡¹ç›®é‡‡ç”¨ **RAG (Retrieval-Augmented Generation)** æ¶æ„ï¼š\n",
    "\n",
    "```mermaid\n",
    "%%{init: {'theme': 'dark', 'themeVariables': { 'fontSize': '14px', 'fontFamily': 'arial', 'primaryColor': '#1a1a2e', 'primaryTextColor': '#fff', 'primaryBorderColor': '#4a9eff', 'lineColor': '#4a9eff'}}}%%\n",
    "flowchart LR\n",
    "    subgraph Input[\" \"]\n",
    "        A[/\"ğŸ” ç”¨æˆ·æŸ¥è¯¢\"/]\n",
    "    end\n",
    "\n",
    "    subgraph Retrieval[\"æ··åˆæ£€ç´¢\"]\n",
    "        direction TB\n",
    "        B1[\"ğŸ“Š å‘é‡æ£€ç´¢<br/>è¯­ä¹‰ç›¸ä¼¼åº¦\"]\n",
    "        B2[\"ğŸ”¤ BM25æ£€ç´¢<br/>å…³é”®è¯åŒ¹é…\"]\n",
    "    end\n",
    "\n",
    "    subgraph Process[\" \"]\n",
    "        C[(\"ğŸ“„ Top-K<br/>ç›¸å…³æ–‡æ¡£\")]\n",
    "        D[\"ğŸ¤– LLMåˆ†æ<br/>Qwen2.5-7B\"]\n",
    "    end\n",
    "\n",
    "    subgraph Output[\" \"]\n",
    "        E[\"ğŸ“Š åˆ†ææŠ¥å‘Š<br/>æ‘˜è¦+å»ºè®®+å¼•ç”¨\"]\n",
    "    end\n",
    "\n",
    "    A --> B1 & B2\n",
    "    B1 & B2 --> C\n",
    "    C --> D\n",
    "    D --> E\n",
    "\n",
    "    style Input fill:#0f3460,stroke:#ffd700,stroke-width:2px\n",
    "    style Retrieval fill:#16213e,stroke:#00d9ff,stroke-width:2px\n",
    "    style Process fill:#1a1a2e,stroke:#00ff88,stroke-width:2px\n",
    "    style Output fill:#3d1a4a,stroke:#bf7fff,stroke-width:2px\n",
    "    style A fill:#0f3460,stroke:#ffd700,color:#fff\n",
    "    style B1 fill:#1e3a5f,stroke:#00ff88,color:#fff\n",
    "    style B2 fill:#1e3a5f,stroke:#00ff88,color:#fff\n",
    "    style C fill:#2d4a3e,stroke:#00ff88,color:#fff\n",
    "    style D fill:#4a1942,stroke:#ff6b9d,color:#fff\n",
    "    style E fill:#3d1a4a,stroke:#bf7fff,color:#fff\n",
    "```\n",
    "\n",
    "### ä¸ºä»€ä¹ˆé€‰æ‹© RAGï¼Ÿ\n",
    "\n",
    "| æ–¹æ¡ˆ | ä¼˜ç‚¹ | ç¼ºç‚¹ |\n",
    "|------|------|------|\n",
    "| **ç›´æ¥é—® LLM** | ç®€å•å¿«é€Ÿ | æ— æ³•è®¿é—®ç§æœ‰æ•°æ®ï¼Œå¯èƒ½äº§ç”Ÿå¹»è§‰ |\n",
    "| **Fine-tuning** | æ¨¡å‹å†…åŒ–é¢†åŸŸçŸ¥è¯† | æˆæœ¬é«˜ï¼Œæ•°æ®æ›´æ–°éœ€é‡æ–°è®­ç»ƒ |\n",
    "| **RAG** | åŸºäºçœŸå®æ•°æ®ï¼Œå¯è¿½æº¯ï¼Œæ˜“æ›´æ–° | éœ€è¦æ„å»ºæ£€ç´¢ç³»ç»Ÿ |\n",
    "\n",
    "RAG æ˜¯ç›®å‰ä¼ä¸šçº§ LLM åº”ç”¨çš„ä¸»æµæ–¹æ¡ˆï¼Œç‰¹åˆ«é€‚åˆéœ€è¦**åŸºäºç§æœ‰æ•°æ®è¿›è¡Œåˆ†æ**çš„åœºæ™¯ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "# 2. æ•°æ®ä»‹ç»\n",
    "\n",
    "## 2.1 æ•°æ®æ¥æº\n",
    "\n",
    "æœ¬é¡¹ç›®ä½¿ç”¨ Kaggle ä¸Šçš„ **Telco Customer Churn with Realistic Customer Feedback** æ•°æ®é›†ã€‚\n",
    "\n",
    "- **æ•°æ®é“¾æ¥**ï¼š[https://www.kaggle.com/datasets/beatafaron/telco-customer-churn-realistic-customer-feedback](https://www.kaggle.com/datasets/beatafaron/telco-customer-churn-realistic-customer-feedback)\n",
    "- **è®¸å¯è¯**ï¼šCC BY 4.0 (Attribution 4.0 International)\n",
    "- **æ•°æ®è§„æ¨¡**ï¼š7,043 ä¸ªå®¢æˆ·è®°å½•\n",
    "\n",
    "è¯¥æ•°æ®é›†åŸºäºç»å…¸çš„ IBM Telco Customer Churn æ•°æ®é›†ï¼Œå¹¶**ä½¿ç”¨ LLM ä¸ºæ¯ä¸ªå®¢æˆ·ç”Ÿæˆäº†æ¨¡æ‹Ÿçš„åé¦ˆæ–‡æœ¬**ï¼Œä½¿å¾—æ•°æ®é›†åŒæ—¶åŒ…å«ç»“æ„åŒ–æ•°æ®å’Œéç»“æ„åŒ–æ–‡æœ¬ã€‚\n",
    "\n",
    "## 2.2 æ•°æ®æ–‡ä»¶è¯´æ˜\n",
    "\n",
    "ä¸‹è½½çš„æ•°æ®åŒ…å«ä»¥ä¸‹æ–‡ä»¶ï¼š\n",
    "\n",
    "| æ–‡ä»¶å | å¤§å° | è¡Œæ•° | è¯´æ˜ |\n",
    "|--------|------|------|------|\n",
    "| `telco_churn_with_all_feedback.csv` | 5.6 MB | 7,043 | **ä¸»æ•°æ®æ–‡ä»¶**ï¼ŒåŒ…å«å®Œæ•´çš„å®¢æˆ·ä¿¡æ¯å’Œåé¦ˆ |\n",
    "| `telco_prep.csv` | 5.8 MB | 7,032 | é¢„å¤„ç†ç‰ˆæœ¬ï¼Œå€¼å·²è½¬ä¸ºå°å†™ï¼Œæ–°å¢ `feedback_length` å’Œ `sentiment` åˆ— |\n",
    "| `telco_noisy_feedback_prep.csv` | 1.7 MB | 7,032 | éƒ¨åˆ†åé¦ˆç¼ºå¤±ç‰ˆæœ¬ï¼Œç”¨äºæµ‹è¯•ç³»ç»Ÿé²æ£’æ€§ |\n",
    "| `model_with_feedback.pkl` | 27 KB | - | é¢„è®­ç»ƒçš„æƒ…æ„Ÿåˆ†ææ¨¡å‹ï¼ˆpickle æ ¼å¼ï¼‰|\n",
    "\n",
    "**æœ¬é¡¹ç›®ä½¿ç”¨ `telco_churn_with_all_feedback.csv` ä½œä¸ºä¸»æ•°æ®æº**ï¼Œå› ä¸ºå®ƒåŒ…å«æ‰€æœ‰å®¢æˆ·çš„å®Œæ•´åé¦ˆã€‚\n",
    "\n",
    "## 2.3 å˜é‡è¯¦è§£\n",
    "\n",
    "### 2.3.1 å®¢æˆ·æ ‡è¯†\n",
    "\n",
    "| å˜é‡å | ç±»å‹ | è¯´æ˜ | ç¤ºä¾‹ |\n",
    "|--------|------|------|------|\n",
    "| `customerID` | String | å®¢æˆ·å”¯ä¸€æ ‡è¯†ç¬¦ | \"7590-VHVEG\" |\n",
    "\n",
    "### 2.3.2 äººå£ç»Ÿè®¡ä¿¡æ¯ (Demographics)\n",
    "\n",
    "| å˜é‡å | ç±»å‹ | å–å€¼ | è¯´æ˜ |\n",
    "|--------|------|------|------|\n",
    "| `gender` | String | Female, Male | å®¢æˆ·æ€§åˆ« |\n",
    "| `SeniorCitizen` | Integer | 0, 1 | æ˜¯å¦ä¸ºè€å¹´å®¢æˆ·ï¼ˆ65å²ä»¥ä¸Šï¼‰<br>0 = å¦ï¼Œ1 = æ˜¯ |\n",
    "| `Partner` | String | Yes, No | æ˜¯å¦æœ‰é…å¶/ä¼´ä¾£ |\n",
    "| `Dependents` | String | Yes, No | æ˜¯å¦æœ‰å®¶å±ï¼ˆå¦‚å­å¥³ï¼‰ |\n",
    "\n",
    "### 2.3.3 è´¦æˆ·ä¿¡æ¯ (Account Information)\n",
    "\n",
    "| å˜é‡å | ç±»å‹ | å–å€¼/èŒƒå›´ | è¯´æ˜ |\n",
    "|--------|------|-----------|------|\n",
    "| `tenure` | Integer | 0 - 72 | åœ¨ç½‘æ—¶é•¿ï¼ˆæœˆæ•°ï¼‰<br>0 è¡¨ç¤ºå½“æœˆæ–°å¼€æˆ· |\n",
    "| `Contract` | String | Month-to-month,<br>One year,<br>Two year | åˆåŒç±»å‹<br>æœˆä»˜ã€ä¸€å¹´æœŸã€ä¸¤å¹´æœŸ |\n",
    "| `PaperlessBilling` | String | Yes, No | æ˜¯å¦ä½¿ç”¨ç”µå­è´¦å• |\n",
    "| `PaymentMethod` | String | Electronic check,<br>Mailed check,<br>Bank transfer (automatic),<br>Credit card (automatic) | ä»˜æ¬¾æ–¹å¼ |\n",
    "\n",
    "### 2.3.4 æœåŠ¡è®¢é˜… (Services)\n",
    "\n",
    "| å˜é‡å | ç±»å‹ | å–å€¼ | è¯´æ˜ |\n",
    "|--------|------|------|------|\n",
    "| `PhoneService` | String | Yes, No | æ˜¯å¦è®¢é˜…ç”µè¯æœåŠ¡ |\n",
    "| `MultipleLines` | String | Yes, No,<br>No phone service | æ˜¯å¦æœ‰å¤šæ¡ç”µè¯çº¿ |\n",
    "| `InternetService` | String | DSL,<br>Fiber optic,<br>No | äº’è”ç½‘æœåŠ¡ç±»å‹<br>DSLã€å…‰çº¤ã€æ—  |\n",
    "| `OnlineSecurity` | String | Yes, No,<br>No internet service | åœ¨çº¿å®‰å…¨æœåŠ¡ |\n",
    "| `OnlineBackup` | String | Yes, No,<br>No internet service | åœ¨çº¿å¤‡ä»½æœåŠ¡ |\n",
    "| `DeviceProtection` | String | Yes, No,<br>No internet service | è®¾å¤‡ä¿æŠ¤æœåŠ¡ |\n",
    "| `TechSupport` | String | Yes, No,<br>No internet service | æŠ€æœ¯æ”¯æŒæœåŠ¡ |\n",
    "| `StreamingTV` | String | Yes, No,<br>No internet service | æµåª’ä½“ç”µè§†æœåŠ¡ |\n",
    "| `StreamingMovies` | String | Yes, No,<br>No internet service | æµåª’ä½“ç”µå½±æœåŠ¡ |\n",
    "\n",
    "### 2.3.5 è´¹ç”¨ä¿¡æ¯ (Charges)\n",
    "\n",
    "| å˜é‡å | ç±»å‹ | èŒƒå›´ | è¯´æ˜ |\n",
    "|--------|------|------|------|\n",
    "| `MonthlyCharges` | Float | $18.25 - $118.75 | æœˆè´¹ï¼ˆç¾å…ƒï¼‰ |\n",
    "| `TotalCharges` | String* | - | ç´¯è®¡è´¹ç”¨ï¼ˆç¾å…ƒï¼‰<br>*æ³¨æ„ï¼šåŸå§‹æ•°æ®ä¸ºå­—ç¬¦ä¸²ç±»å‹ï¼Œéœ€è½¬æ¢ |\n",
    "\n",
    "### 2.3.6 ç›®æ ‡å˜é‡ (Target)\n",
    "\n",
    "| å˜é‡å | ç±»å‹ | å–å€¼ | è¯´æ˜ |\n",
    "|--------|------|------|------|\n",
    "| `Churn` | String | Yes, No | **æ˜¯å¦æµå¤±**<br>Yes = å·²æµå¤±ï¼ˆç¦»ç½‘ï¼‰<br>No = æœªæµå¤±ï¼ˆåœ¨ç½‘ï¼‰ |\n",
    "\n",
    "### 2.3.7 æ–‡æœ¬æ•°æ® (Text)\n",
    "\n",
    "| å˜é‡å | ç±»å‹ | è¯´æ˜ |\n",
    "|--------|------|------|\n",
    "| `PromptInput` | String | ç”¨äºç”Ÿæˆåé¦ˆçš„ LLM æç¤ºè¯ï¼ˆæœ¬é¡¹ç›®ä¸ä½¿ç”¨ï¼‰ |\n",
    "| `CustomerFeedback` | String | **å®¢æˆ·åé¦ˆæ–‡æœ¬**ï¼ˆæœ¬é¡¹ç›®çš„æ ¸å¿ƒæ•°æ®ï¼‰<br>é•¿åº¦çº¦ 277-840 å­—ç¬¦ |\n",
    "\n",
    "## 2.4 æ•°æ®ç‰¹ç‚¹\n",
    "\n",
    "1. **å®Œæ•´æ€§é«˜**ï¼šæ‰€æœ‰ 7,043 æ¡è®°å½•éƒ½æœ‰å®¢æˆ·åé¦ˆæ–‡æœ¬\n",
    "2. **æ ‡ç­¾æ˜ç¡®**ï¼šæµå¤±æ ‡ç­¾ (Churn) æ¸…æ™°ï¼Œä¾¿äºåˆ†æ\n",
    "3. **ä¿¡æ¯ä¸°å¯Œ**ï¼šç»“æ„åŒ–æ•°æ® + éç»“æ„åŒ–æ–‡æœ¬çš„ç»„åˆ\n",
    "4. **ç±»åˆ«ä¸å¹³è¡¡**ï¼šæµå¤±ç‡çº¦ 26.5%ï¼ˆ1,869 æµå¤± vs 5,174 ç•™å­˜ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "# 3. é¡¹ç›®ç»“æ„\n",
    "\n",
    "```\n",
    "LLM-Project/\n",
    "â”œâ”€â”€ data/                                    # æ•°æ®ç›®å½•\n",
    "â”‚   â”œâ”€â”€ telco_churn_with_all_feedback.csv   # ä¸»æ•°æ®æ–‡ä»¶\n",
    "â”‚   â”œâ”€â”€ telco_prep.csv                      # é¢„å¤„ç†æ•°æ®\n",
    "â”‚   â”œâ”€â”€ telco_noisy_feedback_prep.csv       # å™ªå£°åé¦ˆæ•°æ®\n",
    "â”‚   â”œâ”€â”€ model_with_feedback.pkl             # é¢„è®­ç»ƒæ¨¡å‹\n",
    "â”‚   â”œâ”€â”€ faiss_index.bin                     # FAISS å‘é‡ç´¢å¼•ï¼ˆè¿è¡Œåç”Ÿæˆï¼‰\n",
    "â”‚   â””â”€â”€ index_data.pkl                      # ç´¢å¼•å…ƒæ•°æ®ï¼ˆè¿è¡Œåç”Ÿæˆï¼‰\n",
    "â”œâ”€â”€ venv/                                    # Python è™šæ‹Ÿç¯å¢ƒ\n",
    "â”œâ”€â”€ LLM-Churn-RAG-Fintuning.ipynb           # ä¸» Notebookï¼ˆæœ¬æ–‡ä»¶ï¼‰\n",
    "â”œâ”€â”€ Using_tool_required_for_customer_service.ipynb  # å‚è€ƒç¤ºä¾‹ä»£ç \n",
    "â””â”€â”€ README.md                               # é¡¹ç›®è¯´æ˜æ–‡æ¡£\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 4. æ ¸å¿ƒæœ¯è¯­è¡¨\n",
    "\n",
    "| æœ¯è¯­ | è‹±æ–‡å…¨ç§° | è§£é‡Š |\n",
    "|------|----------|------|\n",
    "| **RAG** | Retrieval-Augmented Generation | æ£€ç´¢å¢å¼ºç”Ÿæˆã€‚å…ˆä»çŸ¥è¯†åº“æ£€ç´¢ç›¸å…³ä¿¡æ¯ï¼Œå†è®© LLM åŸºäºè¿™äº›ä¿¡æ¯ç”Ÿæˆå›ç­”ï¼Œé¿å…\"å¹»è§‰\" |\n",
    "| **Embedding** | Vector Embedding | å‘é‡åµŒå…¥ã€‚å°†æ–‡æœ¬è½¬æ¢ä¸ºé«˜ç»´å‘é‡ï¼Œè¯­ä¹‰ç›¸ä¼¼çš„æ–‡æœ¬åœ¨å‘é‡ç©ºé—´ä¸­è·ç¦»æ›´è¿‘ |\n",
    "| **FAISS** | Facebook AI Similarity Search | Meta å¼€å‘çš„é«˜æ•ˆå‘é‡ç›¸ä¼¼åº¦æœç´¢åº“ï¼Œæ”¯æŒåäº¿çº§å‘é‡çš„å¿«é€Ÿæ£€ç´¢ |\n",
    "| **BM25** | Best Matching 25 | ç»å…¸çš„å…³é”®è¯æ£€ç´¢ç®—æ³•ï¼ŒåŸºäºè¯é¢‘å’Œæ–‡æ¡£é¢‘ç‡è®¡ç®—ç›¸å…³æ€§ |\n",
    "| **RRF** | Reciprocal Rank Fusion | æ’åå€’æ•°èåˆã€‚å°†å¤šä¸ªæ£€ç´¢ç»“æœåˆ—è¡¨åˆå¹¶çš„ç®—æ³• |\n",
    "| **Chunk** | Text Chunk | æ–‡æœ¬åˆ†å—ã€‚å°†é•¿æ–‡æ¡£åˆ‡åˆ†ä¸ºå°æ®µï¼Œä¾¿äºæ£€ç´¢å’Œå¤„ç† |\n",
    "| **Cosine Similarity** | - | ä½™å¼¦ç›¸ä¼¼åº¦ã€‚è¡¡é‡ä¸¤ä¸ªå‘é‡æ–¹å‘çš„ç›¸ä¼¼ç¨‹åº¦ï¼Œå€¼åŸŸ [-1, 1] |\n",
    "| **Prompt Engineering** | - | æç¤ºå·¥ç¨‹ã€‚è®¾è®¡æœ‰æ•ˆçš„æç¤ºè¯æ¥å¼•å¯¼ LLM äº§ç”ŸæœŸæœ›çš„è¾“å‡º |\n",
    "| **Hallucination** | - | å¹»è§‰ã€‚LLM ç”Ÿæˆçœ‹ä¼¼åˆç†ä½†å®é™…ä¸å­˜åœ¨æˆ–é”™è¯¯çš„ä¿¡æ¯ |\n",
    "| **Citation** | - | å¼•ç”¨ã€‚æŒ‡å‘æ”¯æŒæŸä¸ªç»“è®ºçš„å…·ä½“æ•°æ®æ¥æº |\n",
    "\n",
    "---\n",
    "\n",
    "# 5. è¿è¡Œç¯å¢ƒ\n",
    "\n",
    "## 5.1 ç¯å¢ƒè¦æ±‚\n",
    "\n",
    "- Python 3.9+\n",
    "- çº¦ 2GB ç£ç›˜ç©ºé—´ï¼ˆç”¨äºæ¨¡å‹å’Œç´¢å¼•ï¼‰\n",
    "- Google Colabï¼ˆå…è´¹ T4 GPUï¼‰\n",
    "\n",
    "## 5.2 ä¾èµ–åº“\n",
    "\n",
    "```\n",
    "pandas>=2.0\n",
    "numpy>=1.24\n",
    "sentence-transformers>=2.2\n",
    "faiss-cpu>=1.7\n",
    "rank_bm25>=0.2\n",
    "transformers>=4.36\n",
    "peft>=0.7\n",
    "bitsandbytes>=0.41\n",
    "accelerate>=0.25\n",
    "```\n",
    "\n",
    "## 5.3 å¯åŠ¨æ–¹å¼\n",
    "\n",
    "```bash\n",
    "# 1. è¿›å…¥é¡¹ç›®ç›®å½•\n",
    "cd /Users/ricky/Desktop/LLM-Project\n",
    "\n",
    "# 2. æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ\n",
    "source venv/bin/activate\n",
    "\n",
    "# 3. å¯åŠ¨ Jupyter\n",
    "jupyter notebook\n",
    "\n",
    "# 4. åœ¨ Jupyter ä¸­é€‰æ‹© Kernel: \"LLM Project (venv)\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 6. é¡¹ç›®é˜¶æ®µæ¦‚è§ˆ\n",
    "\n",
    "| Phase | åç§° | å†…å®¹ | å…³é”®æŠ€æœ¯ |\n",
    "|-------|------|------|----------|\n",
    "| **Phase 1** | æ•°æ®å‡†å¤‡ | åŠ è½½ã€æ¸…æ´—ã€æ„å»ºæ–‡æ¡£ | Pandas, æ•°æ®é¢„å¤„ç† |\n",
    "| **Phase 2** | ç´¢å¼•ä¸æ£€ç´¢ | å‘é‡åŒ–ã€å»ºç´¢å¼•ã€æ··åˆæ£€ç´¢ | Sentence-Transformers, FAISS, BM25, RRF |\n",
    "| **Phase 3** | LLM é›†æˆ | Prompt è®¾è®¡ã€RAG Pipeline | Qwen2.5-7B-Instruct, Prompt Engineering |\n",
    "| **Phase 4** | è¾“å‡ºä¸è¯„ä¼° | æ ¼å¼åŒ–è¾“å‡ºã€å¼•ç”¨éªŒè¯ | JSON è§£æ, è´¨é‡è¯„ä¼° |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rubovs4n3g",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Phase 1: æ•°æ®å‡†å¤‡ (Data Ingestion)\n",
    "\n",
    "åœ¨ RAG ç³»ç»Ÿä¸­ï¼Œ**æ•°æ®è´¨é‡ç›´æ¥å†³å®šäº†æœ€ç»ˆè¾“å‡ºçš„è´¨é‡**ã€‚Phase 1 çš„ç›®æ ‡æ˜¯ï¼š\n",
    "\n",
    "1. åŠ è½½åŸå§‹æ•°æ®ï¼ˆç»“æ„åŒ– + éç»“æ„åŒ–ï¼‰\n",
    "2. æ•°æ®æ¢ç´¢ä¸è´¨é‡æ£€æŸ¥\n",
    "3. æ•°æ®æ¸…æ´—ä¸é¢„å¤„ç†\n",
    "4. æ„å»ºé€‚åˆæ£€ç´¢çš„æ–‡æ¡£æ ¼å¼\n",
    "\n",
    "## 1.0 ç¯å¢ƒé…ç½®\n",
    "\n",
    "é¦–å…ˆå¯¼å…¥å¿…è¦çš„ Python åº“å¹¶é…ç½®è¿è¡Œç¯å¢ƒã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a88ebd8e-6127-44b7-929b-91b78ed8706a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç¯å¢ƒé…ç½®å®Œæˆ âœ“\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# LLM Customer Churn Insight Tool\n",
    "# LLM + RAG + Customer Feedback Analysis\n",
    "# ============================================\n",
    "\n",
    "# Phase 1: æ•°æ®å‡†å¤‡ (Data Ingestion)\n",
    "# Phase 2: ç´¢å¼•ä¸æ£€ç´¢ (Indexing & Retrieval)\n",
    "# Phase 3: LLM é›†æˆ (LLM Integration)\n",
    "# Phase 4: è¾“å‡ºä¸è¯„ä¼° (Output & Evaluation)\n",
    "\n",
    "# --------------------------------------------\n",
    "# 1.0 ç¯å¢ƒé…ç½®ä¸ä¾èµ–å¯¼å…¥\n",
    "# --------------------------------------------\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è®¾ç½®æ˜¾ç¤ºé€‰é¡¹\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "print(\"ç¯å¢ƒé…ç½®å®Œæˆ âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8gdwlov79f",
   "metadata": {},
   "source": [
    "## 1.1 åŠ è½½æ•°æ®\n",
    "\n",
    "æˆ‘ä»¬ä½¿ç”¨çš„æ˜¯ **Telco Customer Churn** æ•°æ®é›†ï¼ŒåŒ…å«ï¼š\n",
    "\n",
    "- **ç»“æ„åŒ–æ•°æ®**ï¼šå®¢æˆ·äººå£ç»Ÿè®¡ä¿¡æ¯ã€è®¢é˜…æœåŠ¡ã€è´¦å•ä¿¡æ¯ã€æµå¤±æ ‡ç­¾\n",
    "- **éç»“æ„åŒ–æ•°æ®**ï¼šå®¢æˆ·åé¦ˆæ–‡æœ¬ï¼ˆCustomerFeedbackï¼‰\n",
    "\n",
    "æ•°æ®æ¥æºï¼š[Kaggle - Telco Customer Churn with Realistic Feedback](https://www.kaggle.com/datasets/beatafaron/telco-customer-churn-realistic-customer-feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b433de6-f0ba-4ed5-8a01-a25c1a70b7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ•°æ®é›†å¤§å°: 7043 è¡Œ, 23 åˆ—\n",
      "\n",
      "åˆ—å:\n",
      "['customerID', 'gender', 'SeniorCitizen', 'Partner', 'Dependents', 'tenure', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod', 'MonthlyCharges', 'TotalCharges', 'Churn', 'PromptInput', 'CustomerFeedback']\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# 1.1 åŠ è½½æ•°æ®\n",
    "# --------------------------------------------\n",
    "\n",
    "# æ•°æ®è·¯å¾„\n",
    "DATA_DIR = './data'\n",
    "\n",
    "# åŠ è½½ä¸»æ•°æ®æ–‡ä»¶ (åŒ…å«å®¢æˆ·ä¿¡æ¯ + åé¦ˆ)\n",
    "df_main = pd.read_csv(f'{DATA_DIR}/telco_churn_with_all_feedback.csv')\n",
    "\n",
    "print(f\"æ•°æ®é›†å¤§å°: {df_main.shape[0]} è¡Œ, {df_main.shape[1]} åˆ—\")\n",
    "print(f\"\\nåˆ—å:\\n{df_main.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0iz7mbfxhbkl",
   "metadata": {},
   "source": [
    "### è¾“å‡ºåˆ†æ\n",
    "\n",
    "æ•°æ®é›†åŒ…å« **7,043 ä¸ªå®¢æˆ·è®°å½•**å’Œ **23 ä¸ªç‰¹å¾åˆ—**ï¼š\n",
    "\n",
    "| ç±»åˆ« | å­—æ®µ |\n",
    "|------|------|\n",
    "| **å®¢æˆ·æ ‡è¯†** | customerID |\n",
    "| **äººå£ç»Ÿè®¡** | gender, SeniorCitizen, Partner, Dependents |\n",
    "| **è´¦æˆ·ä¿¡æ¯** | tenureï¼ˆåœ¨ç½‘æ—¶é•¿ï¼‰, Contract, PaperlessBilling, PaymentMethod |\n",
    "| **æœåŠ¡è®¢é˜…** | PhoneService, MultipleLines, InternetService, OnlineSecurity, OnlineBackup, DeviceProtection, TechSupport, StreamingTV, StreamingMovies |\n",
    "| **è´¹ç”¨** | MonthlyCharges, TotalCharges |\n",
    "| **ç›®æ ‡å˜é‡** | Churnï¼ˆæ˜¯å¦æµå¤±ï¼‰|\n",
    "| **æ–‡æœ¬æ•°æ®** | CustomerFeedbackï¼ˆå®¢æˆ·åé¦ˆï¼‰|\n",
    "\n",
    "---\n",
    "\n",
    "## 1.2 æ•°æ®æ¢ç´¢\n",
    "\n",
    "æŸ¥çœ‹æ•°æ®çš„å‰å‡ è¡Œï¼Œäº†è§£æ•°æ®çš„åŸºæœ¬ç»“æ„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "pg34hov0vng",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerID</th>\n",
       "      <th>gender</th>\n",
       "      <th>SeniorCitizen</th>\n",
       "      <th>Partner</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>tenure</th>\n",
       "      <th>PhoneService</th>\n",
       "      <th>MultipleLines</th>\n",
       "      <th>InternetService</th>\n",
       "      <th>OnlineSecurity</th>\n",
       "      <th>OnlineBackup</th>\n",
       "      <th>DeviceProtection</th>\n",
       "      <th>TechSupport</th>\n",
       "      <th>StreamingTV</th>\n",
       "      <th>StreamingMovies</th>\n",
       "      <th>Contract</th>\n",
       "      <th>PaperlessBilling</th>\n",
       "      <th>PaymentMethod</th>\n",
       "      <th>MonthlyCharges</th>\n",
       "      <th>TotalCharges</th>\n",
       "      <th>Churn</th>\n",
       "      <th>PromptInput</th>\n",
       "      <th>CustomerFeedback</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7590-VHVEG</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>No phone service</td>\n",
       "      <td>DSL</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Electronic check</td>\n",
       "      <td>29.85</td>\n",
       "      <td>29.85</td>\n",
       "      <td>No</td>\n",
       "      <td>Write a realistic customer feedback based on this profile:\\nChurn: No\\nTenure: 1 months\\nContract type: Month-to-month\\nMonthly Charges: $29.85\\nInternet Service: DSL\\nPayment Method: Electronic c...</td>\n",
       "      <td>I have been using the DSL internet service from this provider for the past month and so far, I am satisfied with the service. The connection has been reliable and the speed is sufficient for my ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5575-GNVDE</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>34</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>One year</td>\n",
       "      <td>No</td>\n",
       "      <td>Mailed check</td>\n",
       "      <td>56.95</td>\n",
       "      <td>1889.5</td>\n",
       "      <td>No</td>\n",
       "      <td>Write a realistic customer feedback based on this profile:\\nChurn: No\\nTenure: 34 months\\nContract type: One year\\nMonthly Charges: $56.95\\nInternet Service: DSL\\nPayment Method: Mailed check</td>\n",
       "      <td>I have been a customer with this company for over two and a half years now and I have been very satisfied with their service. The DSL internet has been reliable and the monthly charges are reasona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3668-QPYBK</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Mailed check</td>\n",
       "      <td>53.85</td>\n",
       "      <td>108.15</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Write a realistic customer feedback based on this profile:\\nChurn: Yes\\nTenure: 2 months\\nContract type: Month-to-month\\nMonthly Charges: $53.85\\nInternet Service: DSL\\nPayment Method: Mailed check</td>\n",
       "      <td>I recently signed up for DSL internet service with this provider two months ago on a month-to-month contract. Unfortunately, I have already decided to churn and switch to a different provider. The...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customerID  gender  SeniorCitizen Partner Dependents  tenure PhoneService  \\\n",
       "0  7590-VHVEG  Female              0     Yes         No       1           No   \n",
       "1  5575-GNVDE    Male              0      No         No      34          Yes   \n",
       "2  3668-QPYBK    Male              0      No         No       2          Yes   \n",
       "\n",
       "      MultipleLines InternetService OnlineSecurity OnlineBackup  \\\n",
       "0  No phone service             DSL             No          Yes   \n",
       "1                No             DSL            Yes           No   \n",
       "2                No             DSL            Yes          Yes   \n",
       "\n",
       "  DeviceProtection TechSupport StreamingTV StreamingMovies        Contract  \\\n",
       "0               No          No          No              No  Month-to-month   \n",
       "1              Yes          No          No              No        One year   \n",
       "2               No          No          No              No  Month-to-month   \n",
       "\n",
       "  PaperlessBilling     PaymentMethod  MonthlyCharges TotalCharges Churn  \\\n",
       "0              Yes  Electronic check           29.85        29.85    No   \n",
       "1               No      Mailed check           56.95       1889.5    No   \n",
       "2              Yes      Mailed check           53.85       108.15   Yes   \n",
       "\n",
       "                                                                                                                                                                                               PromptInput  \\\n",
       "0  Write a realistic customer feedback based on this profile:\\nChurn: No\\nTenure: 1 months\\nContract type: Month-to-month\\nMonthly Charges: $29.85\\nInternet Service: DSL\\nPayment Method: Electronic c...   \n",
       "1          Write a realistic customer feedback based on this profile:\\nChurn: No\\nTenure: 34 months\\nContract type: One year\\nMonthly Charges: $56.95\\nInternet Service: DSL\\nPayment Method: Mailed check   \n",
       "2    Write a realistic customer feedback based on this profile:\\nChurn: Yes\\nTenure: 2 months\\nContract type: Month-to-month\\nMonthly Charges: $53.85\\nInternet Service: DSL\\nPayment Method: Mailed check   \n",
       "\n",
       "                                                                                                                                                                                          CustomerFeedback  \n",
       "0  I have been using the DSL internet service from this provider for the past month and so far, I am satisfied with the service. The connection has been reliable and the speed is sufficient for my ne...  \n",
       "1  I have been a customer with this company for over two and a half years now and I have been very satisfied with their service. The DSL internet has been reliable and the monthly charges are reasona...  \n",
       "2  I recently signed up for DSL internet service with this provider two months ago on a month-to-month contract. Unfortunately, I have already decided to churn and switch to a different provider. The...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# 1.2 æ•°æ®æ¢ç´¢ - åŸºæœ¬ä¿¡æ¯\n",
    "# --------------------------------------------\n",
    "\n",
    "# æŸ¥çœ‹å‰å‡ è¡Œæ•°æ®\n",
    "df_main.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sg9uzruelkf",
   "metadata": {},
   "source": [
    "### æ•°æ®ç±»å‹æ£€æŸ¥\n",
    "\n",
    "æ£€æŸ¥æ¯åˆ—çš„æ•°æ®ç±»å‹å’Œç¼ºå¤±å€¼æƒ…å†µã€‚è¿™ä¸€æ­¥å¾ˆé‡è¦ï¼Œå› ä¸ºï¼š\n",
    "- æ•°å€¼å‹å­—æ®µå¯èƒ½è¢«é”™è¯¯åœ°è¯†åˆ«ä¸ºå­—ç¬¦ä¸²ï¼ˆå¦‚ TotalChargesï¼‰\n",
    "- ç¼ºå¤±å€¼éœ€è¦åœ¨åç»­å¤„ç†ä¸­å¡«å……æˆ–åˆ é™¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c6k94hdbds",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "æ•°æ®ç±»å‹:\n",
      "==================================================\n",
      "customerID           object\n",
      "gender               object\n",
      "SeniorCitizen         int64\n",
      "Partner              object\n",
      "Dependents           object\n",
      "tenure                int64\n",
      "PhoneService         object\n",
      "MultipleLines        object\n",
      "InternetService      object\n",
      "OnlineSecurity       object\n",
      "OnlineBackup         object\n",
      "DeviceProtection     object\n",
      "TechSupport          object\n",
      "StreamingTV          object\n",
      "StreamingMovies      object\n",
      "Contract             object\n",
      "PaperlessBilling     object\n",
      "PaymentMethod        object\n",
      "MonthlyCharges      float64\n",
      "TotalCharges         object\n",
      "Churn                object\n",
      "PromptInput          object\n",
      "CustomerFeedback     object\n",
      "dtype: object\n",
      "\n",
      "==================================================\n",
      "ç¼ºå¤±å€¼ç»Ÿè®¡:\n",
      "==================================================\n",
      "customerID          0\n",
      "gender              0\n",
      "SeniorCitizen       0\n",
      "Partner             0\n",
      "Dependents          0\n",
      "tenure              0\n",
      "PhoneService        0\n",
      "MultipleLines       0\n",
      "InternetService     0\n",
      "OnlineSecurity      0\n",
      "OnlineBackup        0\n",
      "DeviceProtection    0\n",
      "TechSupport         0\n",
      "StreamingTV         0\n",
      "StreamingMovies     0\n",
      "Contract            0\n",
      "PaperlessBilling    0\n",
      "PaymentMethod       0\n",
      "MonthlyCharges      0\n",
      "TotalCharges        0\n",
      "Churn               0\n",
      "PromptInput         0\n",
      "CustomerFeedback    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# æ•°æ®ç±»å‹å’Œç¼ºå¤±å€¼æ£€æŸ¥\n",
    "print(\"=\" * 50)\n",
    "print(\"æ•°æ®ç±»å‹:\")\n",
    "print(\"=\" * 50)\n",
    "print(df_main.dtypes)\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ç¼ºå¤±å€¼ç»Ÿè®¡:\")\n",
    "print(\"=\" * 50)\n",
    "print(df_main.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bop29v6g2hi",
   "metadata": {},
   "source": [
    "### è¾“å‡ºåˆ†æ\n",
    "\n",
    "**å‘ç°çš„é—®é¢˜**ï¼š\n",
    "- `TotalCharges` æ˜¯ `object` ç±»å‹è€Œé `float64`ï¼Œè¯´æ˜å­˜åœ¨éæ•°å€¼å­—ç¬¦ï¼ˆå¦‚ç©ºå­—ç¬¦ä¸²ï¼‰ï¼Œéœ€è¦è½¬æ¢\n",
    "- è¡¨é¢ä¸Šæ²¡æœ‰ç¼ºå¤±å€¼ï¼ˆ`isnull().sum()` å…¨ä¸º 0ï¼‰ï¼Œä½†ç©ºå­—ç¬¦ä¸²ä¸ä¼šè¢«æ£€æµ‹ä¸º NaN\n",
    "\n",
    "---\n",
    "\n",
    "## 1.3 æµå¤±åˆ†æ\n",
    "\n",
    "åˆ†æç›®æ ‡å˜é‡ï¼ˆChurnï¼‰çš„åˆ†å¸ƒæƒ…å†µã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "nehmkmsc9rc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æµå¤±ç»Ÿè®¡:\n",
      "Churn\n",
      "No     5174\n",
      "Yes    1869\n",
      "Name: count, dtype: int64\n",
      "\n",
      "æµå¤±ç‡: 26.54%\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# 1.3 æ•°æ®æ¢ç´¢ - æµå¤±åˆ†æ\n",
    "# --------------------------------------------\n",
    "\n",
    "# æµå¤±ç‡ç»Ÿè®¡\n",
    "churn_counts = df_main['Churn'].value_counts()\n",
    "print(\"æµå¤±ç»Ÿè®¡:\")\n",
    "print(churn_counts)\n",
    "print(f\"\\næµå¤±ç‡: {(df_main['Churn'] == 'Yes').mean() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6p0rtluc0ev",
   "metadata": {},
   "source": [
    "### è¾“å‡ºåˆ†æ\n",
    "\n",
    "- **æµå¤±å®¢æˆ·**ï¼š1,869 äººï¼ˆ26.54%ï¼‰\n",
    "- **ç•™å­˜å®¢æˆ·**ï¼š5,174 äººï¼ˆ73.46%ï¼‰\n",
    "\n",
    "è¿™æ˜¯ä¸€ä¸ªå…¸å‹çš„**ä¸å¹³è¡¡æ•°æ®é›†**ï¼ˆImbalanced Datasetï¼‰ï¼Œæµå¤±å®¢æˆ·å æ¯”çº¦ 1/4ã€‚åœ¨å®é™…ä¸šåŠ¡ä¸­ï¼Œè¿™ä¸ªæµå¤±ç‡å·²ç»ç›¸å½“é«˜ï¼Œéœ€è¦é‡ç‚¹åˆ†ææµå¤±åŸå› ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## 1.4 å®¢æˆ·åé¦ˆåˆ†æ\n",
    "\n",
    "æŸ¥çœ‹æµå¤±å®¢æˆ·å’Œç•™å­˜å®¢æˆ·çš„åé¦ˆæ–‡æœ¬ç¤ºä¾‹ï¼Œäº†è§£æ–‡æœ¬çš„ç‰¹ç‚¹å’Œå·®å¼‚ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3h6adxp0wfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å®¢æˆ·åé¦ˆç¤ºä¾‹ (æµå¤±å®¢æˆ·):\n",
      "--------------------------------------------------\n",
      "I recently signed up for DSL internet service with this provider two months ago on a month-to-month contract. Unfortunately, I have already decided to churn and switch to a different provider. The monthly charges of $53.85 were reasonable, but I found the internet service to be unreliable and slow. Additionally, having to mail in a check for payment was inconvenient and outdated. I would not recommend this provider to others looking for reliable and convenient internet service.\n",
      "\n",
      "==================================================\n",
      "\n",
      "å®¢æˆ·åé¦ˆç¤ºä¾‹ (éæµå¤±å®¢æˆ·):\n",
      "--------------------------------------------------\n",
      "I have been using the DSL internet service from this provider for the past month and so far, I am satisfied with the service. The connection has been reliable and the speed is sufficient for my needs. The monthly charges are reasonable at $29.85 and I appreciate the convenience of paying through electronic check. Overall, I have had a positive experience and would recommend this provider to others.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# 1.4 æ•°æ®æ¢ç´¢ - å®¢æˆ·åé¦ˆåˆ†æ\n",
    "# --------------------------------------------\n",
    "\n",
    "# æ£€æŸ¥å®¢æˆ·åé¦ˆåˆ—\n",
    "print(\"å®¢æˆ·åé¦ˆç¤ºä¾‹ (æµå¤±å®¢æˆ·):\")\n",
    "print(\"-\" * 50)\n",
    "churned_feedback = df_main[df_main['Churn'] == 'Yes']['CustomerFeedback'].iloc[0]\n",
    "print(churned_feedback[:500] if len(churned_feedback) > 500 else churned_feedback)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"\\nå®¢æˆ·åé¦ˆç¤ºä¾‹ (éæµå¤±å®¢æˆ·):\")\n",
    "print(\"-\" * 50)\n",
    "retained_feedback = df_main[df_main['Churn'] == 'No']['CustomerFeedback'].iloc[0]\n",
    "print(retained_feedback[:500] if len(retained_feedback) > 500 else retained_feedback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o363pvze3z9",
   "metadata": {},
   "source": [
    "### è¾“å‡ºåˆ†æ\n",
    "\n",
    "é€šè¿‡å¯¹æ¯”ä¸¤ç±»å®¢æˆ·çš„åé¦ˆï¼Œå¯ä»¥è§‚å¯Ÿåˆ°æ˜æ˜¾çš„**æƒ…æ„Ÿå·®å¼‚**ï¼š\n",
    "\n",
    "| å®¢æˆ·ç±»å‹ | åé¦ˆç‰¹ç‚¹ |\n",
    "|----------|----------|\n",
    "| **æµå¤±å®¢æˆ·** | ä½¿ç”¨è´Ÿé¢è¯æ±‡ï¼ˆunreliable, slow, inconvenientï¼‰ï¼Œæ˜ç¡®è¡¨ç¤ºä¸æ»¡å’Œç¦»å¼€æ„æ„¿ |\n",
    "| **ç•™å­˜å®¢æˆ·** | ä½¿ç”¨æ­£é¢è¯æ±‡ï¼ˆsatisfied, reliable, reasonableï¼‰ï¼Œè¡¨ç¤ºæ¨èæ„æ„¿ |\n",
    "\n",
    "è¿™äº›åé¦ˆæ–‡æœ¬å°†æ˜¯ RAG ç³»ç»Ÿçš„**æ ¸å¿ƒçŸ¥è¯†åº“**ï¼ŒLLM å°†åŸºäºè¿™äº›çœŸå®åé¦ˆè¿›è¡Œåˆ†æã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## 1.5 æ•°æ®æ¸…æ´—ä¸é¢„å¤„ç†\n",
    "\n",
    "è¿›è¡Œå¿…è¦çš„æ•°æ®æ¸…æ´—ï¼š\n",
    "1. åˆ é™¤æ— ç”¨åˆ—ï¼ˆPromptInput æ˜¯ç”Ÿæˆåé¦ˆçš„æç¤ºè¯ï¼Œä¸éœ€è¦ä¿ç•™ï¼‰\n",
    "2. è½¬æ¢æ•°æ®ç±»å‹ï¼ˆTotalCharges è½¬ä¸ºæ•°å€¼å‹ï¼‰\n",
    "3. å¤„ç†ç¼ºå¤±å€¼\n",
    "4. åˆ›å»ºæ•°å€¼å‹ç›®æ ‡å˜é‡ï¼ˆä¾¿äºåç»­åˆ†æï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0os9zxqr5kk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²åˆ é™¤ PromptInput åˆ—\n",
      "TotalCharges ç¼ºå¤±å€¼: 11\n",
      "\n",
      "æ¸…æ´—åæ•°æ®é›†å¤§å°: (7043, 23)\n",
      "æ•°æ®æ¸…æ´—å®Œæˆ âœ“\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# 1.5 æ•°æ®æ¸…æ´—ä¸é¢„å¤„ç†\n",
    "# --------------------------------------------\n",
    "\n",
    "# åˆ›å»ºå·¥ä½œå‰¯æœ¬\n",
    "df = df_main.copy()\n",
    "\n",
    "# 1. åˆ é™¤ä¸éœ€è¦çš„åˆ— (PromptInput æ˜¯ç”¨äºç”Ÿæˆåé¦ˆçš„æç¤ºï¼Œä¸éœ€è¦ä¿ç•™)\n",
    "if 'PromptInput' in df.columns:\n",
    "    df = df.drop(columns=['PromptInput'])\n",
    "    print(\"å·²åˆ é™¤ PromptInput åˆ—\")\n",
    "\n",
    "# 2. å¤„ç† TotalCharges ä¸­çš„ç©ºå€¼ (è½¬æ¢ä¸ºæ•°å€¼å‹)\n",
    "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "print(f\"TotalCharges ç¼ºå¤±å€¼: {df['TotalCharges'].isnull().sum()}\")\n",
    "\n",
    "# 3. å¡«å……ç¼ºå¤±å€¼ (ä½¿ç”¨ MonthlyCharges å¡«å……ï¼Œå› ä¸ºæ–°å®¢æˆ· TotalCharges å¯èƒ½ä¸ºç©º)\n",
    "df['TotalCharges'] = df['TotalCharges'].fillna(df['MonthlyCharges'])\n",
    "\n",
    "# 4. å°† Churn è½¬æ¢ä¸ºæ•°å€¼ (ä¾¿äºåç»­åˆ†æ)\n",
    "df['Churn_Binary'] = (df['Churn'] == 'Yes').astype(int)\n",
    "\n",
    "print(f\"\\næ¸…æ´—åæ•°æ®é›†å¤§å°: {df.shape}\")\n",
    "print(\"æ•°æ®æ¸…æ´—å®Œæˆ âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dry43r1izds",
   "metadata": {},
   "source": [
    "### è¾“å‡ºåˆ†æ\n",
    "\n",
    "- æˆåŠŸè½¬æ¢ `TotalCharges` ä¸ºæ•°å€¼å‹åï¼Œå‘ç° **11 ä¸ªç¼ºå¤±å€¼**\n",
    "- è¿™äº›ç¼ºå¤±å€¼å¯¹åº”æ–°å®¢æˆ·ï¼ˆtenure=0ï¼‰ï¼Œä½¿ç”¨ `MonthlyCharges` å¡«å……æ˜¯åˆç†çš„\n",
    "- åˆ›å»ºäº† `Churn_Binary` åˆ—ï¼ˆ0=ç•™å­˜ï¼Œ1=æµå¤±ï¼‰ä¾¿äºæ•°å€¼è®¡ç®—\n",
    "\n",
    "## 1.6 åé¦ˆæ–‡æœ¬å¤„ç†\n",
    "\n",
    "æ£€æŸ¥åé¦ˆæ–‡æœ¬çš„æœ‰æ•ˆæ€§ï¼Œç¡®ä¿æ‰€æœ‰è®°å½•éƒ½æœ‰å¯ç”¨çš„åé¦ˆå†…å®¹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "v0ep9kfar6l",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æœ‰æ•ˆåé¦ˆæ•°é‡: 7043 / 7043\n",
      "æœ‰æ•ˆåé¦ˆæ¯”ä¾‹: 100.00%\n",
      "\n",
      "åé¦ˆé•¿åº¦ç»Ÿè®¡:\n",
      "count    7043.000000\n",
      "mean      460.571205\n",
      "std        68.197917\n",
      "min       277.000000\n",
      "25%       414.000000\n",
      "50%       454.000000\n",
      "75%       499.000000\n",
      "max       840.000000\n",
      "Name: feedback_length, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# 1.6 åé¦ˆæ–‡æœ¬å¤„ç†\n",
    "# --------------------------------------------\n",
    "\n",
    "# æ£€æŸ¥åé¦ˆæ–‡æœ¬çš„æœ‰æ•ˆæ€§\n",
    "df['feedback_length'] = df['CustomerFeedback'].fillna('').apply(len)\n",
    "df['has_feedback'] = df['feedback_length'] > 10  # è‡³å°‘10ä¸ªå­—ç¬¦ç®—æœ‰æ•ˆåé¦ˆ\n",
    "\n",
    "print(f\"æœ‰æ•ˆåé¦ˆæ•°é‡: {df['has_feedback'].sum()} / {len(df)}\")\n",
    "print(f\"æœ‰æ•ˆåé¦ˆæ¯”ä¾‹: {df['has_feedback'].mean() * 100:.2f}%\")\n",
    "\n",
    "# åé¦ˆé•¿åº¦ç»Ÿè®¡\n",
    "print(f\"\\nåé¦ˆé•¿åº¦ç»Ÿè®¡:\")\n",
    "print(df[df['has_feedback']]['feedback_length'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u5khtufau6m",
   "metadata": {},
   "source": [
    "### è¾“å‡ºåˆ†æ\n",
    "\n",
    "- **100% çš„å®¢æˆ·éƒ½æœ‰æœ‰æ•ˆåé¦ˆ**ï¼ˆé•¿åº¦ > 10 å­—ç¬¦ï¼‰\n",
    "- åé¦ˆé•¿åº¦ç»Ÿè®¡ï¼šå¹³å‡ 461 å­—ç¬¦ï¼ŒèŒƒå›´ 277-840 å­—ç¬¦\n",
    "- è¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥ä¸ºæ‰€æœ‰ 7,043 ä¸ªå®¢æˆ·æ„å»ºæ–‡æ¡£\n",
    "\n",
    "---\n",
    "\n",
    "## 1.7 æ„å»º RAG æ–‡æ¡£\n",
    "\n",
    "**è¿™æ˜¯ Phase 1 æœ€å…³é”®çš„ä¸€æ­¥**ã€‚æˆ‘ä»¬éœ€è¦å°†ç»“æ„åŒ–æ•°æ®å’Œéç»“æ„åŒ–åé¦ˆæ•´åˆæˆç»Ÿä¸€çš„æ–‡æ¡£æ ¼å¼ã€‚\n",
    "\n",
    "### ä¸ºä»€ä¹ˆè¦æ„å»ºæ–‡æ¡£ï¼Ÿ\n",
    "\n",
    "RAG ç³»ç»Ÿçš„æ£€ç´¢å•ä½æ˜¯\"æ–‡æ¡£\"ã€‚ä¸€ä¸ªå¥½çš„æ–‡æ¡£åº”è¯¥ï¼š\n",
    "1. **è‡ªåŒ…å«**ï¼šåŒ…å«è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè®© LLM æ— éœ€é¢å¤–ä¿¡æ¯å³å¯ç†è§£\n",
    "2. **ç»“æ„åŒ–**ï¼šä¿¡æ¯ç»„ç»‡æ¸…æ™°ï¼Œä¾¿äº LLM æå–å…³é”®ä¿¡æ¯\n",
    "3. **æœ‰æ ‡è¯†**ï¼šåŒ…å«å”¯ä¸€æ ‡è¯†ï¼ˆCustomer IDï¼‰ï¼Œä¾¿äºå¼•ç”¨è¿½æº¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4z0neq8vfgr",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²åˆ›å»º 7043 ä¸ªå®¢æˆ·æ–‡æ¡£\n",
      "\n",
      "ç¤ºä¾‹æ–‡æ¡£:\n",
      "==================================================\n",
      "Customer ID: 7590-VHVEG\n",
      "Churn Status: No\n",
      "\n",
      "Customer Profile:\n",
      "- Gender: Female\n",
      "- Senior Citizen: No\n",
      "- Partner: Yes\n",
      "- Dependents: No\n",
      "- Tenure: 1 months\n",
      "\n",
      "Services:\n",
      "- Phone Service: No\n",
      "- Internet Service: DSL\n",
      "- Online Security: No\n",
      "- Tech Support: No\n",
      "- Streaming TV: No\n",
      "- Streaming Movies: No\n",
      "\n",
      "Contract & Billing:\n",
      "- Contract: Month-to-month\n",
      "- Monthly Charges: $29.85\n",
      "- Total Charges: $29.85\n",
      "- Payment Method: Electronic check\n",
      "\n",
      "Customer Feedback:\n",
      "I have been using the DSL internet service from this provider for the past month and so far, I am satisfied with the service. The connection has been reliable and the speed is sufficient for my needs. The monthly charges are reasonable at $29.85 and I appreciate the convenience of paying through electronic check. Overall, I have had a positive experience and\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# 1.7 å‡†å¤‡ RAG æ–‡æ¡£\n",
    "# --------------------------------------------\n",
    "\n",
    "# ä¸ºæ¯ä¸ªå®¢æˆ·åˆ›å»ºä¸€ä¸ªåŒ…å«å®Œæ•´ä¿¡æ¯çš„æ–‡æ¡£ (ç”¨äº RAG æ£€ç´¢)\n",
    "def create_customer_document(row):\n",
    "    \"\"\"\n",
    "    å°†å®¢æˆ·çš„ç»“æ„åŒ–æ•°æ®å’Œåé¦ˆæ•´åˆä¸ºä¸€ä¸ªæ–‡æ¡£\n",
    "    \"\"\"\n",
    "    doc = f\"\"\"Customer ID: {row['customerID']}\n",
    "Churn Status: {row['Churn']}\n",
    "\n",
    "Customer Profile:\n",
    "- Gender: {row['gender']}\n",
    "- Senior Citizen: {'Yes' if row['SeniorCitizen'] == 1 else 'No'}\n",
    "- Partner: {row['Partner']}\n",
    "- Dependents: {row['Dependents']}\n",
    "- Tenure: {row['tenure']} months\n",
    "\n",
    "Services:\n",
    "- Phone Service: {row['PhoneService']}\n",
    "- Internet Service: {row['InternetService']}\n",
    "- Online Security: {row['OnlineSecurity']}\n",
    "- Tech Support: {row['TechSupport']}\n",
    "- Streaming TV: {row['StreamingTV']}\n",
    "- Streaming Movies: {row['StreamingMovies']}\n",
    "\n",
    "Contract & Billing:\n",
    "- Contract: {row['Contract']}\n",
    "- Monthly Charges: ${row['MonthlyCharges']}\n",
    "- Total Charges: ${row['TotalCharges']:.2f}\n",
    "- Payment Method: {row['PaymentMethod']}\n",
    "\n",
    "Customer Feedback:\n",
    "{row['CustomerFeedback']}\n",
    "\"\"\"\n",
    "    return doc\n",
    "\n",
    "# åªå¯¹æœ‰åé¦ˆçš„å®¢æˆ·åˆ›å»ºæ–‡æ¡£\n",
    "df_with_feedback = df[df['has_feedback']].copy()\n",
    "df_with_feedback['document'] = df_with_feedback.apply(create_customer_document, axis=1)\n",
    "\n",
    "print(f\"å·²åˆ›å»º {len(df_with_feedback)} ä¸ªå®¢æˆ·æ–‡æ¡£\")\n",
    "print(\"\\nç¤ºä¾‹æ–‡æ¡£:\")\n",
    "print(\"=\" * 50)\n",
    "print(df_with_feedback['document'].iloc[0][:800])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jany4c1bx1r",
   "metadata": {},
   "source": [
    "### è¾“å‡ºåˆ†æ\n",
    "\n",
    "æˆåŠŸåˆ›å»ºäº† **7,043 ä¸ªå®¢æˆ·æ–‡æ¡£**ã€‚æ¯ä¸ªæ–‡æ¡£åŒ…å«ï¼š\n",
    "- å®¢æˆ· ID å’Œæµå¤±çŠ¶æ€ï¼ˆä¾¿äºå¼•ç”¨ï¼‰\n",
    "- å®¢æˆ·ç”»åƒï¼ˆäººå£ç»Ÿè®¡ä¿¡æ¯ï¼‰\n",
    "- æœåŠ¡è®¢é˜…æƒ…å†µ\n",
    "- è´¹ç”¨å’ŒåˆåŒä¿¡æ¯\n",
    "- åŸå§‹å®¢æˆ·åé¦ˆ\n",
    "\n",
    "è¿™ç§æ ¼å¼ç¡®ä¿äº† LLM åœ¨åˆ†ææ—¶èƒ½å¤ŸåŒæ—¶çœ‹åˆ°**å®šé‡æ•°æ®**ï¼ˆè´¹ç”¨ã€åœ¨ç½‘æ—¶é•¿ï¼‰å’Œ**å®šæ€§æ•°æ®**ï¼ˆåé¦ˆæ–‡æœ¬ï¼‰ã€‚\n",
    "\n",
    "## Phase 1 æ€»ç»“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "y4f1q5wfdla",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Phase 1: æ•°æ®å‡†å¤‡å®Œæˆ\n",
      "==================================================\n",
      "æ€»å®¢æˆ·æ•°: 7043\n",
      "æœ‰åé¦ˆçš„å®¢æˆ·æ•°: 7043\n",
      "æµå¤±å®¢æˆ·æ•°: 1869\n",
      "æµå¤±ç‡: 26.54%\n",
      "\n",
      "æµå¤±å®¢æˆ·ä¸­æœ‰åé¦ˆçš„æ¯”ä¾‹: 26.54%\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# Phase 1 å®Œæˆ - æ•°æ®æ‘˜è¦\n",
    "# --------------------------------------------\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Phase 1: æ•°æ®å‡†å¤‡å®Œæˆ\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"æ€»å®¢æˆ·æ•°: {len(df)}\")\n",
    "print(f\"æœ‰åé¦ˆçš„å®¢æˆ·æ•°: {len(df_with_feedback)}\")\n",
    "print(f\"æµå¤±å®¢æˆ·æ•°: {df['Churn_Binary'].sum()}\")\n",
    "print(f\"æµå¤±ç‡: {df['Churn_Binary'].mean() * 100:.2f}%\")\n",
    "print(f\"\\næµå¤±å®¢æˆ·ä¸­æœ‰åé¦ˆçš„æ¯”ä¾‹: {df_with_feedback['Churn_Binary'].mean() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macqc1205a",
   "metadata": {},
   "source": [
    "### Phase 1 å®Œæˆ\n",
    "\n",
    "æ•°æ®å‡†å¤‡é˜¶æ®µçš„å…³é”®æˆæœï¼š\n",
    "- âœ… åŠ è½½å¹¶æ¸…æ´—äº† 7,043 æ¡å®¢æˆ·æ•°æ®\n",
    "- âœ… æµå¤±ç‡ä¸º 26.54%ï¼ˆ1,869 åæµå¤±å®¢æˆ·ï¼‰\n",
    "- âœ… æ„å»ºäº† 7,043 ä¸ªç»“æ„åŒ–æ–‡æ¡£ï¼Œready for indexing\n",
    "\n",
    "---\n",
    "\n",
    "# Phase 2: ç´¢å¼•ä¸æ£€ç´¢ (Indexing & Retrieval)\n",
    "\n",
    "Phase 2 æ˜¯ RAG ç³»ç»Ÿçš„**æ ¸å¿ƒæŠ€æœ¯ç¯èŠ‚**ï¼Œç›®æ ‡æ˜¯å®ç°é«˜æ•ˆã€å‡†ç¡®çš„æ–‡æ¡£æ£€ç´¢ã€‚\n",
    "\n",
    "## ä¸ºä»€ä¹ˆéœ€è¦æ£€ç´¢ï¼Ÿ\n",
    "\n",
    "LLM çš„ä¸Šä¸‹æ–‡çª—å£æœ‰é™ï¼ˆå¦‚ Qwen2.5-7B çº¦ 32K tokensï¼‰ï¼Œæ— æ³•ä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰ 7,043 ä¸ªæ–‡æ¡£ã€‚æˆ‘ä»¬éœ€è¦ï¼š\n",
    "1. æ ¹æ®ç”¨æˆ·æŸ¥è¯¢ï¼Œå¿«é€Ÿæ‰¾åˆ°æœ€ç›¸å…³çš„å°‘é‡æ–‡æ¡£ï¼ˆå¦‚ Top 5ï¼‰\n",
    "2. åªå°†è¿™äº›ç›¸å…³æ–‡æ¡£é€å…¥ LLM è¿›è¡Œåˆ†æ\n",
    "\n",
    "## æ£€ç´¢ç­–ç•¥ï¼šæ··åˆæ£€ç´¢ï¼ˆHybrid Retrievalï¼‰\n",
    "\n",
    "æˆ‘ä»¬é‡‡ç”¨**å‘é‡æ£€ç´¢ + å…³é”®è¯æ£€ç´¢**çš„æ··åˆç­–ç•¥ï¼š\n",
    "\n",
    "| æ–¹æ³• | ä¼˜åŠ¿ | åŠ£åŠ¿ |\n",
    "|------|------|------|\n",
    "| **å‘é‡æ£€ç´¢** | ç†è§£è¯­ä¹‰ç›¸ä¼¼æ€§ï¼ˆå¦‚\"å–æ¶ˆæœåŠ¡\" â‰ˆ \"é€€è®¢\"ï¼‰ | å¯èƒ½å¿½ç•¥ç²¾ç¡®å…³é”®è¯åŒ¹é… |\n",
    "| **BM25 å…³é”®è¯æ£€ç´¢** | ç²¾ç¡®åŒ¹é…å…³é”®è¯ï¼Œè§£é‡Šæ€§å¼º | æ— æ³•ç†è§£åŒä¹‰è¯å’Œè¯­ä¹‰ |\n",
    "| **æ··åˆæ£€ç´¢** | ç»“åˆä¸¤è€…ä¼˜åŠ¿ | éœ€è¦è®¾è®¡èåˆç­–ç•¥ |\n",
    "\n",
    "## 2.0 åŠ è½½ä¾èµ–\n",
    "\n",
    "å¯¼å…¥ Phase 2 æ‰€éœ€çš„åº“ï¼š\n",
    "- `sentence-transformers`ï¼šç”Ÿæˆæ–‡æœ¬å‘é‡åµŒå…¥\n",
    "- `faiss`ï¼šé«˜æ•ˆå‘é‡ç›¸ä¼¼åº¦æœç´¢\n",
    "- `rank_bm25`ï¼šBM25 å…³é”®è¯æ£€ç´¢ç®—æ³•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "qaar8l1ojso",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 2 ä¾èµ–åŠ è½½å®Œæˆ âœ“\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Phase 2: ç´¢å¼•ä¸æ£€ç´¢ (Indexing & Retrieval)\n",
    "# ============================================\n",
    "\n",
    "# å®‰è£…å¿…è¦çš„ä¾èµ– (å¦‚æœå°šæœªå®‰è£…)\n",
    "# !pip install sentence-transformers faiss-cpu rank_bm25 -q\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from rank_bm25 import BM25Okapi\n",
    "import re\n",
    "\n",
    "print(\"Phase 2 ä¾èµ–åŠ è½½å®Œæˆ âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nklnz1llk2",
   "metadata": {},
   "source": [
    "## 2.1 æ–‡æœ¬é¢„å¤„ç†\n",
    "\n",
    "åœ¨åˆ›å»ºå‘é‡åµŒå…¥ä¹‹å‰ï¼Œéœ€è¦å¯¹æ–‡æœ¬è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†ï¼š\n",
    "- åˆå¹¶å¤šä½™çš„ç©ºç™½å­—ç¬¦\n",
    "- å»é™¤é¦–å°¾ç©ºæ ¼\n",
    "\n",
    "è¿™ä¸€æ­¥ç¡®ä¿æ–‡æœ¬æ ¼å¼ä¸€è‡´ï¼Œé¿å…æ— æ„ä¹‰çš„å·®å¼‚å½±å“æ£€ç´¢æ•ˆæœã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "avst4uof19r",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é¢„å¤„ç†å®Œæˆ: 7043 ä¸ªæ–‡æ¡£\n",
      "å¹³å‡æ–‡æ¡£é•¿åº¦: 918 å­—ç¬¦\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# 2.1 æ–‡æœ¬é¢„å¤„ç†\n",
    "# --------------------------------------------\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    æ¸…ç†æ–‡æœ¬ï¼šå»é™¤å¤šä½™ç©ºç™½\n",
    "    \"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)  # åˆå¹¶å¤šä½™ç©ºç™½\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "# é¢„å¤„ç†æ‰€æœ‰æ–‡æ¡£\n",
    "documents = df_with_feedback['document'].apply(preprocess_text).tolist()\n",
    "customer_ids = df_with_feedback['customerID'].tolist()\n",
    "\n",
    "print(f\"é¢„å¤„ç†å®Œæˆ: {len(documents)} ä¸ªæ–‡æ¡£\")\n",
    "print(f\"å¹³å‡æ–‡æ¡£é•¿åº¦: {np.mean([len(d) for d in documents]):.0f} å­—ç¬¦\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59tjjvsvmzk",
   "metadata": {},
   "source": [
    "## 2.2 åˆ›å»ºå‘é‡åµŒå…¥ (Vector Embeddings)\n",
    "\n",
    "### ä»€ä¹ˆæ˜¯å‘é‡åµŒå…¥ï¼Ÿ\n",
    "\n",
    "å‘é‡åµŒå…¥æ˜¯å°†æ–‡æœ¬æ˜ å°„åˆ°é«˜ç»´å‘é‡ç©ºé—´çš„æŠ€æœ¯ã€‚åœ¨è¿™ä¸ªç©ºé—´ä¸­ï¼š\n",
    "- **è¯­ä¹‰ç›¸ä¼¼çš„æ–‡æœ¬**è·ç¦»æ›´è¿‘\n",
    "- **è¯­ä¹‰ä¸åŒçš„æ–‡æœ¬**è·ç¦»æ›´è¿œ\n",
    "\n",
    "ä¾‹å¦‚ï¼š\n",
    "- \"I want to cancel my service\" å’Œ \"I'm leaving this company\" çš„å‘é‡ä¼šå¾ˆæ¥è¿‘\n",
    "- \"I love this service\" å’Œ \"I want to cancel\" çš„å‘é‡ä¼šå¾ˆè¿œ\n",
    "\n",
    "### åµŒå…¥æ¨¡å‹é€‰æ‹©\n",
    "\n",
    "æˆ‘ä»¬ä½¿ç”¨ **BGE (BAAI General Embedding)** æ¨¡å‹ï¼š\n",
    "- æ¨¡å‹ï¼š`BAAI/bge-base-en-v1.5`\n",
    "- ç»´åº¦ï¼š768 ç»´\n",
    "- ç‰¹ç‚¹ï¼šåœ¨å¤šä¸ªæ£€ç´¢åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¸”å®Œå…¨å¼€æºå…è´¹\n",
    "\n",
    "### æ•°å­¦åŸç†\n",
    "\n",
    "ç»™å®šä¸€ä¸ªæ–‡æœ¬ $t$ï¼ŒåµŒå…¥æ¨¡å‹ $f$ å°†å…¶æ˜ å°„ä¸ºå‘é‡ï¼š\n",
    "\n",
    "$$\\vec{v} = f(t) \\in \\mathbb{R}^{768}$$\n",
    "\n",
    "ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦é€šè¿‡**ä½™å¼¦ç›¸ä¼¼åº¦**è®¡ç®—ï¼š\n",
    "\n",
    "$$\\text{similarity}(t_1, t_2) = \\cos(\\vec{v_1}, \\vec{v_2}) = \\frac{\\vec{v_1} \\cdot \\vec{v_2}}{||\\vec{v_1}|| \\cdot ||\\vec{v_2}||}$$\n",
    "\n",
    "å½“å‘é‡å·²å½’ä¸€åŒ–ï¼ˆ$||\\vec{v}|| = 1$ï¼‰æ—¶ï¼Œä½™å¼¦ç›¸ä¼¼åº¦ç­‰äºå‘é‡å†…ç§¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "elofkhhc699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŠ è½½åµŒå…¥æ¨¡å‹...\n",
      "ç”Ÿæˆæ–‡æ¡£åµŒå…¥ (å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 221/221 [01:47<00:00,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "åµŒå…¥ç»´åº¦: (7043, 768)\n",
      "å‘é‡åµŒå…¥åˆ›å»ºå®Œæˆ âœ“\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# 2.2 åˆ›å»ºå‘é‡åµŒå…¥\n",
    "# --------------------------------------------\n",
    "\n",
    "# åŠ è½½åµŒå…¥æ¨¡å‹ (ä½¿ç”¨ BGE æ¨¡å‹ï¼Œæ€§èƒ½å¥½ä¸”å…è´¹)\n",
    "print(\"åŠ è½½åµŒå…¥æ¨¡å‹...\")\n",
    "embedding_model = SentenceTransformer('BAAI/bge-base-en-v1.5')\n",
    "\n",
    "# ç”Ÿæˆæ–‡æ¡£åµŒå…¥\n",
    "print(\"ç”Ÿæˆæ–‡æ¡£åµŒå…¥ (å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)...\")\n",
    "document_embeddings = embedding_model.encode(\n",
    "    documents, \n",
    "    show_progress_bar=True,\n",
    "    normalize_embeddings=True  # å½’ä¸€åŒ–ï¼Œç”¨äºä½™å¼¦ç›¸ä¼¼åº¦\n",
    ")\n",
    "\n",
    "print(f\"\\nåµŒå…¥ç»´åº¦: {document_embeddings.shape}\")\n",
    "print(\"å‘é‡åµŒå…¥åˆ›å»ºå®Œæˆ âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oqa3mv4d96",
   "metadata": {},
   "source": [
    "### è¾“å‡ºåˆ†æ\n",
    "\n",
    "- æˆåŠŸç”Ÿæˆ **7,043 ä¸ªæ–‡æ¡£**çš„å‘é‡åµŒå…¥\n",
    "- æ¯ä¸ªå‘é‡çš„ç»´åº¦æ˜¯ **768**\n",
    "- ä½¿ç”¨äº† `normalize_embeddings=True` è¿›è¡Œ L2 å½’ä¸€åŒ–ï¼Œä½¿å¾—åç»­å¯ä»¥ç”¨å†…ç§¯ä»£æ›¿ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "\n",
    "## 2.3 æ„å»ºå‘é‡å­˜å‚¨ (FAISS Index)\n",
    "\n",
    "### ä»€ä¹ˆæ˜¯ FAISSï¼Ÿ\n",
    "\n",
    "**FAISS (Facebook AI Similarity Search)** æ˜¯ Meta å¼€å‘çš„é«˜æ•ˆå‘é‡ç›¸ä¼¼åº¦æœç´¢åº“ï¼Œèƒ½å¤Ÿï¼š\n",
    "- åœ¨æ¯«ç§’çº§å®Œæˆç™¾ä¸‡çº§å‘é‡çš„ç›¸ä¼¼åº¦æœç´¢\n",
    "- æ”¯æŒ GPU åŠ é€Ÿ\n",
    "- æä¾›å¤šç§ç´¢å¼•ç±»å‹ï¼ˆç²¾ç¡®æœç´¢ã€è¿‘ä¼¼æœç´¢ï¼‰\n",
    "\n",
    "### ç´¢å¼•ç±»å‹é€‰æ‹©\n",
    "\n",
    "æˆ‘ä»¬ä½¿ç”¨ `IndexFlatIP`ï¼ˆFlat Index with Inner Productï¼‰ï¼š\n",
    "- **Flat**ï¼šç²¾ç¡®æœç´¢ï¼Œä¸åšè¿‘ä¼¼\n",
    "- **IP**ï¼šInner Productï¼ˆå†…ç§¯ï¼‰ï¼Œå› ä¸ºå‘é‡å·²å½’ä¸€åŒ–ï¼Œç­‰ä»·äºä½™å¼¦ç›¸ä¼¼åº¦\n",
    "\n",
    "å¯¹äº 7,043 ä¸ªæ–‡æ¡£ï¼Œç²¾ç¡®æœç´¢çš„æ€§èƒ½å·²ç»è¶³å¤Ÿå¥½ã€‚å¦‚æœæ–‡æ¡£æ•°é‡è¾¾åˆ°ç™¾ä¸‡çº§ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨ `IndexIVFFlat` ç­‰è¿‘ä¼¼ç´¢å¼•ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6xyw0270jxv",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS ç´¢å¼•å·²åˆ›å»º\n",
      "ç´¢å¼•ä¸­çš„å‘é‡æ•°: 7043\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# 2.3 æ„å»ºå‘é‡å­˜å‚¨ (FAISS)\n",
    "# --------------------------------------------\n",
    "\n",
    "# åˆ›å»º FAISS ç´¢å¼•\n",
    "dimension = document_embeddings.shape[1]\n",
    "faiss_index = faiss.IndexFlatIP(dimension)  # å†…ç§¯ (å› ä¸ºå·²å½’ä¸€åŒ–ï¼Œç­‰ä»·äºä½™å¼¦ç›¸ä¼¼åº¦)\n",
    "faiss_index.add(document_embeddings.astype('float32'))\n",
    "\n",
    "print(f\"FAISS ç´¢å¼•å·²åˆ›å»º\")\n",
    "print(f\"ç´¢å¼•ä¸­çš„å‘é‡æ•°: {faiss_index.ntotal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l0zdl2g6x8",
   "metadata": {},
   "source": [
    "## 2.4 æ„å»º BM25 ç´¢å¼•\n",
    "\n",
    "### ä»€ä¹ˆæ˜¯ BM25ï¼Ÿ\n",
    "\n",
    "**BM25 (Best Matching 25)** æ˜¯ç»å…¸çš„å…³é”®è¯æ£€ç´¢ç®—æ³•ï¼Œå¹¿æ³›ç”¨äºæœç´¢å¼•æ“ã€‚å®ƒåŸºäºè¯è¢‹æ¨¡å‹ï¼Œè€ƒè™‘ï¼š\n",
    "- **è¯é¢‘ (TF)**ï¼šè¯åœ¨æ–‡æ¡£ä¸­å‡ºç°çš„æ¬¡æ•°\n",
    "- **é€†æ–‡æ¡£é¢‘ç‡ (IDF)**ï¼šè¯åœ¨æ•´ä¸ªè¯­æ–™åº“ä¸­çš„ç¨€æœ‰ç¨‹åº¦\n",
    "- **æ–‡æ¡£é•¿åº¦å½’ä¸€åŒ–**ï¼šé¿å…é•¿æ–‡æ¡£è·å¾—ä¸å…¬å¹³ä¼˜åŠ¿\n",
    "\n",
    "### BM25 å…¬å¼\n",
    "\n",
    "å¯¹äºæŸ¥è¯¢ $Q$ å’Œæ–‡æ¡£ $D$ï¼ŒBM25 å¾—åˆ†ä¸ºï¼š\n",
    "\n",
    "$$\\text{BM25}(D, Q) = \\sum_{i=1}^{n} \\text{IDF}(q_i) \\cdot \\frac{f(q_i, D) \\cdot (k_1 + 1)}{f(q_i, D) + k_1 \\cdot (1 - b + b \\cdot \\frac{|D|}{\\text{avgdl}})}$$\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "- $f(q_i, D)$ï¼šè¯ $q_i$ åœ¨æ–‡æ¡£ $D$ ä¸­çš„è¯é¢‘\n",
    "- $|D|$ï¼šæ–‡æ¡£é•¿åº¦\n",
    "- $\\text{avgdl}$ï¼šå¹³å‡æ–‡æ¡£é•¿åº¦\n",
    "- $k_1$ã€$b$ï¼šè°ƒèŠ‚å‚æ•°ï¼ˆé€šå¸¸ $k_1 = 1.5$ï¼Œ$b = 0.75$ï¼‰\n",
    "- $\\text{IDF}(q_i)$ï¼šè¯ $q_i$ çš„é€†æ–‡æ¡£é¢‘ç‡\n",
    "\n",
    "### BM25 çš„ä¼˜åŠ¿\n",
    "\n",
    "- å¯¹**ç²¾ç¡®å…³é”®è¯åŒ¹é…**æ•ˆæœå¥½\n",
    "- è®¡ç®—é«˜æ•ˆï¼Œæ— éœ€ GPU\n",
    "- ç»“æœå¯è§£é‡Šï¼ˆå¯ä»¥çŸ¥é“å“ªäº›è¯åŒ¹é…äº†ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "zekfwchaae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 ç´¢å¼•å·²åˆ›å»º\n",
      "è¯æ±‡è¡¨å¤§å°: 17770\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# 2.4 æ„å»º BM25 ç´¢å¼• (å…³é”®è¯æ£€ç´¢)\n",
    "# --------------------------------------------\n",
    "\n",
    "# åˆ†è¯å¤„ç†\n",
    "tokenized_docs = [doc.lower().split() for doc in documents]\n",
    "\n",
    "# åˆ›å»º BM25 ç´¢å¼•\n",
    "bm25_index = BM25Okapi(tokenized_docs)\n",
    "\n",
    "print(f\"BM25 ç´¢å¼•å·²åˆ›å»º\")\n",
    "print(f\"è¯æ±‡è¡¨å¤§å°: {len(bm25_index.idf)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2zs16iompra",
   "metadata": {},
   "source": [
    "### è¾“å‡ºåˆ†æ\n",
    "\n",
    "- BM25 ç´¢å¼•åŒ…å« **17,770 ä¸ªå”¯ä¸€è¯æ±‡**\n",
    "- åˆ†è¯ä½¿ç”¨ç®€å•çš„ç©ºæ ¼åˆ†å‰²ï¼ˆé€‚ç”¨äºè‹±æ–‡ï¼‰ï¼Œä¸­æ–‡éœ€è¦ä½¿ç”¨ jieba ç­‰åˆ†è¯å·¥å…·\n",
    "\n",
    "## 2.5 å®ç°æ··åˆæ£€ç´¢ (Hybrid Search)\n",
    "\n",
    "### ä¸ºä»€ä¹ˆè¦æ··åˆï¼Ÿ\n",
    "\n",
    "å•ä¸€æ£€ç´¢æ–¹æ³•éƒ½æœ‰å±€é™ï¼š\n",
    "\n",
    "| æŸ¥è¯¢ç¤ºä¾‹ | å‘é‡æ£€ç´¢ | BM25 |\n",
    "|----------|----------|------|\n",
    "| \"customers unhappy with service\" | âœ… èƒ½ç†è§£\"unhappy\"çš„è¯­ä¹‰ | âŒ å¯èƒ½æ¼æ‰\"dissatisfied\"çš„æ–‡æ¡£ |\n",
    "| \"DSL internet\" | âŒ å¯èƒ½è¿”å›å…¶ä»–ç±»å‹çš„ç½‘ç»œæœåŠ¡ | âœ… ç²¾ç¡®åŒ¹é…\"DSL\" |\n",
    "\n",
    "æ··åˆæ£€ç´¢ç»“åˆä¸¤è€…ä¼˜åŠ¿ï¼Œæé«˜æ£€ç´¢çš„**å¬å›ç‡**å’Œ**å‡†ç¡®ç‡**ã€‚\n",
    "\n",
    "### èåˆç®—æ³•ï¼šRRF (Reciprocal Rank Fusion)\n",
    "\n",
    "**RRF** æ˜¯ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ’åèåˆç®—æ³•ï¼Œå…¬å¼ä¸ºï¼š\n",
    "\n",
    "$$\\text{RRF}(d) = \\sum_{r \\in R} \\frac{1}{k + r(d)}$$\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "- $R$ï¼šæ‰€æœ‰æ£€ç´¢ç»“æœåˆ—è¡¨ï¼ˆæœ¬é¡¹ç›®ä¸­æ˜¯å‘é‡æ£€ç´¢å’Œ BM25ï¼‰\n",
    "- $r(d)$ï¼šæ–‡æ¡£ $d$ åœ¨æŸä¸ªåˆ—è¡¨ä¸­çš„æ’åï¼ˆä» 1 å¼€å§‹ï¼‰\n",
    "- $k$ï¼šå¹³æ»‘å¸¸æ•°ï¼ˆé€šå¸¸å– 60ï¼‰\n",
    "\n",
    "**RRF çš„ç‰¹ç‚¹**ï¼š\n",
    "- ä¸éœ€è¦å¯¹ä¸åŒæ£€ç´¢æ–¹æ³•çš„å¾—åˆ†è¿›è¡Œå½’ä¸€åŒ–\n",
    "- å¯¹æ’åé å‰çš„æ–‡æ¡£ç»™äºˆæ›´é«˜æƒé‡\n",
    "- å‚æ•° $k$ æ§åˆ¶æ’åå·®å¼‚çš„å½±å“ç¨‹åº¦\n",
    "\n",
    "### æƒé‡å‚æ•° Î±\n",
    "\n",
    "å¯¹äºæ–‡æ¡£ dï¼Œå…¶ RRF èåˆå¾—åˆ†å®šä¹‰ä¸ºï¼š\n",
    "\n",
    "$$\n",
    "\\mathrm{RRF}(d)\n",
    "=\n",
    "\\alpha \\cdot \\frac{1}{k + r_{\\text{faiss}}(d)}\n",
    "+\n",
    "(1 - \\alpha) \\cdot \\frac{1}{k + r_{\\text{bm25}}(d)}\n",
    "$$\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "\n",
    "* $d$ï¼šå€™é€‰æ–‡æ¡£\n",
    "* $r_{\\text{faiss}}(d)$ï¼šæ–‡æ¡£ $d$ åœ¨å‘é‡æ£€ç´¢ï¼ˆFAISSï¼‰ç»“æœä¸­çš„æ’åï¼ˆä» 1 å¼€å§‹ï¼‰\n",
    "* $r_{\\text{bm25}}(d)$ï¼šæ–‡æ¡£ $d$ åœ¨ BM25 å…³é”®è¯æ£€ç´¢ç»“æœä¸­çš„æ’åï¼ˆä» 1 å¼€å§‹ï¼‰\n",
    "* $k$ï¼šRRF çš„å¹³æ»‘å¸¸æ•°ï¼Œç”¨äºå‡å°æ’åå·®å¼‚çš„å½±å“ï¼Œé€šå¸¸å– $k = 60$\n",
    "* $\\alpha \\in [0, 1]$ï¼šæƒé‡å‚æ•°ï¼Œç”¨äºæ§åˆ¶å‘é‡æ£€ç´¢ä¸ BM25 æ£€ç´¢çš„ç›¸å¯¹é‡è¦æ€§\n",
    "\n",
    "  * $\\alpha = 0.5$ï¼šä¸¤ç§æ£€ç´¢æ–¹æ³•æƒé‡ç›¸ç­‰\n",
    "  * $\\alpha > 0.5$ï¼šæ›´åå‘å‘é‡æ£€ç´¢\n",
    "  * $\\alpha < 0.5$ï¼šæ›´åå‘ BM25 æ£€ç´¢\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "favr48ly3g9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ··åˆæ£€ç´¢å‡½æ•°å®šä¹‰å®Œæˆ âœ“\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# 2.5 å®ç°æ··åˆæ£€ç´¢\n",
    "# --------------------------------------------\n",
    "\n",
    "def vector_search(query, k=10):\n",
    "    \"\"\"\n",
    "    å‘é‡æ£€ç´¢\n",
    "    \"\"\"\n",
    "    query_embedding = embedding_model.encode([query], normalize_embeddings=True)\n",
    "    scores, indices = faiss_index.search(query_embedding.astype('float32'), k)\n",
    "    return list(zip(indices[0], scores[0]))\n",
    "\n",
    "def bm25_search(query, k=10):\n",
    "    \"\"\"\n",
    "    BM25 å…³é”®è¯æ£€ç´¢\n",
    "    \"\"\"\n",
    "    tokenized_query = query.lower().split()\n",
    "    scores = bm25_index.get_scores(tokenized_query)\n",
    "    top_indices = np.argsort(scores)[::-1][:k]\n",
    "    return [(idx, scores[idx]) for idx in top_indices]\n",
    "\n",
    "def hybrid_search(query, k=10, alpha=0.5):\n",
    "    \"\"\"\n",
    "    æ··åˆæ£€ç´¢ (RRF - Reciprocal Rank Fusion)\n",
    "    \n",
    "    Args:\n",
    "        query: æŸ¥è¯¢æ–‡æœ¬\n",
    "        k: è¿”å›ç»“æœæ•°\n",
    "        alpha: å‘é‡æ£€ç´¢æƒé‡ (1-alpha ä¸º BM25 æƒé‡)\n",
    "    \"\"\"\n",
    "    # è·å–ä¸¤ç§æ£€ç´¢ç»“æœ\n",
    "    vector_results = vector_search(query, k=k*2)\n",
    "    bm25_results = bm25_search(query, k=k*2)\n",
    "    \n",
    "    # RRF èåˆ\n",
    "    rrf_scores = {}\n",
    "    rrf_k = 60  # RRF å¸¸æ•°\n",
    "    \n",
    "    for rank, (idx, _) in enumerate(vector_results):\n",
    "        rrf_scores[idx] = rrf_scores.get(idx, 0) + alpha / (rrf_k + rank + 1)\n",
    "    \n",
    "    for rank, (idx, _) in enumerate(bm25_results):\n",
    "        rrf_scores[idx] = rrf_scores.get(idx, 0) + (1 - alpha) / (rrf_k + rank + 1)\n",
    "    \n",
    "    # æ’åºå¹¶è¿”å›\n",
    "    sorted_results = sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "    \n",
    "    return sorted_results\n",
    "\n",
    "print(\"æ··åˆæ£€ç´¢å‡½æ•°å®šä¹‰å®Œæˆ âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hds8ng77kaw",
   "metadata": {},
   "source": [
    "## 2.6 æµ‹è¯•æ£€ç´¢åŠŸèƒ½\n",
    "\n",
    "ä½¿ç”¨ä¸€ä¸ªç¤ºä¾‹æŸ¥è¯¢æµ‹è¯•æ··åˆæ£€ç´¢ç³»ç»Ÿï¼ŒéªŒè¯æ£€ç´¢ç»“æœçš„ç›¸å…³æ€§ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cng6n6ujeov",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æµ‹è¯•æŸ¥è¯¢: customers who are unhappy with the internet service and want to cancel\n",
      "==================================================\n",
      "\n",
      "æ£€ç´¢ç»“æœ:\n",
      "\n",
      "--- ç»“æœ 1 (Score: 0.0082) ---\n",
      "Customer ID: 6598-RFFVI\n",
      "Churn: Yes\n",
      "Feedback: I am extremely dissatisfied with my experience with this service provider. Despite being promised internet service, I have not received any connection in the two months since signing up. The fact that I am locked into a one-year contract with monthly charges of $19.3 is unacceptable. The automatic credit card payment method adds insult to injury, as I am essentially paying for a service I am not receiving. I will be looking to cancel my contract and find a more reliable provider as soon as possible.\n",
      "\n",
      "--- ç»“æœ 2 (Score: 0.0082) ---\n",
      "Customer ID: 7321-ZNSLA\n",
      "Churn: No\n",
      "Feedback: I have been a customer of this DSL internet service provider for the past 13 months and I am pleased to say that I have not experienced any issues that would make me want to churn. The monthly charges of $40.55 are reasonable and the service provided has been reliable. \n",
      "\n",
      "I appreciate the option to pay by mailed check as it is convenient for me. Overall, I am satisfied with the service and would recommend it to others who are looking for a reliable internet service provider.\n",
      "\n",
      "--- ç»“æœ 3 (Score: 0.0081) ---\n",
      "Customer ID: 4871-JTKJF\n",
      "Churn: Yes\n",
      "Feedback: I decided to cancel my service with this company after only 1 month of being a customer. The monthly charges of $69.65 for fiber optic internet were reasonable, but the service itself was not up to par. I experienced frequent outages and slow speeds, which was frustrating. Additionally, the process of cancelling my service was not smooth and took longer than expected. I will be looking for a more reliable internet provider in the future.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# 2.6 æµ‹è¯•æ£€ç´¢åŠŸèƒ½\n",
    "# --------------------------------------------\n",
    "\n",
    "# æµ‹è¯•æŸ¥è¯¢\n",
    "test_query = \"customers who are unhappy with the internet service and want to cancel\"\n",
    "\n",
    "print(f\"æµ‹è¯•æŸ¥è¯¢: {test_query}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# æ‰§è¡Œæ··åˆæ£€ç´¢\n",
    "results = hybrid_search(test_query, k=3)\n",
    "\n",
    "print(\"\\næ£€ç´¢ç»“æœ:\")\n",
    "for rank, (idx, score) in enumerate(results, 1):\n",
    "    print(f\"\\n--- ç»“æœ {rank} (Score: {score:.4f}) ---\")\n",
    "    print(f\"Customer ID: {customer_ids[idx]}\")\n",
    "    print(f\"Churn: {df_with_feedback.iloc[idx]['Churn']}\")\n",
    "    # æ˜¾ç¤ºåé¦ˆçš„å‰200å­—ç¬¦\n",
    "    feedback = df_with_feedback.iloc[idx]['CustomerFeedback']\n",
    "    print(f\"Feedback: {feedback}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g0d2ws0llb",
   "metadata": {},
   "source": [
    "### è¾“å‡ºåˆ†æ\n",
    "\n",
    "æ£€ç´¢æµ‹è¯•ç»“æœæ˜¾ç¤ºç³»ç»Ÿå·¥ä½œæ­£å¸¸ï¼š\n",
    "\n",
    "| æ’å | å®¢æˆ· ID | æµå¤±çŠ¶æ€ | åˆ†æ |\n",
    "|------|---------|----------|------|\n",
    "| 1 | 6598-RFFVI | Yes | âœ… æ£€ç´¢åˆ°æµå¤±ä¸”å¯¹ç½‘ç»œæœåŠ¡ä¸æ»¡çš„å®¢æˆ· |\n",
    "| 2 | 7321-ZNSLA | No | æä¾›å¯¹æ¯”è§†è§’ï¼ˆæ»¡æ„çš„å®¢æˆ·ï¼‰ |\n",
    "| 3 | 4871-JTKJF | Yes | âœ… æ£€ç´¢åˆ°å¦ä¸€ä½å› æœåŠ¡è´¨é‡æµå¤±çš„å®¢æˆ· |\n",
    "\n",
    "æ£€ç´¢ç³»ç»ŸæˆåŠŸæ‰¾åˆ°äº†ä¸æŸ¥è¯¢ç›¸å…³çš„å®¢æˆ·åé¦ˆï¼ŒåŒ…æ‹¬æµå¤±å’Œéæµå¤±å®¢æˆ·ï¼Œè¿™å¯¹äºå…¨é¢åˆ†æå¾ˆæœ‰ä»·å€¼ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "# Phase 3: LLM é›†æˆ (LLM Integration)\n",
    "\n",
    "Phase 3 å°†æ£€ç´¢ç³»ç»Ÿä¸å¤§è¯­è¨€æ¨¡å‹è¿æ¥ï¼Œæ„å»ºå®Œæ•´çš„ **RAG Pipeline**ã€‚\n",
    "\n",
    "## RAG å·¥ä½œæµç¨‹\n",
    "\n",
    "```mermaid\n",
    "%%{init: {'theme': 'dark', 'themeVariables': { 'fontSize': '14px'}}}%%\n",
    "flowchart LR\n",
    "    A[\"ğŸ” ç”¨æˆ·é—®é¢˜\"] --> B[\"âš¡ æ··åˆæ£€ç´¢\"]\n",
    "    B --> C[\"ğŸ“„ Top-Kæ–‡æ¡£\"]\n",
    "    C --> D[\"ğŸ“ æ„å»ºPrompt\"]\n",
    "    D --> E[\"ğŸ¤– Qwen2.5-7B\"]\n",
    "    E --> F[\"ğŸ“Š ç»“æ„åŒ–è¾“å‡º\"]\n",
    "\n",
    "    D -.-> G[\"System Prompt<br/>+ Context<br/>+ Query\"]\n",
    "\n",
    "    style A fill:#0f3460,stroke:#ffd700,color:#fff\n",
    "    style B fill:#1e3a5f,stroke:#00ff88,color:#fff\n",
    "    style C fill:#2d4a3e,stroke:#00ff88,color:#fff\n",
    "    style D fill:#16213e,stroke:#00d9ff,color:#fff\n",
    "    style E fill:#4a1942,stroke:#ff6b9d,color:#fff\n",
    "    style F fill:#3d1a4a,stroke:#bf7fff,color:#fff\n",
    "    style G fill:#1a1a2e,stroke:#888,color:#aaa\n",
    "```\n",
    "\n",
    "## ä¸ºä»€ä¹ˆç”¨ RAG è€Œä¸æ˜¯ç›´æ¥é—® LLMï¼Ÿ\n",
    "\n",
    "| æ–¹æ³• | ä¼˜ç‚¹ | ç¼ºç‚¹ |\n",
    "|------|------|------|\n",
    "| **ç›´æ¥é—® LLM** | ç®€å• | å¯èƒ½äº§ç”Ÿå¹»è§‰ï¼Œæ— æ³•å¼•ç”¨å…·ä½“æ•°æ® |\n",
    "| **RAG** | åŸºäºçœŸå®æ•°æ®ï¼Œå¯è¿½æº¯å¼•ç”¨ | éœ€è¦æ„å»ºæ£€ç´¢ç³»ç»Ÿ |\n",
    "\n",
    "RAG è®© LLM çš„å›ç­”**æœ‰æ®å¯æŸ¥**ï¼Œè¿™åœ¨å•†ä¸šåˆ†æåœºæ™¯ä¸­è‡³å…³é‡è¦ã€‚\n",
    "\n",
    "## 3.0 åŠ è½½å¼€æº LLM æ¨¡å‹\n",
    "\n",
    "åŠ è½½ Qwen2.5-7B-Instructï¼ˆ4-bit é‡åŒ–ï¼‰ä½œä¸º RAG Pipeline çš„ç”Ÿæˆç«¯ã€‚ä½¿ç”¨ BitsAndBytes 4-bit é‡åŒ–ï¼ŒVRAM å ç”¨çº¦ 4.5 GBï¼ŒT4 GPU å®Œå…¨å¤Ÿç”¨ã€‚\n",
    "\n",
    "> **âš ï¸ ä» Phase 3 å¼€å§‹ï¼Œéœ€è¦åœ¨ Google Colabï¼ˆT4 GPUï¼‰ä¸Šè¿è¡Œ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "rdcst3v91cg",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI å®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ âœ“\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Phase 3: LLM é›†æˆ (LLM Integration)\n",
    "# âš ï¸ åœ¨ Google Colab (T4 GPU) ä¸Šè¿è¡Œ\n",
    "# ============================================\n",
    "\n",
    "# å®‰è£…å¾®è°ƒç›¸å…³ä¾èµ–ï¼ˆPhase 3-8 å‡éœ€è¦ï¼‰\n",
    "# !pip install -q transformers peft bitsandbytes trl datasets accelerate\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import json\n",
    "\n",
    "# 4-bit é‡åŒ–é…ç½®ï¼ˆNF4 + åŒé‡é‡åŒ–ï¼Œæœ€å¤§åŒ–å‹ç¼©ç‡ï¼‰\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",             # NF4 é‡åŒ–ï¼šä¿¡æ¯è®ºæœ€ä¼˜çš„ 4-bit æ•°æ®ç±»å‹\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # è®¡ç®—æ—¶ä½¿ç”¨ bfloat16 ç²¾åº¦\n",
    "    bnb_4bit_use_double_quant=True,         # åŒé‡é‡åŒ–ï¼šè¿›ä¸€æ­¥å‹ç¼©é‡åŒ–å‚æ•°\n",
    ")\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "print(f\"åŠ è½½æ¨¡å‹: {MODEL_NAME} (4-bit é‡åŒ–)...\")\n",
    "print(\"é¢„è®¡éœ€è¦ 2-3 åˆ†é’Ÿ...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"\\næ¨¡å‹åŠ è½½å®Œæˆ âœ“\")\n",
    "print(f\"GPU æ˜¾å­˜ä½¿ç”¨: {torch.cuda.memory_allocated() / 1024**3:.1f} GB\")\n",
    "print(f\"GPU æ˜¾å­˜æ€»é‡: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xginqxgzih",
   "metadata": {},
   "source": [
    "## 3.1 å®šä¹‰ Prompt æ¨¡æ¿\n",
    "\n",
    "### Prompt Engineeringï¼ˆæç¤ºå·¥ç¨‹ï¼‰\n",
    "\n",
    "Prompt è®¾è®¡æ˜¯å½±å“ LLM è¾“å‡ºè´¨é‡çš„å…³é”®å› ç´ ã€‚ä¸€ä¸ªå¥½çš„ Prompt åº”è¯¥ï¼š\n",
    "\n",
    "1. **æ˜ç¡®è§’è‰²**ï¼šå‘Šè¯‰ LLM å®ƒæ˜¯ä»€ä¹ˆä¸“å®¶\n",
    "2. **æ˜ç¡®ä»»åŠ¡**ï¼šæ¸…æ™°æè¿°éœ€è¦å®Œæˆçš„å·¥ä½œ\n",
    "3. **æŒ‡å®šæ ¼å¼**ï¼šè¦æ±‚ç»“æ„åŒ–è¾“å‡ºï¼ˆå¦‚ JSONï¼‰\n",
    "4. **æä¾›ç¤ºä¾‹**ï¼šå±•ç¤ºæœŸæœ›çš„è¾“å‡ºæ ¼å¼\n",
    "5. **è®¾å®šçº¦æŸ**ï¼šæ˜ç¡®é™åˆ¶å’Œè¦æ±‚\n",
    "\n",
    "### æˆ‘ä»¬çš„ Prompt ç»“æ„\n",
    "\n",
    "```mermaid\n",
    "%%{init: {'theme': 'dark', 'themeVariables': { 'fontSize': '14px'}}}%%\n",
    "flowchart TB\n",
    "    subgraph SP[\"ğŸ”§ System Prompt\"]\n",
    "        direction LR\n",
    "        S1[\"ğŸ‘¤ è§’è‰²å®šä¹‰<br/>æµå¤±åˆ†æä¸“å®¶\"]\n",
    "        S2[\"ğŸ“‹ è¾“å‡ºæ ¼å¼<br/>JSON Schema\"]\n",
    "        S3[\"ğŸ“ åˆ†ææŒ‡å—<br/>çº¦æŸæ¡ä»¶\"]\n",
    "    end\n",
    "\n",
    "    PLUS((\"+\"))\n",
    "\n",
    "    subgraph UP[\"ğŸ’¬ User Prompt\"]\n",
    "        direction LR\n",
    "        U1[\"â“ ç”¨æˆ·æŸ¥è¯¢\"]\n",
    "        U2[\"ğŸ“Š æ£€ç´¢åˆ°çš„<br/>å®¢æˆ·æ•°æ®\"]\n",
    "    end\n",
    "\n",
    "    SP --> PLUS --> UP\n",
    "\n",
    "    style SP fill:#1a1a2e,stroke:#4a9eff,stroke-width:2px\n",
    "    style UP fill:#16213e,stroke:#00d9ff,stroke-width:2px\n",
    "    style PLUS fill:#ffd700,stroke:#ffd700,color:#000\n",
    "    style S1 fill:#0f3460,stroke:#ffd700,color:#fff\n",
    "    style S2 fill:#0f3460,stroke:#ffd700,color:#fff\n",
    "    style S3 fill:#0f3460,stroke:#ffd700,color:#fff\n",
    "    style U1 fill:#1e3a5f,stroke:#00ff88,color:#fff\n",
    "    style U2 fill:#1e3a5f,stroke:#00ff88,color:#fff\n",
    "```\n",
    "\n",
    "### ç»“æ„åŒ–è¾“å‡ºçš„é‡è¦æ€§\n",
    "\n",
    "æˆ‘ä»¬è¦æ±‚ LLM è¿”å› JSON æ ¼å¼ï¼Œè¿™æ ·ï¼š\n",
    "- ä¾¿äºç¨‹åºè§£æå’Œåç»­å¤„ç†\n",
    "- ç¡®ä¿è¾“å‡ºåŒ…å«æ‰€æœ‰å¿…è¦å­—æ®µ\n",
    "- å¯ä»¥è¿›è¡Œè‡ªåŠ¨åŒ–éªŒè¯ï¼ˆå¦‚æ£€æŸ¥å¼•ç”¨æ˜¯å¦æœ‰æ•ˆï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7p8ycxn6qm2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt æ¨¡æ¿å®šä¹‰å®Œæˆ âœ“\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# 3.1 å®šä¹‰ Prompt æ¨¡æ¿\n",
    "# --------------------------------------------\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a customer churn analysis expert. Based on the provided customer feedback and profile data, analyze the root causes of churn, assess risk levels, and provide actionable recommendations.\n",
    "\n",
    "You must respond in the following JSON format:\n",
    "{\n",
    "    \"summary\": \"Brief overall summary of the analysis (2-3 sentences)\",\n",
    "    \"top_reasons\": [\"reason 1\", \"reason 2\", \"reason 3\"],\n",
    "    \"risk_level\": \"high/medium/low\",\n",
    "    \"actions\": [\"recommended action 1\", \"recommended action 2\", \"recommended action 3\"],\n",
    "    \"citations\": [\"7590-VHVEG\", \"5575-GNVDE\"]\n",
    "}\n",
    "\n",
    "Guidelines:\n",
    "- Base your analysis ONLY on the provided customer data\n",
    "- IMPORTANT: In citations, use ONLY the exact Customer ID format shown in the data (e.g., \"7590-VHVEG\"), NOT \"customerID_xxx\"\n",
    "- Risk level should be based on the proportion of churned customers and severity of issues\n",
    "- Actions should be specific and actionable\n",
    "- Always respond in valid JSON format\"\"\"\n",
    "\n",
    "USER_TEMPLATE = \"\"\"Query: {query}\n",
    "\n",
    "Relevant Customer Data:\n",
    "{context}\n",
    "\n",
    "Please analyze the above customer feedback and provide insights in the specified JSON format.\"\"\"\n",
    "\n",
    "print(\"Prompt æ¨¡æ¿å®šä¹‰å®Œæˆ âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01gt20mh7e06",
   "metadata": {},
   "source": [
    "## 3.2 å®ç° RAG Pipeline\n",
    "\n",
    "RAG Pipeline æ˜¯æ•´ä¸ªç³»ç»Ÿçš„æ ¸å¿ƒï¼Œå®ƒå°†æ£€ç´¢å’Œç”Ÿæˆä¸¤ä¸ªæ­¥éª¤ä¸²è”èµ·æ¥ã€‚\n",
    "\n",
    "### Pipeline æ­¥éª¤\n",
    "\n",
    "> ä½¿ç”¨æœ¬åœ° Qwen2.5-7B-Instruct æ¨¡å‹ï¼Œæ— éœ€ API Keyï¼Œå®Œå…¨å…è´¹\n",
    "\n",
    "1. **æ£€ç´¢ (Retrieve)**ï¼šä½¿ç”¨æ··åˆæ£€ç´¢æ‰¾åˆ° Top-K ç›¸å…³æ–‡æ¡£\n",
    "2. **æ„å»ºä¸Šä¸‹æ–‡ (Build Context)**ï¼šå°†æ£€ç´¢åˆ°çš„æ–‡æ¡£æ ¼å¼åŒ–ä¸º LLM å¯è¯»çš„æ–‡æœ¬\n",
    "3. **ç”Ÿæˆ (Generate)**ï¼šè°ƒç”¨ LLM APIï¼Œä¼ å…¥ System Promptã€ä¸Šä¸‹æ–‡å’Œç”¨æˆ·æŸ¥è¯¢\n",
    "4. **è§£æ (Parse)**ï¼šè§£æ LLM è¿”å›çš„ JSON å“åº”\n",
    "\n",
    "### æ¨¡å‹é€‰æ‹©\n",
    "\n",
    "æˆ‘ä»¬ä½¿ç”¨ **Qwen2.5-7B-Instruct**ï¼ˆ4-bit é‡åŒ–ï¼‰ï¼š\n",
    "- å®Œå…¨å…è´¹ï¼Œæ— éœ€ API Key\n",
    "- JSON ç»“æ„åŒ–è¾“å‡ºèƒ½åŠ›å¼º\n",
    "- 128K ä¸Šä¸‹æ–‡çª—å£ï¼Œè¶³å¤Ÿå¤„ç† 5 ä¸ªå®¢æˆ·æ–‡æ¡£\n",
    "\n",
    "### Temperature å‚æ•°\n",
    "\n",
    "`temperature=0.7` æ§åˆ¶è¾“å‡ºçš„éšæœºæ€§ï¼š\n",
    "- `0`ï¼šå®Œå…¨ç¡®å®šæ€§ï¼Œæ¯æ¬¡è¾“å‡ºç›¸åŒ\n",
    "- `1`ï¼šæ›´æœ‰åˆ›é€ æ€§ï¼Œä½†å¯èƒ½ä¸ç¨³å®š\n",
    "- `0.7`ï¼šå¹³è¡¡å‡†ç¡®æ€§å’Œå¤šæ ·æ€§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "y4pcgyahi7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Pipeline å®šä¹‰å®Œæˆ âœ“\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# 3.2 å®ç° RAG Pipeline\n",
    "# âš ï¸ åœ¨ Google Colab ä¸Šè¿è¡Œï¼ˆä½¿ç”¨æœ¬åœ°å¼€æºæ¨¡å‹ï¼‰\n",
    "# --------------------------------------------\n",
    "\n",
    "def build_context(search_results, max_docs=5):\n",
    "    \"\"\"\n",
    "    æ„å»º LLM ä¸Šä¸‹æ–‡\n",
    "    \"\"\"\n",
    "    context_parts = []\n",
    "    for idx, score in search_results[:max_docs]:\n",
    "        doc = documents[idx]\n",
    "        context_parts.append(f\"--- Document (Relevance: {score:.4f}) ---\\n{doc}\\n\")\n",
    "    return \"\\n\".join(context_parts)\n",
    "\n",
    "def rag_query(user_query, k=5, max_new_tokens=1024):\n",
    "    \"\"\"\n",
    "    RAG æŸ¥è¯¢ Pipelineï¼ˆä½¿ç”¨æœ¬åœ° Qwen2.5-7B-Instructï¼‰\n",
    "\n",
    "    Args:\n",
    "        user_query: ç”¨æˆ·æŸ¥è¯¢\n",
    "        k: æ£€ç´¢æ–‡æ¡£æ•°\n",
    "        max_new_tokens: æœ€å¤§ç”Ÿæˆ token æ•°\n",
    "\n",
    "    Returns:\n",
    "        dict: è§£æåçš„ JSON ç»“æœ\n",
    "    \"\"\"\n",
    "    # Step 1: æ£€ç´¢ç›¸å…³æ–‡æ¡£\n",
    "    search_results = hybrid_search(user_query, k=k)\n",
    "\n",
    "    # Step 2: æ„å»ºä¸Šä¸‹æ–‡\n",
    "    context = build_context(search_results, max_docs=k)\n",
    "\n",
    "    # Step 3: æ„é€  ChatML prompt å¹¶ç”¨æœ¬åœ°æ¨¡å‹ç”Ÿæˆ\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": USER_TEMPLATE.format(\n",
    "            query=user_query,\n",
    "            context=context\n",
    "        )},\n",
    "    ]\n",
    "\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    # åªè§£ç æ–°ç”Ÿæˆçš„éƒ¨åˆ†\n",
    "    generated_ids = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "    response_text = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    # Step 4: JSON è§£æ + regex fallbackï¼ˆå¼€æºæ¨¡å‹å¯èƒ½åœ¨ JSON å‰åé™„åŠ æ–‡æœ¬ï¼‰\n",
    "    try:\n",
    "        result = json.loads(response_text)\n",
    "    except json.JSONDecodeError:\n",
    "        import re as _re\n",
    "        json_match = _re.search(r'\\{[\\s\\S]*\\}', response_text)\n",
    "        if json_match:\n",
    "            try:\n",
    "                result = json.loads(json_match.group())\n",
    "            except json.JSONDecodeError:\n",
    "                result = {\"error\": \"Failed to parse JSON\", \"raw_response\": response_text}\n",
    "        else:\n",
    "            result = {\"error\": \"Failed to parse JSON\", \"raw_response\": response_text}\n",
    "\n",
    "    # æ·»åŠ æ£€ç´¢ä¿¡æ¯\n",
    "    result[\"retrieved_docs\"] = len(search_results)\n",
    "    result[\"retrieved_customer_ids\"] = [customer_ids[idx] for idx, _ in search_results]\n",
    "\n",
    "    return result\n",
    "\n",
    "print(\"RAG Pipeline å®šä¹‰å®Œæˆ âœ“\")\n",
    "print(\"  ä½¿ç”¨æ¨¡å‹: Qwen2.5-7B-Instruct (æœ¬åœ°, 4-bit)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ion7a97m0l",
   "metadata": {},
   "source": [
    "## 3.3 æµ‹è¯• RAG Pipeline\n",
    "\n",
    "ä½¿ç”¨å®é™…æŸ¥è¯¢æµ‹è¯•å®Œæ•´çš„ RAG ç³»ç»Ÿï¼ŒéªŒè¯ç«¯åˆ°ç«¯çš„å·¥ä½œæµç¨‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "qkcoqmpiwxl",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æµ‹è¯• RAG Pipeline\n",
      "==================================================\n",
      "Query: What are the main reasons customers are leaving due to internet service issues?\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mQuery: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_queries[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m50\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m result = \u001b[43mrag_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_queries\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33måˆ†æç»“æœ:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(json.dumps(result, indent=\u001b[32m2\u001b[39m, ensure_ascii=\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mrag_query\u001b[39m\u001b[34m(user_query, k, model)\u001b[39m\n\u001b[32m     31\u001b[39m context = build_context(search_results, max_docs=k)\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Step 3: è°ƒç”¨ LLM\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mSYSTEM_PROMPT\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mUSER_TEMPLATE\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m            \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mjson_object\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.7\u001b[39;49m\n\u001b[32m     45\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Step 4: è§£æç»“æœ\u001b[39;00m\n\u001b[32m     48\u001b[39m result_text = response.choices[\u001b[32m0\u001b[39m].message.content\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM-Project/venv/lib/python3.12/site-packages/openai/_utils/_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM-Project/venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:1192\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1145\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1146\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1147\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1189\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   1190\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1191\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1201\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1203\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1204\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1205\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1206\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1207\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1208\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1209\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1210\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1211\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1212\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1213\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_retention\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_retention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1214\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1215\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1216\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1217\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1218\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1219\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1220\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1221\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1225\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1227\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1229\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1230\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1231\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1232\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1237\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1238\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1242\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM-Project/venv/lib/python3.12/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM-Project/venv/lib/python3.12/site-packages/openai/_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# 3.3 æµ‹è¯• RAG Pipeline\n",
    "# --------------------------------------------\n",
    "\n",
    "# æµ‹è¯•æŸ¥è¯¢\n",
    "test_queries = [\n",
    "    \"What are the main reasons customers are leaving due to internet service issues?\",\n",
    "    \"Why are customers on month-to-month contracts churning?\",\n",
    "    \"What do customers say about pricing and value for money?\"\n",
    "]\n",
    "\n",
    "# æ‰§è¡Œç¬¬ä¸€ä¸ªæµ‹è¯•æŸ¥è¯¢\n",
    "print(\"æµ‹è¯• RAG Pipeline\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Query: {test_queries[0]}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "result = rag_query(test_queries[0], k=5)\n",
    "\n",
    "print(\"\\nåˆ†æç»“æœ:\")\n",
    "print(json.dumps(result, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zd55zgww39",
   "metadata": {},
   "source": [
    "### è¾“å‡ºåˆ†æ\n",
    "\n",
    "RAG Pipeline æˆåŠŸè¿”å›äº†ç»“æ„åŒ–çš„åˆ†æç»“æœï¼š\n",
    "\n",
    "| å­—æ®µ | å†…å®¹ | è¯´æ˜ |\n",
    "|------|------|------|\n",
    "| **summary** | æ•´ä½“åˆ†ææ‘˜è¦ | LLM åŸºäºæ£€ç´¢æ–‡æ¡£ç”Ÿæˆçš„æ€»ç»“ |\n",
    "| **top_reasons** | æµå¤±åŸå› åˆ—è¡¨ | ä»å®¢æˆ·åé¦ˆä¸­æå–çš„å…³é”®é—®é¢˜ |\n",
    "| **risk_level** | high | åŸºäºæ•°æ®åˆ¤æ–­çš„é£é™©ç­‰çº§ |\n",
    "| **actions** | å»ºè®®è¡ŒåŠ¨ | å¯æ‰§è¡Œçš„æ”¹è¿›æªæ–½ |\n",
    "| **citations** | å®¢æˆ· ID åˆ—è¡¨ | æ”¯æŒç»“è®ºçš„æ•°æ®æ¥æº |\n",
    "\n",
    "**å…³é”®è§‚å¯Ÿ**ï¼š\n",
    "- LLM å‡†ç¡®åœ°ä»åé¦ˆä¸­æå–äº†\"ç½‘ç»œæœåŠ¡ä¸å¯ç”¨\"ã€\"é«˜ä»·æ ¼\"ã€\"å®¢æœé—®é¢˜\"ç­‰æ ¸å¿ƒåŸå› \n",
    "- æ‰€æœ‰å¼•ç”¨çš„å®¢æˆ· ID éƒ½æ¥è‡ªæ£€ç´¢ç»“æœï¼Œè¯´æ˜ LLM éµå¾ªäº†\"åŸºäºæ•°æ®åˆ†æ\"çš„æŒ‡ä»¤\n",
    "- å»ºè®®çš„è¡ŒåŠ¨å…·æœ‰å¯æ“ä½œæ€§\n",
    "\n",
    "---\n",
    "\n",
    "# Phase 4: è¾“å‡ºä¸è¯„ä¼° (Output & Evaluation)\n",
    "\n",
    "Phase 4 å…³æ³¨å¦‚ä½•**å±•ç¤ºç»“æœ**å’Œ**éªŒè¯è´¨é‡**ã€‚\n",
    "\n",
    "## è¯„ä¼°çš„é‡è¦æ€§\n",
    "\n",
    "RAG ç³»ç»Ÿçš„è¾“å‡ºéœ€è¦éªŒè¯ï¼š\n",
    "1. **æ ¼å¼æ­£ç¡®æ€§**ï¼šJSON æ˜¯å¦å¯è§£æ\n",
    "2. **å¼•ç”¨å‡†ç¡®æ€§**ï¼šLLM å¼•ç”¨çš„å®¢æˆ· ID æ˜¯å¦æ¥è‡ªæ£€ç´¢ç»“æœ\n",
    "3. **å†…å®¹ç›¸å…³æ€§**ï¼šåˆ†ææ˜¯å¦å›ç­”äº†ç”¨æˆ·é—®é¢˜\n",
    "\n",
    "## 4.1 æ ¼å¼åŒ–è¾“å‡º\n",
    "\n",
    "å°† JSON ç»“æœè½¬æ¢ä¸ºäººç±»å¯è¯»çš„æŠ¥å‘Šæ ¼å¼ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04nrdsnkq776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ“Š CUSTOMER CHURN ANALYSIS REPORT\n",
      "============================================================\n",
      "\n",
      "ğŸ“ Summary:\n",
      "   The analysis of customer feedback reveals that the primary reasons for churn are related to inadequate internet service availability, high pricing, and poor customer service experiences. Customers express dissatisfaction with billing issues and payment methods, which further exacerbate their frustrations leading to their decision to leave.\n",
      "\n",
      "ğŸ” Top Reasons for Churn:\n",
      "   1. Lack of internet service availability in certain areas\n",
      "   2. High pricing for internet services\n",
      "   3. Poor customer service and billing issues\n",
      "\n",
      "âš ï¸ Risk Level: HIGH\n",
      "\n",
      "ğŸ’¡ Recommended Actions:\n",
      "   1. Expand internet service availability in underserved areas to attract and retain customers.\n",
      "   2. Review and adjust pricing strategies to ensure competitiveness and perceived value.\n",
      "   3. Enhance customer service training and streamline billing processes to improve customer satisfaction.\n",
      "\n",
      "ğŸ“ Citations (Customer IDs):\n",
      "   4877-EVATK, 1871-MOWRM, 5687-DKDTV, 6892-EZDTG, 8065-YKXKD\n",
      "\n",
      "ğŸ“š Retrieved Documents: 5\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Phase 4: è¾“å‡ºä¸è¯„ä¼° (Output & Evaluation)\n",
    "# ============================================\n",
    "\n",
    "# --------------------------------------------\n",
    "# 4.1 æ ¼å¼åŒ–è¾“å‡ºå‡½æ•°\n",
    "# --------------------------------------------\n",
    "\n",
    "def display_analysis(result):\n",
    "    \"\"\"\n",
    "    ç¾åŒ–æ˜¾ç¤ºåˆ†æç»“æœ\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ğŸ“Š CUSTOMER CHURN ANALYSIS REPORT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if \"error\" in result:\n",
    "        print(f\"âŒ Error: {result['error']}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nğŸ“ Summary:\")\n",
    "    print(f\"   {result.get('summary', 'N/A')}\")\n",
    "    \n",
    "    print(f\"\\nğŸ” Top Reasons for Churn:\")\n",
    "    for i, reason in enumerate(result.get('top_reasons', []), 1):\n",
    "        print(f\"   {i}. {reason}\")\n",
    "    \n",
    "    print(f\"\\nâš ï¸ Risk Level: {result.get('risk_level', 'N/A').upper()}\")\n",
    "    \n",
    "    print(f\"\\nğŸ’¡ Recommended Actions:\")\n",
    "    for i, action in enumerate(result.get('actions', []), 1):\n",
    "        print(f\"   {i}. {action}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“ Citations (Customer IDs):\")\n",
    "    print(f\"   {', '.join(result.get('citations', []))}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“š Retrieved Documents: {result.get('retrieved_docs', 0)}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "# æµ‹è¯•æ ¼å¼åŒ–è¾“å‡º\n",
    "display_analysis(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9barvq76hvm",
   "metadata": {},
   "source": [
    "## 4.2 å¼•ç”¨éªŒè¯ (Citation Validation)\n",
    "\n",
    "### ä»€ä¹ˆæ˜¯å¼•ç”¨éªŒè¯ï¼Ÿ\n",
    "\n",
    "å¼•ç”¨éªŒè¯æ£€æŸ¥ LLM ç”Ÿæˆçš„å®¢æˆ· ID å¼•ç”¨æ˜¯å¦**çœŸå®å­˜åœ¨äºæ£€ç´¢ç»“æœä¸­**ã€‚\n",
    "\n",
    "è¿™æ˜¯é˜²æ­¢ **LLM å¹»è§‰ (Hallucination)** çš„é‡è¦æªæ–½ã€‚å¹»è§‰æŒ‡ LLM ç”Ÿæˆçœ‹ä¼¼åˆç†ä½†å®é™…ä¸å­˜åœ¨çš„ä¿¡æ¯ã€‚\n",
    "\n",
    "### éªŒè¯é€»è¾‘\n",
    "\n",
    "```mermaid\n",
    "%%{init: {'theme': 'dark', 'themeVariables': { 'fontSize': '13px'}}}%%\n",
    "flowchart LR\n",
    "    A[\"LLMè¿”å›<br/>citations\"] --> B{\"å¯¹æ¯”éªŒè¯\"}\n",
    "    C[\"æ£€ç´¢è¿”å›<br/>customer_ids\"] --> B\n",
    "    B --> D[\"è®¡ç®—<br/>å‡†ç¡®ç‡\"]\n",
    "\n",
    "    style A fill:#4a1942,stroke:#ff6b9d,color:#fff\n",
    "    style B fill:#1a1a2e,stroke:#ffd700,color:#fff\n",
    "    style C fill:#2d4a3e,stroke:#00ff88,color:#fff\n",
    "    style D fill:#3d1a4a,stroke:#bf7fff,color:#fff\n",
    "```\n",
    "\n",
    "### ä¸ºä»€ä¹ˆéœ€è¦ ID æ ‡å‡†åŒ–ï¼Ÿ\n",
    "\n",
    "LLM å¯èƒ½è¿”å›ä¸åŒæ ¼å¼çš„ IDï¼š\n",
    "- `7590-VHVEG`ï¼ˆæ­£ç¡®ï¼‰\n",
    "- `customerID_7590-VHVEG`ï¼ˆæ·»åŠ äº†å‰ç¼€ï¼‰\n",
    "- `7590-vhveg`ï¼ˆå¤§å°å†™ä¸åŒï¼‰\n",
    "\n",
    "æˆ‘ä»¬çš„ `normalize_customer_id` å‡½æ•°å¤„ç†è¿™äº›å˜ä½“ï¼Œç¡®ä¿éªŒè¯çš„é²æ£’æ€§ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fletycwbo1n",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¼•ç”¨éªŒè¯ç»“æœ:\n",
      "  æ€»å¼•ç”¨æ•°: 5\n",
      "  æœ‰æ•ˆå¼•ç”¨: ['4877-EVATK', '1871-MOWRM', '5687-DKDTV', '6892-EZDTG', '8065-YKXKD']\n",
      "  æ— æ•ˆå¼•ç”¨: []\n",
      "  å¼•ç”¨å‡†ç¡®ç‡: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# 4.2 å¼•ç”¨éªŒè¯\n",
    "# --------------------------------------------\n",
    "\n",
    "def normalize_customer_id(cid):\n",
    "    \"\"\"\n",
    "    æ ‡å‡†åŒ–å®¢æˆ· ID, å¤„ç†å„ç§æ ¼å¼\n",
    "    \"\"\"\n",
    "    cid = cid.strip().lower()\n",
    "    # ç§»é™¤å¯èƒ½çš„å‰ç¼€\n",
    "    prefixes = ['customerid_', 'customer_id_', 'cid_', 'id_']\n",
    "    for prefix in prefixes:\n",
    "        if cid.startswith(prefix):\n",
    "            cid = cid[len(prefix):]\n",
    "    return cid\n",
    "\n",
    "def validate_citations(result):\n",
    "    \"\"\"\n",
    "    éªŒè¯å¼•ç”¨çš„å®¢æˆ· ID æ˜¯å¦åœ¨æ£€ç´¢ç»“æœä¸­\n",
    "    \"\"\"\n",
    "    citations = result.get('citations', [])\n",
    "    retrieved_ids = result.get('retrieved_customer_ids', [])\n",
    "    \n",
    "    # æ ‡å‡†åŒ–æ‰€æœ‰ ID\n",
    "    retrieved_normalized = {normalize_customer_id(rid): rid for rid in retrieved_ids}\n",
    "    \n",
    "    valid_citations = []\n",
    "    invalid_citations = []\n",
    "    \n",
    "    for cid in citations:\n",
    "        normalized = normalize_customer_id(cid)\n",
    "        if normalized in retrieved_normalized:\n",
    "            valid_citations.append(cid)\n",
    "        else:\n",
    "            invalid_citations.append(cid)\n",
    "    \n",
    "    accuracy = len(valid_citations) / len(citations) if citations else 0\n",
    "    \n",
    "    return {\n",
    "        \"total_citations\": len(citations),\n",
    "        \"valid_citations\": valid_citations,\n",
    "        \"invalid_citations\": invalid_citations,\n",
    "        \"accuracy\": accuracy\n",
    "    }\n",
    "\n",
    "# éªŒè¯ç»“æœ\n",
    "validation = validate_citations(result)\n",
    "print(\"å¼•ç”¨éªŒè¯ç»“æœ:\")\n",
    "print(f\"  æ€»å¼•ç”¨æ•°: {validation['total_citations']}\")\n",
    "print(f\"  æœ‰æ•ˆå¼•ç”¨: {validation['valid_citations']}\")\n",
    "print(f\"  æ— æ•ˆå¼•ç”¨: {validation['invalid_citations']}\")\n",
    "print(f\"  å¼•ç”¨å‡†ç¡®ç‡: {validation['accuracy']*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kd6bjstt3g",
   "metadata": {},
   "source": [
    "### è¾“å‡ºåˆ†æ\n",
    "\n",
    "å¼•ç”¨éªŒè¯ç»“æœæ˜¾ç¤º **100% å‡†ç¡®ç‡**ï¼Œæ‰€æœ‰å¼•ç”¨éƒ½æ˜¯æœ‰æ•ˆçš„ã€‚è¿™è¯´æ˜ï¼š\n",
    "1. Prompt è®¾è®¡æœ‰æ•ˆï¼ŒLLM éµå¾ªäº†\"åªå¼•ç”¨æä¾›çš„æ•°æ®\"çš„æŒ‡ä»¤\n",
    "2. ç³»ç»Ÿæ²¡æœ‰äº§ç”Ÿå¹»è§‰\n",
    "\n",
    "**å¦‚æœå‡†ç¡®ç‡ä½äº 100%**ï¼Œå¯èƒ½éœ€è¦ï¼š\n",
    "- è°ƒæ•´ Promptï¼Œå¼ºè°ƒå¼•ç”¨çº¦æŸ\n",
    "- æ£€æŸ¥æ£€ç´¢è´¨é‡ï¼Œç¡®ä¿ç›¸å…³æ–‡æ¡£è¢«æ£€ç´¢åˆ°\n",
    "- è€ƒè™‘ä½¿ç”¨æ›´å¤§å‚æ•°çš„å¼€æºæ¨¡å‹ï¼ˆå¦‚ Qwen2.5-14Bï¼‰\n",
    "\n",
    "## 4.3 äº¤äº’å¼åˆ†æç•Œé¢\n",
    "\n",
    "å°è£…å®Œæ•´çš„åˆ†ææµç¨‹ï¼Œæä¾›ç®€æ´çš„è°ƒç”¨æ¥å£ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "h8z02dz6msq",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ‰ Customer Churn Insight Tool Ready!\n",
      "============================================================\n",
      "\n",
      "ä½¿ç”¨æ–¹æ³•: result = analyze_churn('your query here')\n",
      "\n",
      "ç¤ºä¾‹æŸ¥è¯¢:\n",
      "  1. 'Why are customers with fiber optic internet churning?'\n",
      "  2. 'What issues do senior citizens have with our service?'\n",
      "  3. 'What are the common complaints about customer support?'\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# 4.3 äº¤äº’å¼åˆ†æç•Œé¢\n",
    "# --------------------------------------------\n",
    "\n",
    "def analyze_churn(query):\n",
    "    \"\"\"\n",
    "    å®Œæ•´çš„æµå¤±åˆ†æå‡½æ•°\n",
    "    \n",
    "    Args:\n",
    "        query: åˆ†ææŸ¥è¯¢\n",
    "    \n",
    "    Returns:\n",
    "        åˆ†æç»“æœ\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ” Analyzing: {query}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # æ‰§è¡Œ RAG æŸ¥è¯¢\n",
    "    result = rag_query(query, k=5)\n",
    "    \n",
    "    # æ˜¾ç¤ºç»“æœ\n",
    "    display_analysis(result)\n",
    "    \n",
    "    # éªŒè¯å¼•ç”¨\n",
    "    validation = validate_citations(result)\n",
    "    print(f\"\\nâœ… Citation Accuracy: {validation['accuracy']*100:.1f}%\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# ç¤ºä¾‹æŸ¥è¯¢\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ‰ Customer Churn Insight Tool Ready!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nä½¿ç”¨æ–¹æ³•: result = analyze_churn('your query here')\")\n",
    "print(\"\\nç¤ºä¾‹æŸ¥è¯¢:\")\n",
    "print(\"  1. 'Why are customers with fiber optic internet churning?'\")\n",
    "print(\"  2. 'What issues do senior citizens have with our service?'\")\n",
    "print(\"  3. 'What are the common complaints about customer support?'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1qemegspvm",
   "metadata": {},
   "source": [
    "## 4.4 å®Œæ•´ç¤ºä¾‹\n",
    "\n",
    "è¿è¡Œä¸€ä¸ªå®Œæ•´çš„åˆ†ææŸ¥è¯¢ï¼Œå±•ç¤ºç³»ç»Ÿçš„ç«¯åˆ°ç«¯èƒ½åŠ›ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa5dst734k",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Analyzing: Why are customers with fiber optic internet churning?\n",
      "--------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š CUSTOMER CHURN ANALYSIS REPORT\n",
      "============================================================\n",
      "\n",
      "ğŸ“ Summary:\n",
      "   The analysis indicates that customers with fiber optic internet are primarily churning due to service disruptions and inconsistent speeds. While some customers appreciate the initial pricing, the overall dissatisfaction with reliability and customer support significantly contributes to their decision to leave.\n",
      "\n",
      "ğŸ” Top Reasons for Churn:\n",
      "   1. Frequent service disruptions\n",
      "   2. Inconsistent internet speeds\n",
      "   3. Poor customer support\n",
      "\n",
      "âš ï¸ Risk Level: HIGH\n",
      "\n",
      "ğŸ’¡ Recommended Actions:\n",
      "   1. Enhance service reliability through infrastructure improvements\n",
      "   2. Implement a proactive customer support system with quicker response times\n",
      "   3. Gather customer feedback regularly to address service issues promptly\n",
      "\n",
      "ğŸ“ Citations (Customer IDs):\n",
      "   6680-WKXRZ, 8065-YKXKD\n",
      "\n",
      "ğŸ“š Retrieved Documents: 5\n",
      "============================================================\n",
      "\n",
      "âœ… Citation Accuracy: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# ç¤ºä¾‹: è¿è¡Œå®Œæ•´åˆ†æ\n",
    "# --------------------------------------------\n",
    "\n",
    "# å–æ¶ˆæ³¨é‡Šä»¥è¿è¡Œåˆ†æ\n",
    "result = analyze_churn(\"Why are customers with fiber optic internet churning?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dwkkejfrqg",
   "metadata": {},
   "source": [
    "### è¾“å‡ºåˆ†æ\n",
    "\n",
    "å®Œæ•´åˆ†ææµç¨‹æ‰§è¡ŒæˆåŠŸï¼š\n",
    "1. âœ… æ£€ç´¢åˆ°ç›¸å…³å®¢æˆ·æ•°æ®\n",
    "2. âœ… LLM ç”Ÿæˆäº†ç»“æ„åŒ–åˆ†ææŠ¥å‘Š\n",
    "3. âœ… å¼•ç”¨éªŒè¯é€šè¿‡ï¼ˆ100% å‡†ç¡®ç‡ï¼‰\n",
    "\n",
    "åˆ†ææ­ç¤ºäº†å…‰çº¤ç”¨æˆ·æµå¤±çš„ä¸»è¦åŸå› ï¼šæœåŠ¡ä¸­æ–­é¢‘ç¹ã€ç½‘é€Ÿä¸ç¨³å®šã€å®¢æœæ”¯æŒä¸è¶³ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "## æŠ€æœ¯æ ˆå›é¡¾\n",
    "\n",
    "| ç»„ä»¶ | æŠ€æœ¯ | ä½œç”¨ |\n",
    "|------|------|------|\n",
    "| æ•°æ®å¤„ç† | Pandas | åŠ è½½ã€æ¸…æ´—ã€è½¬æ¢æ•°æ® |\n",
    "| æ–‡æœ¬åµŒå…¥ | Sentence-Transformers (BGE) | å°†æ–‡æœ¬è½¬æ¢ä¸º 768 ç»´å‘é‡ |\n",
    "| å‘é‡æ£€ç´¢ | FAISS | é«˜æ•ˆç›¸ä¼¼åº¦æœç´¢ |\n",
    "| å…³é”®è¯æ£€ç´¢ | BM25 | ç²¾ç¡®å…³é”®è¯åŒ¹é… |\n",
    "| ç»“æœèåˆ | RRF | æ··åˆæ£€ç´¢ç»“æœæ’å |\n",
    "| æ–‡æœ¬ç”Ÿæˆ | Qwen2.5-7B-Instruct (4-bit) | åˆ†æå’Œæ¨ç† |\n",
    "| è¾“å‡ºæ ¼å¼ | JSON | ç»“æ„åŒ–ã€å¯è§£æçš„è¾“å‡º |\n",
    "\n",
    "## ç³»ç»Ÿç‰¹ç‚¹\n",
    "\n",
    "1. **å¯è¿½æº¯æ€§**ï¼šæ‰€æœ‰åˆ†æç»“è®ºéƒ½æœ‰å®¢æˆ· ID å¼•ç”¨æ”¯æŒ\n",
    "2. **å‡†ç¡®æ€§**ï¼šæ··åˆæ£€ç´¢ + å¼•ç”¨éªŒè¯ç¡®ä¿æ•°æ®å‡†ç¡®\n",
    "3. **å¯æ‰©å±•æ€§**ï¼šå¯è½»æ¾æ‰©å±•åˆ°æ›´å¤§æ•°æ®é›†ï¼ˆä½¿ç”¨è¿‘ä¼¼ç´¢å¼•ï¼‰\n",
    "4. **å¯è§£é‡Šæ€§**ï¼šè¾“å‡ºç»“æ„åŒ–ï¼Œä¾¿äºç†è§£å’Œä½¿ç”¨\n",
    "\n",
    "## åç»­æ”¹è¿›æ–¹å‘\n",
    "\n",
    "1. **æ£€ç´¢ä¼˜åŒ–**ï¼š\n",
    "   - æ·»åŠ  Rerankerï¼ˆå¦‚ cross-encoderï¼‰è¿›ä¸€æ­¥æå‡ç²¾åº¦\n",
    "   - å°è¯•ä¸åŒçš„åµŒå…¥æ¨¡å‹ï¼ˆå¦‚ bge-large-en-v1.5ï¼‰\n",
    "\n",
    "2. **LLM ä¼˜åŒ–**ï¼š\n",
    "   - ä½¿ç”¨ QLoRA å¾®è°ƒæå‡è¾“å‡ºè´¨é‡ï¼ˆè§ Phase 5-8ï¼‰\n",
    "   - å®ç°å¤šè½®å¯¹è¯ï¼Œæ”¯æŒè¿½é—®\n",
    "\n",
    "3. **è¯„ä¼°ä½“ç³»**ï¼š\n",
    "   - æ„å»ºæ ‡æ³¨æ•°æ®é›†è¿›è¡Œå®šé‡è¯„ä¼°\n",
    "   - å®ç° A/B æµ‹è¯•æ¡†æ¶\n",
    "\n",
    "4. **ç”Ÿäº§éƒ¨ç½²**ï¼š\n",
    "   - æŒä¹…åŒ–å‘é‡ç´¢å¼•ï¼ˆä¿å­˜/åŠ è½½ FAISS ç´¢å¼•ï¼‰\n",
    "   - æ·»åŠ  API æ¥å£ï¼ˆFastAPI/Flaskï¼‰\n",
    "   - å®ç°ç¼“å­˜æœºåˆ¶\n",
    "\n",
    "## ä½¿ç”¨ç¤ºä¾‹\n",
    "\n",
    "```python\n",
    "# åˆ†æå®¢æˆ·æµå¤±åŸå› \n",
    "result = analyze_churn(\"Why are customers with fiber optic internet churning?\")\n",
    "\n",
    "# åˆ†æç‰¹å®šç¾¤ä½“\n",
    "result = analyze_churn(\"What issues do senior citizens have with our service?\")\n",
    "\n",
    "# åˆ†ææœåŠ¡é—®é¢˜\n",
    "result = analyze_churn(\"What are the common complaints about customer support?\")\n",
    "\n",
    "# åˆ†æä»·æ ¼æ•æ„Ÿåº¦\n",
    "result = analyze_churn(\"Are customers leaving because of high prices?\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cdb964",
   "metadata": {},
   "source": [
    "## Phase 3-4 æ¨¡å‹æ¸…ç†\n",
    "\n",
    "åœ¨å¼€å§‹ Phase 5ï¼ˆåŠ è½½ 14B æ•™å¸ˆæ¨¡å‹ï¼‰ä¹‹å‰ï¼Œéœ€è¦é‡Šæ”¾ Phase 3-4 ä¸­åŠ è½½çš„ 7B æ¨¡å‹çš„æ˜¾å­˜ã€‚\n",
    "\n",
    "- 7B æ¨¡å‹å ç”¨ ~4.5 GB VRAM\n",
    "- 14B æ•™å¸ˆæ¨¡å‹éœ€è¦ ~8 GB VRAM\n",
    "- T4 æ€»å…± 15 GBï¼Œéœ€è¦å…ˆé‡Šæ”¾ 7B æ‰èƒ½åŠ è½½ 14B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bcaf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# é‡Šæ”¾ Phase 3-4 çš„æ¨¡å‹æ˜¾å­˜\n",
    "# ï¼ˆä¸º Phase 5 çš„ 14B æ•™å¸ˆæ¨¡å‹è…¾å‡ºç©ºé—´ï¼‰\n",
    "# âš ï¸ åœ¨ Google Colab ä¸Šè¿è¡Œ\n",
    "# --------------------------------------------\n",
    "\n",
    "import gc\n",
    "\n",
    "print(f\"é‡Šæ”¾å‰ GPU æ˜¾å­˜: {torch.cuda.memory_allocated() / 1024**3:.1f} GB\")\n",
    "\n",
    "# é‡Šæ”¾ Phase 3 åŠ è½½çš„ 7B æ¨¡å‹\n",
    "if 'model' in dir() and model is not None:\n",
    "    del model\n",
    "if 'tokenizer' in dir() and tokenizer is not None:\n",
    "    del tokenizer\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"é‡Šæ”¾å GPU æ˜¾å­˜: {torch.cuda.memory_allocated() / 1024**3:.1f} GB\")\n",
    "print(\"\\n7B æ¨¡å‹å·²é‡Šæ”¾ï¼Œå¯ä»¥åŠ è½½ 14B æ•™å¸ˆæ¨¡å‹ âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3378018",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Phase 5: è®­ç»ƒæ•°æ®å‡†å¤‡ï¼ˆTraining Data Generationï¼‰\n",
    "\n",
    "> **âš ï¸ ä» Phase 5 å¼€å§‹ï¼Œæ‰€æœ‰ä»£ç å‡åœ¨ Google Colabï¼ˆå…è´¹ T4 GPUï¼‰ä¸Šè¿è¡Œ**\n",
    "\n",
    "## ç›®æ ‡\n",
    "\n",
    "ä½¿ç”¨ **Qwen2.5-14B-Instruct**ï¼ˆ4-bit é‡åŒ–ï¼‰ä½œä¸ºæ•™å¸ˆæ¨¡å‹ï¼Œä¸ºæ¯æ¡æŸ¥è¯¢ç”Ÿæˆé«˜è´¨é‡çš„ JSON æ ¼å¼è®­ç»ƒæ•°æ®ï¼Œç”¨äºåç»­å¾®è°ƒ 7B å­¦ç”Ÿæ¨¡å‹ã€‚\n",
    "\n",
    "## ç­–ç•¥\n",
    "\n",
    "| æ­¥éª¤ | è¯´æ˜ |\n",
    "|------|------|\n",
    "| 5.0 | Colab ç¯å¢ƒæ­å»º + é‡å»ºæ£€ç´¢ç³»ç»Ÿ |\n",
    "| 5.1 | è®¾è®¡ 8 ç±»æŸ¥è¯¢æ¨¡æ¿ï¼Œç”Ÿæˆ ~60-80 æ¡å”¯ä¸€æŸ¥è¯¢ |\n",
    "| 5.2 | ç”¨ 14B æ•™å¸ˆæ¨¡å‹ç”Ÿæˆ Gold Standard è®­ç»ƒæ•°æ® |\n",
    "| 5.3 | æ•°æ®å¢å¼ºï¼ˆk=3,5,7 å¤šè½®ç”Ÿæˆï¼‰ |\n",
    "| 5.4 | åˆ’åˆ†è®­ç»ƒé›†/éªŒè¯é›†ï¼ˆ85%/15%ï¼‰ |\n",
    "| 5.5 | å¯¼å‡º ChatML æ ¼å¼ JSONL |\n",
    "| 5.6 | é‡Šæ”¾æ•™å¸ˆæ¨¡å‹æ˜¾å­˜ |\n",
    "\n",
    "### VRAM é¢„ä¼°\n",
    "\n",
    "- BGE åµŒå…¥æ¨¡å‹: ~0.5 GB\n",
    "- Qwen2.5-14B-Instruct (4-bit): ~8 GB\n",
    "- **åˆè®¡: ~8.5 GB â†’ T4 (15 GB) å®‰å…¨**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac80377",
   "metadata": {},
   "source": [
    "## 5.0 Colab ç¯å¢ƒæ­å»º\n",
    "\n",
    "åœ¨ Colab ä¸Šå®‰è£…æ‰€æœ‰ä¾èµ–å¹¶é‡å»ºæ£€ç´¢ç³»ç»Ÿã€‚ç”±äº Colab æ˜¯å…¨æ–°ç¯å¢ƒï¼Œéœ€è¦é‡æ–°åŠ è½½æ•°æ®å’Œæ„å»ºç´¢å¼•ã€‚\n",
    "\n",
    "**å®‰è£…çš„æ–°ä¾èµ–**ï¼š\n",
    "- `peft`: LoRA é€‚é…å™¨æ¡†æ¶\n",
    "- `bitsandbytes`: 4-bit / 8-bit é‡åŒ–æ”¯æŒ\n",
    "- `trl`: Transformer Reinforcement Learningï¼Œæä¾› `SFTTrainer`\n",
    "- `datasets`: Hugging Face æ•°æ®é›†åŠ è½½\n",
    "- `accelerate`: åˆ†å¸ƒå¼è®­ç»ƒå’Œæ··åˆç²¾åº¦æ”¯æŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53777be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Phase 5: è®­ç»ƒæ•°æ®å‡†å¤‡ (Training Data Generation)\n",
    "# âš ï¸ åœ¨ Google Colab (T4 GPU) ä¸Šè¿è¡Œ\n",
    "# ============================================\n",
    "\n",
    "# --------------------------------------------\n",
    "# 5.0 Colab ç¯å¢ƒæ­å»º\n",
    "# --------------------------------------------\n",
    "\n",
    "# å®‰è£…ä¾èµ–\n",
    "!pip install -q transformers peft bitsandbytes trl datasets accelerate\n",
    "!pip install -q sentence-transformers faiss-cpu rank_bm25\n",
    "\n",
    "# æŒ‚è½½ Google Driveï¼ˆç”¨äºåç»­ä¿å­˜æ¨¡å‹ adapterï¼‰\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# æ£€æŸ¥ GPU å¯ç”¨æ€§\n",
    "print(f\"GPU å¯ç”¨: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU å‹å·: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "print(\"\\nç¯å¢ƒé…ç½®å®Œæˆ âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e80abc2",
   "metadata": {},
   "source": [
    "### ä¸Šä¼ æ•°æ®å¹¶é‡å»ºæ£€ç´¢ç³»ç»Ÿ\n",
    "\n",
    "å¤ç”¨ Phase 1-2 çš„é€»è¾‘ï¼šåŠ è½½ CSV â†’ åˆ›å»ºå®¢æˆ·æ–‡æ¡£ â†’ æ„å»º FAISS + BM25 ç´¢å¼•ã€‚\n",
    "\n",
    "**æ³¨æ„**ï¼šè¯·å…ˆå°† `telco_churn_with_all_feedback.csv` ä¸Šä¼ åˆ° Colabï¼Œæˆ–æ”¾å…¥ Google Drive ä¸­ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b660f47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# 5.0b åŠ è½½æ•°æ® + é‡å»ºæ£€ç´¢ç³»ç»Ÿ\n",
    "# [å¯é€‰] å¦‚æœ Phase 1-2 çš„å˜é‡ä»åœ¨å†…å­˜ä¸­å¯è·³è¿‡\n",
    "# ï¼ˆå¤ç”¨ Phase 1-2 é€»è¾‘ï¼‰\n",
    "# âš ï¸ åœ¨ Google Colab ä¸Šè¿è¡Œ\n",
    "# --------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# === åŠ è½½æ•°æ® ===\n",
    "# æ–¹æ³•1: ä» Colab ä¸Šä¼ \n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "\n",
    "# æ–¹æ³•2: ä» Google Drive è¯»å–ï¼ˆæ¨èï¼‰\n",
    "DATA_PATH = '/content/drive/MyDrive/telco_churn_with_all_feedback.csv'\n",
    "\n",
    "# å¦‚æœ Drive ä¸­æ²¡æœ‰ï¼Œå°è¯•å½“å‰ç›®å½•\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    DATA_PATH = '/content/telco_churn_with_all_feedback.csv'\n",
    "    if not os.path.exists(DATA_PATH):\n",
    "        from google.colab import files\n",
    "        print(\"è¯·ä¸Šä¼  telco_churn_with_all_feedback.csv æ–‡ä»¶:\")\n",
    "        uploaded = files.upload()\n",
    "        DATA_PATH = list(uploaded.keys())[0]\n",
    "\n",
    "df_main = pd.read_csv(DATA_PATH)\n",
    "print(f\"æ•°æ®åŠ è½½å®Œæˆ: {df_main.shape}\")\n",
    "\n",
    "# === æ•°æ®é¢„å¤„ç†ï¼ˆåŒ Phase 1ï¼‰ ===\n",
    "df = df_main.copy()\n",
    "if 'PromptInput' in df.columns:\n",
    "    df = df.drop(columns=['PromptInput'])\n",
    "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "df['TotalCharges'] = df['TotalCharges'].fillna(df['MonthlyCharges'])\n",
    "df['Churn_Binary'] = (df['Churn'] == 'Yes').astype(int)\n",
    "\n",
    "# æ£€æŸ¥åé¦ˆå­—æ®µ\n",
    "df['has_feedback'] = df['CustomerFeedback'].notna() & (df['CustomerFeedback'].str.len() > 10)\n",
    "\n",
    "# === åˆ›å»ºå®¢æˆ·æ–‡æ¡£ï¼ˆåŒ Phase 1.7ï¼‰ ===\n",
    "def create_customer_document(row):\n",
    "    doc = f\"\"\"Customer ID: {row['customerID']}\n",
    "Churn Status: {row['Churn']}\n",
    "\n",
    "Customer Profile:\n",
    "- Gender: {row['gender']}\n",
    "- Senior Citizen: {'Yes' if row['SeniorCitizen'] == 1 else 'No'}\n",
    "- Partner: {row['Partner']}\n",
    "- Dependents: {row['Dependents']}\n",
    "- Tenure: {row['tenure']} months\n",
    "\n",
    "Services:\n",
    "- Phone Service: {row['PhoneService']}\n",
    "- Internet Service: {row['InternetService']}\n",
    "- Online Security: {row['OnlineSecurity']}\n",
    "- Tech Support: {row['TechSupport']}\n",
    "- Streaming TV: {row['StreamingTV']}\n",
    "- Streaming Movies: {row['StreamingMovies']}\n",
    "\n",
    "Contract & Billing:\n",
    "- Contract: {row['Contract']}\n",
    "- Monthly Charges: ${row['MonthlyCharges']}\n",
    "- Total Charges: ${row['TotalCharges']:.2f}\n",
    "- Payment Method: {row['PaymentMethod']}\n",
    "\n",
    "Customer Feedback:\n",
    "{row['CustomerFeedback']}\n",
    "\"\"\"\n",
    "    return doc\n",
    "\n",
    "df_with_feedback = df[df['has_feedback']].copy()\n",
    "df_with_feedback['document'] = df_with_feedback.apply(create_customer_document, axis=1)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "documents = df_with_feedback['document'].apply(preprocess_text).tolist()\n",
    "customer_ids = df_with_feedback['customerID'].tolist()\n",
    "\n",
    "# === æ„å»ºæ£€ç´¢ç³»ç»Ÿï¼ˆåŒ Phase 2ï¼‰ ===\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "print(\"åŠ è½½ BGE åµŒå…¥æ¨¡å‹...\")\n",
    "embedding_model = SentenceTransformer('BAAI/bge-base-en-v1.5')\n",
    "\n",
    "print(\"ç”Ÿæˆæ–‡æ¡£åµŒå…¥...\")\n",
    "document_embeddings = embedding_model.encode(\n",
    "    documents, show_progress_bar=True, normalize_embeddings=True\n",
    ")\n",
    "\n",
    "# FAISS ç´¢å¼•\n",
    "dimension = document_embeddings.shape[1]\n",
    "faiss_index = faiss.IndexFlatIP(dimension)\n",
    "faiss_index.add(document_embeddings.astype('float32'))\n",
    "\n",
    "# BM25 ç´¢å¼•\n",
    "tokenized_docs = [doc.lower().split() for doc in documents]\n",
    "bm25_index = BM25Okapi(tokenized_docs)\n",
    "\n",
    "# æ£€ç´¢å‡½æ•°ï¼ˆåŒ Phase 2.5ï¼‰\n",
    "def vector_search(query, k=10):\n",
    "    query_embedding = embedding_model.encode([query], normalize_embeddings=True)\n",
    "    scores, indices = faiss_index.search(query_embedding.astype('float32'), k)\n",
    "    return list(zip(indices[0], scores[0]))\n",
    "\n",
    "def bm25_search(query, k=10):\n",
    "    tokenized_query = query.lower().split()\n",
    "    scores = bm25_index.get_scores(tokenized_query)\n",
    "    top_indices = np.argsort(scores)[::-1][:k]\n",
    "    return [(idx, scores[idx]) for idx in top_indices]\n",
    "\n",
    "def hybrid_search(query, k=10, alpha=0.5):\n",
    "    vector_results = vector_search(query, k=k*2)\n",
    "    bm25_results = bm25_search(query, k=k*2)\n",
    "    rrf_scores = {}\n",
    "    rrf_k = 60\n",
    "    for rank, (idx, _) in enumerate(vector_results):\n",
    "        rrf_scores[idx] = rrf_scores.get(idx, 0) + alpha / (rrf_k + rank + 1)\n",
    "    for rank, (idx, _) in enumerate(bm25_results):\n",
    "        rrf_scores[idx] = rrf_scores.get(idx, 0) + (1 - alpha) / (rrf_k + rank + 1)\n",
    "    sorted_results = sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "    return sorted_results\n",
    "\n",
    "def build_context(search_results, max_docs=5):\n",
    "    context_parts = []\n",
    "    for idx, score in search_results[:max_docs]:\n",
    "        doc = documents[idx]\n",
    "        context_parts.append(f\"--- Document (Relevance: {score:.4f}) ---\\n{doc}\\n\")\n",
    "    return \"\\n\".join(context_parts)\n",
    "\n",
    "# Prompt æ¨¡æ¿ï¼ˆåŒ Phase 3.1ï¼‰\n",
    "SYSTEM_PROMPT = \"\"\"You are a customer churn analysis expert. Based on the provided customer feedback and profile data, analyze the root causes of churn, assess risk levels, and provide actionable recommendations.\n",
    "\n",
    "You must respond in the following JSON format:\n",
    "{\n",
    "    \"summary\": \"Brief overall summary of the analysis (2-3 sentences)\",\n",
    "    \"top_reasons\": [\"reason 1\", \"reason 2\", \"reason 3\"],\n",
    "    \"risk_level\": \"high/medium/low\",\n",
    "    \"actions\": [\"recommended action 1\", \"recommended action 2\", \"recommended action 3\"],\n",
    "    \"citations\": [\"7590-VHVEG\", \"5575-GNVDE\"]\n",
    "}\n",
    "\n",
    "Guidelines:\n",
    "- Base your analysis ONLY on the provided customer data\n",
    "- IMPORTANT: In citations, use ONLY the exact Customer ID format shown in the data (e.g., \"7590-VHVEG\"), NOT \"customerID_xxx\"\n",
    "- Risk level should be based on the proportion of churned customers and severity of issues\n",
    "- Actions should be specific and actionable\n",
    "- Always respond in valid JSON format\"\"\"\n",
    "\n",
    "USER_TEMPLATE = \"\"\"Query: {query}\n",
    "\n",
    "Relevant Customer Data:\n",
    "{context}\n",
    "\n",
    "Please analyze the above customer feedback and provide insights in the specified JSON format.\"\"\"\n",
    "\n",
    "print(f\"\\næ£€ç´¢ç³»ç»Ÿé‡å»ºå®Œæˆ:\")\n",
    "print(f\"  æ–‡æ¡£æ•°: {len(documents)}\")\n",
    "print(f\"  FAISS ç´¢å¼•: {faiss_index.ntotal} ä¸ªå‘é‡ ({dimension}ç»´)\")\n",
    "print(f\"  BM25 è¯æ±‡è¡¨: {len(bm25_index.idf)} ä¸ªè¯\")\n",
    "print(\"âœ“ Phase 5.0 å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bd40f5",
   "metadata": {},
   "source": [
    "## 5.1 è®¾è®¡æŸ¥è¯¢æ¨¡æ¿\n",
    "\n",
    "æˆ‘ä»¬è®¾è®¡ **8 ç±»æŸ¥è¯¢æ¨¡æ¿**ï¼Œæ¶µç›–ä¸åŒåˆ†æç»´åº¦ï¼š\n",
    "\n",
    "| ç±»åˆ« | è¯´æ˜ | ç¤ºä¾‹ |\n",
    "|------|------|------|\n",
    "| æœåŠ¡é—®é¢˜ | ç‰¹å®šæœåŠ¡ç›¸å…³çš„æµå¤±åˆ†æ | \"Why are fiber optic customers churning?\" |\n",
    "| äººå£ç»Ÿè®¡ | ä¸åŒå®¢æˆ·ç¾¤ä½“çš„ç‰¹å¾ | \"What issues do senior citizens face?\" |\n",
    "| åˆåŒç±»å‹ | åˆåŒä¸æµå¤±çš„å…³ç³» | \"Why do month-to-month customers leave?\" |\n",
    "| åœ¨ç½‘æ—¶é•¿ | ä¸åŒ tenure çš„æµå¤±æ¨¡å¼ | \"What are the concerns of new customers?\" |\n",
    "| æƒ…æ„Ÿåˆ†æ | å®¢æˆ·æƒ…ç»ªç›¸å…³æŸ¥è¯¢ | \"What are the most negative feedback themes?\" |\n",
    "| å®šä»·ç›¸å…³ | ä»·æ ¼æ•æ„Ÿåº¦åˆ†æ | \"How does pricing affect customer retention?\" |\n",
    "| å¯¹æ¯”åˆ†æ | ä¸åŒç¾¤ä½“é—´çš„å¯¹æ¯” | \"Compare churn between DSL and fiber optic users\" |\n",
    "| è¡ŒåŠ¨å»ºè®® | é¢å‘å†³ç­–çš„æŸ¥è¯¢ | \"What should we do to reduce churn?\" |\n",
    "\n",
    "é€šè¿‡å‚æ•°æ›¿æ¢ï¼ˆæœåŠ¡åç§°ã€å®¢æˆ·ç¾¤ä½“ç­‰ï¼‰ï¼Œæ¯ç±»ç”Ÿæˆ 8-10 æ¡æŸ¥è¯¢ï¼Œæ€»è®¡ ~60-80 æ¡å”¯ä¸€æŸ¥è¯¢ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a487067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# 5.1 è®¾è®¡æŸ¥è¯¢æ¨¡æ¿\n",
    "# âš ï¸ åœ¨ Google Colab ä¸Šè¿è¡Œ\n",
    "# --------------------------------------------\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# 8 ç±»æŸ¥è¯¢æ¨¡æ¿\n",
    "query_templates = {\n",
    "    \"service\": [\n",
    "        \"Why are customers with {service} churning?\",\n",
    "        \"What are the main complaints about {service}?\",\n",
    "        \"How does {service} quality affect customer retention?\",\n",
    "        \"What feedback do churned customers give about {service}?\",\n",
    "    ],\n",
    "    \"demographics\": [\n",
    "        \"What issues do {group} face with our services?\",\n",
    "        \"Why are {group} more likely to churn?\",\n",
    "        \"What are the common complaints from {group}?\",\n",
    "        \"How can we improve retention for {group}?\",\n",
    "    ],\n",
    "    \"contract\": [\n",
    "        \"Why do {contract} contract customers leave?\",\n",
    "        \"What are the churn patterns for {contract} contracts?\",\n",
    "        \"How does {contract} contract type affect customer satisfaction?\",\n",
    "    ],\n",
    "    \"tenure\": [\n",
    "        \"What are the concerns of customers with {tenure} tenure?\",\n",
    "        \"Why do customers with {tenure} tenure churn?\",\n",
    "        \"What feedback patterns exist for {tenure} tenure customers?\",\n",
    "    ],\n",
    "    \"sentiment\": [\n",
    "        \"What are the most negative feedback themes?\",\n",
    "        \"What do unhappy customers complain about most?\",\n",
    "        \"What are the key drivers of customer dissatisfaction?\",\n",
    "        \"What emotional patterns appear in churned customer feedback?\",\n",
    "        \"Which service issues generate the strongest negative reactions?\",\n",
    "    ],\n",
    "    \"pricing\": [\n",
    "        \"How does pricing affect customer retention?\",\n",
    "        \"What do customers say about value for money?\",\n",
    "        \"Are high-paying customers more likely to churn?\",\n",
    "        \"What pricing-related complaints lead to churn?\",\n",
    "        \"How do monthly charges relate to customer satisfaction?\",\n",
    "    ],\n",
    "    \"comparison\": [\n",
    "        \"Compare churn between {comp_a} and {comp_b} customers\",\n",
    "        \"What differences exist between {comp_a} and {comp_b} customer feedback?\",\n",
    "        \"Which group has higher churn risk: {comp_a} or {comp_b}?\",\n",
    "    ],\n",
    "    \"action\": [\n",
    "        \"What should we do to reduce churn among {target} customers?\",\n",
    "        \"What retention strategies would work for {target} customers?\",\n",
    "        \"What immediate actions can prevent {target} customers from leaving?\",\n",
    "        \"How can we improve the experience for {target} customers?\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# å‚æ•°é›†\n",
    "params = {\n",
    "    \"service\": [\"fiber optic internet\", \"DSL internet\", \"phone service\",\n",
    "                 \"online security\", \"tech support\", \"streaming TV\", \"streaming movies\"],\n",
    "    \"group\": [\"senior citizens\", \"customers without partners\",\n",
    "              \"customers with dependents\", \"young customers\", \"female customers\"],\n",
    "    \"contract\": [\"month-to-month\", \"one year\", \"two year\"],\n",
    "    \"tenure\": [\"less than 6 months\", \"6-12 months\", \"1-2 years\",\n",
    "               \"2-4 years\", \"over 5 years\"],\n",
    "    \"comp_a\": [\"DSL\", \"month-to-month\", \"senior citizen\", \"male\"],\n",
    "    \"comp_b\": [\"fiber optic\", \"two-year contract\", \"non-senior\", \"female\"],\n",
    "    \"target\": [\"high-value\", \"month-to-month\", \"fiber optic\", \"new\",\n",
    "               \"senior citizen\", \"long-tenure\", \"price-sensitive\"],\n",
    "}\n",
    "\n",
    "# ç”Ÿæˆæ‰€æœ‰æŸ¥è¯¢\n",
    "all_queries = []\n",
    "\n",
    "for category, templates in query_templates.items():\n",
    "    for template in templates:\n",
    "        if \"{service}\" in template:\n",
    "            for svc in params[\"service\"]:\n",
    "                all_queries.append({\"query\": template.format(service=svc), \"category\": category})\n",
    "        elif \"{group}\" in template:\n",
    "            for grp in params[\"group\"]:\n",
    "                all_queries.append({\"query\": template.format(group=grp), \"category\": category})\n",
    "        elif \"{contract}\" in template:\n",
    "            for ct in params[\"contract\"]:\n",
    "                all_queries.append({\"query\": template.format(contract=ct), \"category\": category})\n",
    "        elif \"{tenure}\" in template:\n",
    "            for tn in params[\"tenure\"]:\n",
    "                all_queries.append({\"query\": template.format(tenure=tn), \"category\": category})\n",
    "        elif \"{comp_a}\" in template:\n",
    "            for a, b in zip(params[\"comp_a\"], params[\"comp_b\"]):\n",
    "                all_queries.append({\"query\": template.format(comp_a=a, comp_b=b), \"category\": category})\n",
    "        elif \"{target}\" in template:\n",
    "            for tgt in params[\"target\"]:\n",
    "                all_queries.append({\"query\": template.format(target=tgt), \"category\": category})\n",
    "        else:\n",
    "            all_queries.append({\"query\": template, \"category\": category})\n",
    "\n",
    "# å»é‡\n",
    "seen = set()\n",
    "unique_queries = []\n",
    "for q in all_queries:\n",
    "    if q[\"query\"] not in seen:\n",
    "        seen.add(q[\"query\"])\n",
    "        unique_queries.append(q)\n",
    "\n",
    "print(f\"ç”ŸæˆæŸ¥è¯¢æ€»æ•°: {len(unique_queries)}\")\n",
    "print(f\"\\nå„ç±»åˆ«æŸ¥è¯¢æ•°:\")\n",
    "from collections import Counter\n",
    "cat_counts = Counter(q[\"category\"] for q in unique_queries)\n",
    "for cat, count in sorted(cat_counts.items()):\n",
    "    print(f\"  {cat}: {count}\")\n",
    "print(f\"\\nç¤ºä¾‹æŸ¥è¯¢:\")\n",
    "for q in unique_queries[:5]:\n",
    "    print(f\"  [{q['category']}] {q['query']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6f0015",
   "metadata": {},
   "source": [
    "## 5.2 åŠ è½½æ•™å¸ˆæ¨¡å‹ç”Ÿæˆ Gold Standard è®­ç»ƒæ•°æ®\n",
    "\n",
    "### æ•™å¸ˆæ¨¡å‹é€‰æ‹©ï¼šQwen2.5-14B-Instruct\n",
    "\n",
    "| å±æ€§ | è¯´æ˜ |\n",
    "|------|------|\n",
    "| æ¨¡å‹ | `Qwen/Qwen2.5-14B-Instruct` |\n",
    "| é‡åŒ– | 4-bit (NF4 + åŒé‡é‡åŒ–) |\n",
    "| VRAM | ~8 GB |\n",
    "| ä¼˜åŠ¿ | JSON ç»“æ„åŒ–è¾“å‡ºèƒ½åŠ›å¼ºï¼Œä¸­è‹±æ–‡å‡ä¼˜ |\n",
    "| è®¸å¯è¯ | Apache 2.0 |\n",
    "\n",
    "### ç”Ÿæˆæµç¨‹\n",
    "\n",
    "å¯¹æ¯æ¡æŸ¥è¯¢ï¼š\n",
    "1. è°ƒç”¨ `hybrid_search()` è·å– Top-K æ–‡æ¡£\n",
    "2. ç”¨ `SYSTEM_PROMPT` + `USER_TEMPLATE` æ„é€  prompt\n",
    "3. ç”¨ 14B æ•™å¸ˆæ¨¡å‹æ¨ç†ç”Ÿæˆ JSON å“åº”\n",
    "4. éªŒè¯ JSON åŒ…å«å…¨éƒ¨ 5 ä¸ªå¿…éœ€å­—æ®µï¼ˆsummary, top_reasons, risk_level, actions, citationsï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad8383d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# 5.2 åŠ è½½æ•™å¸ˆæ¨¡å‹ (Qwen2.5-14B-Instruct 4-bit)\n",
    "# âš ï¸ åœ¨ Google Colab ä¸Šè¿è¡Œ\n",
    "# --------------------------------------------\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import json\n",
    "import gc\n",
    "\n",
    "# 4-bit é‡åŒ–é…ç½®\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",             # NF4 é‡åŒ–ï¼ˆä¿¡æ¯è®ºæœ€ä¼˜ï¼‰\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # è®¡ç®—ä½¿ç”¨ bfloat16\n",
    "    bnb_4bit_use_double_quant=True,         # åŒé‡é‡åŒ–ï¼Œè¿›ä¸€æ­¥å‹ç¼©\n",
    ")\n",
    "\n",
    "TEACHER_MODEL_NAME = \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "\n",
    "print(f\"åŠ è½½æ•™å¸ˆæ¨¡å‹: {TEACHER_MODEL_NAME} (4-bit)...\")\n",
    "print(\"é¢„è®¡éœ€è¦ 3-5 åˆ†é’Ÿ...\")\n",
    "\n",
    "teacher_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    TEACHER_MODEL_NAME,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "teacher_model = AutoModelForCausalLM.from_pretrained(\n",
    "    TEACHER_MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "teacher_model.eval()\n",
    "\n",
    "# æ£€æŸ¥æ˜¾å­˜ä½¿ç”¨\n",
    "print(f\"\\næ•™å¸ˆæ¨¡å‹åŠ è½½å®Œæˆ âœ“\")\n",
    "print(f\"GPU æ˜¾å­˜ä½¿ç”¨: {torch.cuda.memory_allocated() / 1024**3:.1f} GB\")\n",
    "print(f\"GPU æ˜¾å­˜æ€»é‡: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0a985c",
   "metadata": {},
   "source": [
    "### ä½¿ç”¨æ•™å¸ˆæ¨¡å‹ç”Ÿæˆè®­ç»ƒæ•°æ®\n",
    "\n",
    "å¯¹æ¯æ¡æŸ¥è¯¢ï¼Œç”¨æ•™å¸ˆæ¨¡å‹ç”Ÿæˆç»“æ„åŒ– JSON å“åº”ï¼Œå¹¶è¿›è¡Œä¸¥æ ¼çš„æ ¼å¼éªŒè¯ã€‚\n",
    "\n",
    "**éªŒè¯è§„åˆ™**ï¼š\n",
    "- å¿…é¡»æ˜¯åˆæ³• JSON\n",
    "- å¿…é¡»åŒ…å« 5 ä¸ªå­—æ®µï¼š`summary`, `top_reasons`, `risk_level`, `actions`, `citations`\n",
    "- `risk_level` å¿…é¡»æ˜¯ `high`/`medium`/`low` ä¹‹ä¸€\n",
    "- å„åˆ—è¡¨å­—æ®µå¿…é¡»éç©º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60680ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# 5.2b ç”¨æ•™å¸ˆæ¨¡å‹ç”Ÿæˆè®­ç»ƒæ•°æ®\n",
    "# âš ï¸ åœ¨ Google Colab ä¸Šè¿è¡Œ\n",
    "# --------------------------------------------\n",
    "\n",
    "def generate_with_teacher(query, k=5, max_new_tokens=1024):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨æ•™å¸ˆæ¨¡å‹ç”Ÿæˆ RAG å“åº”\n",
    "\n",
    "    Args:\n",
    "        query: ç”¨æˆ·æŸ¥è¯¢\n",
    "        k: æ£€ç´¢æ–‡æ¡£æ•°\n",
    "        max_new_tokens: æœ€å¤§ç”Ÿæˆ token æ•°\n",
    "\n",
    "    Returns:\n",
    "        dict: è§£æåçš„ JSON ç»“æœï¼Œå¤±è´¥è¿”å› None\n",
    "    \"\"\"\n",
    "    # Step 1-2: æ£€ç´¢ + æ„å»ºä¸Šä¸‹æ–‡ï¼ˆä¸åŸ rag_query ç›¸åŒï¼‰\n",
    "    search_results = hybrid_search(query, k=k)\n",
    "    context = build_context(search_results, max_docs=k)\n",
    "\n",
    "    # Step 3: æ„é€  ChatML æ ¼å¼æ¶ˆæ¯\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": USER_TEMPLATE.format(query=query, context=context)},\n",
    "    ]\n",
    "\n",
    "    # ä½¿ç”¨ tokenizer çš„ chat template\n",
    "    text = teacher_tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    inputs = teacher_tokenizer(text, return_tensors=\"pt\").to(teacher_model.device)\n",
    "\n",
    "    # ç”Ÿæˆ\n",
    "    with torch.no_grad():\n",
    "        outputs = teacher_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=teacher_tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    # è§£ç ï¼ˆåªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼‰\n",
    "    generated_ids = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "    response_text = teacher_tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    # Step 4: è§£æ JSON\n",
    "    try:\n",
    "        # å°è¯•ç›´æ¥è§£æ\n",
    "        result = json.loads(response_text)\n",
    "    except json.JSONDecodeError:\n",
    "        # å°è¯•æå– JSON å—ï¼ˆæ¨¡å‹å¯èƒ½åœ¨ JSON å‰åæ·»åŠ äº†æ–‡æœ¬ï¼‰\n",
    "        import re\n",
    "        json_match = re.search(r'\\{[\\s\\S]*\\}', response_text)\n",
    "        if json_match:\n",
    "            try:\n",
    "                result = json.loads(json_match.group())\n",
    "            except json.JSONDecodeError:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # éªŒè¯å¿…éœ€å­—æ®µ\n",
    "    required_fields = ['summary', 'top_reasons', 'risk_level', 'actions', 'citations']\n",
    "    if not all(field in result for field in required_fields):\n",
    "        return None\n",
    "\n",
    "    # éªŒè¯å­—æ®µç±»å‹å’Œå€¼\n",
    "    if not isinstance(result['top_reasons'], list) or len(result['top_reasons']) == 0:\n",
    "        return None\n",
    "    if result['risk_level'] not in ['high', 'medium', 'low']:\n",
    "        return None\n",
    "    if not isinstance(result['actions'], list) or len(result['actions']) == 0:\n",
    "        return None\n",
    "    if not isinstance(result['citations'], list):\n",
    "        return None\n",
    "\n",
    "    # æ·»åŠ æ£€ç´¢å…ƒä¿¡æ¯\n",
    "    result['retrieved_customer_ids'] = [customer_ids[idx] for idx, _ in search_results]\n",
    "\n",
    "    return result\n",
    "\n",
    "# æµ‹è¯•æ•™å¸ˆæ¨¡å‹ç”Ÿæˆ\n",
    "print(\"æµ‹è¯•æ•™å¸ˆæ¨¡å‹ç”Ÿæˆ...\")\n",
    "test_result = generate_with_teacher(\"Why are fiber optic customers churning?\", k=5)\n",
    "if test_result:\n",
    "    print(\"âœ“ æ•™å¸ˆæ¨¡å‹ç”Ÿæˆæµ‹è¯•é€šè¿‡\")\n",
    "    print(f\"  summary: {test_result['summary'][:80]}...\")\n",
    "    print(f\"  risk_level: {test_result['risk_level']}\")\n",
    "    print(f\"  top_reasons: {len(test_result['top_reasons'])} æ¡\")\n",
    "    print(f\"  actions: {len(test_result['actions'])} æ¡\")\n",
    "    print(f\"  citations: {test_result['citations']}\")\n",
    "else:\n",
    "    print(\"âœ— æ•™å¸ˆæ¨¡å‹ç”Ÿæˆæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ¨¡å‹è¾“å‡º\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b85b9c",
   "metadata": {},
   "source": [
    "## 5.3 æ•°æ®å¢å¼º\n",
    "\n",
    "å¯¹æ¯æ¡æŸ¥è¯¢ï¼Œåˆ†åˆ«ä½¿ç”¨ **k=3, 5, 7**ï¼ˆæ£€ç´¢æ–‡æ¡£æ•°ï¼‰ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼Œæ€»é‡çº¦ **180-240 æ¡**ã€‚\n",
    "\n",
    "ä¸åŒ k å€¼çš„æ„ä¹‰ï¼š\n",
    "- **k=3**: å°‘é‡é«˜ç›¸å…³æ–‡æ¡£ â†’ æ¨¡å‹å­¦ä¹ ç²¾ç¡®åˆ†æ\n",
    "- **k=5**: æ ‡å‡†æ£€ç´¢é‡ â†’ ä¸æ¨ç†æ—¶ä¸€è‡´\n",
    "- **k=7**: æ›´å¤šä¸Šä¸‹æ–‡ â†’ æ¨¡å‹å­¦ä¹ å¤„ç†å™ªå£°ä¿¡æ¯\n",
    "\n",
    "## 5.4 åˆ’åˆ†è®­ç»ƒé›†/éªŒè¯é›†\n",
    "\n",
    "ä½¿ç”¨ **85%/15%** çš„åˆ†å±‚åˆ’åˆ†ï¼ŒæŒ‰æŸ¥è¯¢ç±»åˆ«åˆ†å±‚ç¡®ä¿å„ç±»åˆ«å‡åŒ€åˆ†å¸ƒã€‚\n",
    "\n",
    "## 5.5 å¯¼å‡º ChatML æ ¼å¼\n",
    "\n",
    "ä½¿ç”¨ Qwen2.5 çš„ ChatML æ¨¡æ¿æ ¼å¼ï¼Œæ¯æ¡æ•°æ®åŒ…å«å®Œæ•´çš„ system + user + assistant ä¸‰è½®å¯¹è¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742998c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# 5.3 æ•°æ®å¢å¼º + 5.4 åˆ’åˆ† + 5.5 å¯¼å‡º\n",
    "# âš ï¸ åœ¨ Google Colab ä¸Šè¿è¡Œ\n",
    "# --------------------------------------------\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === 5.3 æ•°æ®å¢å¼ºï¼šå¤š k å€¼ç”Ÿæˆ ===\n",
    "training_samples = []\n",
    "failed_count = 0\n",
    "k_values = [3, 5, 7]\n",
    "\n",
    "print(\"å¼€å§‹ç”Ÿæˆè®­ç»ƒæ•°æ®...\")\n",
    "print(f\"æŸ¥è¯¢æ•°: {len(unique_queries)}, k å€¼: {k_values}\")\n",
    "print(f\"é¢„è®¡ç”Ÿæˆ: {len(unique_queries) * len(k_values)} æ¡æ ·æœ¬\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"\\n--- ä½¿ç”¨ k={k} ç”Ÿæˆ ---\")\n",
    "    for i, q_info in enumerate(tqdm(unique_queries, desc=f\"k={k}\")):\n",
    "        query = q_info[\"query\"]\n",
    "        category = q_info[\"category\"]\n",
    "\n",
    "        result = generate_with_teacher(query, k=k)\n",
    "\n",
    "        if result is not None:\n",
    "            # æ„å»ºä¸Šä¸‹æ–‡ï¼ˆä¸ç”Ÿæˆæ—¶ç›¸åŒï¼‰\n",
    "            search_results = hybrid_search(query, k=k)\n",
    "            context = build_context(search_results, max_docs=k)\n",
    "\n",
    "            # æ„å»ºè®­ç»ƒæ ·æœ¬\n",
    "            sample = {\n",
    "                \"query\": query,\n",
    "                \"category\": category,\n",
    "                \"k\": k,\n",
    "                \"context\": context,\n",
    "                \"response\": json.dumps(result, ensure_ascii=False),\n",
    "            }\n",
    "            training_samples.append(sample)\n",
    "        else:\n",
    "            failed_count += 1\n",
    "\n",
    "        # æ¯ 20 æ¡æ¸…ç†ä¸€æ¬¡ GPU ç¼“å­˜\n",
    "        if (i + 1) % 20 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\nç”Ÿæˆå®Œæˆ:\")\n",
    "print(f\"  æˆåŠŸ: {len(training_samples)} æ¡\")\n",
    "print(f\"  å¤±è´¥: {failed_count} æ¡\")\n",
    "print(f\"  æˆåŠŸç‡: {len(training_samples)/(len(training_samples)+failed_count)*100:.1f}%\")\n",
    "\n",
    "# === 5.4 åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†ï¼ˆ85%/15%ï¼ŒæŒ‰ç±»åˆ«åˆ†å±‚ï¼‰ ===\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# æŒ‰ç±»åˆ«åˆ†å±‚åˆ’åˆ†\n",
    "categories = [s[\"category\"] for s in training_samples]\n",
    "train_samples, val_samples = train_test_split(\n",
    "    training_samples, test_size=0.15, random_state=42, stratify=categories\n",
    ")\n",
    "\n",
    "print(f\"\\næ•°æ®åˆ’åˆ†:\")\n",
    "print(f\"  è®­ç»ƒé›†: {len(train_samples)} æ¡\")\n",
    "print(f\"  éªŒè¯é›†: {len(val_samples)} æ¡\")\n",
    "\n",
    "# === 5.5 å¯¼å‡º ChatML æ ¼å¼ JSONL ===\n",
    "def to_chatml(sample):\n",
    "    \"\"\"å°†æ ·æœ¬è½¬æ¢ä¸º Qwen2.5 ChatML æ ¼å¼\"\"\"\n",
    "    user_content = USER_TEMPLATE.format(\n",
    "        query=sample[\"query\"],\n",
    "        context=sample[\"context\"]\n",
    "    )\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "        {\"role\": \"assistant\", \"content\": sample[\"response\"]},\n",
    "    ]\n",
    "\n",
    "    # ä½¿ç”¨ tokenizer çš„ chat template ç”Ÿæˆå®Œæ•´æ–‡æœ¬\n",
    "    text = teacher_tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=False\n",
    "    )\n",
    "\n",
    "    return {\"text\": text, \"messages\": messages}\n",
    "\n",
    "# å¯¼å‡ºåˆ° JSONL\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "train_path = \"data/finetune_train_hf.jsonl\"\n",
    "val_path = \"data/finetune_val_hf.jsonl\"\n",
    "\n",
    "with open(train_path, 'w', encoding='utf-8') as f:\n",
    "    for sample in train_samples:\n",
    "        chatml = to_chatml(sample)\n",
    "        f.write(json.dumps(chatml, ensure_ascii=False) + '\\n')\n",
    "\n",
    "with open(val_path, 'w', encoding='utf-8') as f:\n",
    "    for sample in val_samples:\n",
    "        chatml = to_chatml(sample)\n",
    "        f.write(json.dumps(chatml, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"\\nå¯¼å‡ºå®Œæˆ:\")\n",
    "print(f\"  è®­ç»ƒé›†: {train_path} ({len(train_samples)} æ¡)\")\n",
    "print(f\"  éªŒè¯é›†: {val_path} ({len(val_samples)} æ¡)\")\n",
    "\n",
    "# éªŒè¯å¯¼å‡ºæ–‡ä»¶\n",
    "with open(train_path, 'r') as f:\n",
    "    first_line = json.loads(f.readline())\n",
    "    print(f\"\\næ ·æœ¬éªŒè¯:\")\n",
    "    print(f\"  å­—æ®µ: {list(first_line.keys())}\")\n",
    "    print(f\"  text é•¿åº¦: {len(first_line['text'])} å­—ç¬¦\")\n",
    "    print(f\"  messages æ•°: {len(first_line['messages'])} è½®\")\n",
    "\n",
    "# åŒæ—¶ä¿å­˜åˆ° Google Drive\n",
    "drive_data_dir = '/content/drive/MyDrive/lora_finetune_data'\n",
    "os.makedirs(drive_data_dir, exist_ok=True)\n",
    "import shutil\n",
    "shutil.copy(train_path, os.path.join(drive_data_dir, 'finetune_train_hf.jsonl'))\n",
    "shutil.copy(val_path, os.path.join(drive_data_dir, 'finetune_val_hf.jsonl'))\n",
    "print(f\"\\nå·²å¤‡ä»½åˆ° Google Drive: {drive_data_dir}\")\n",
    "print(\"\\nâœ“ Phase 5.3-5.5 å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fd1f5f",
   "metadata": {},
   "source": [
    "## 5.6 é‡Šæ”¾æ•™å¸ˆæ¨¡å‹æ˜¾å­˜\n",
    "\n",
    "14B æ•™å¸ˆæ¨¡å‹å ç”¨çº¦ 8GB æ˜¾å­˜ï¼Œéœ€è¦é‡Šæ”¾åæ‰èƒ½åŠ è½½ 7B å­¦ç”Ÿæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚\n",
    "\n",
    "ä½¿ç”¨ `del model` + `torch.cuda.empty_cache()` + `gc.collect()` ä¸‰é‡æ¸…ç†ç¡®ä¿æ˜¾å­˜å®Œå…¨é‡Šæ”¾ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a180bad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# 5.6 é‡Šæ”¾æ•™å¸ˆæ¨¡å‹æ˜¾å­˜\n",
    "# âš ï¸ åœ¨ Google Colab ä¸Šè¿è¡Œ\n",
    "# --------------------------------------------\n",
    "\n",
    "print(f\"é‡Šæ”¾å‰ GPU æ˜¾å­˜: {torch.cuda.memory_allocated() / 1024**3:.1f} GB\")\n",
    "\n",
    "# åˆ é™¤æ•™å¸ˆæ¨¡å‹å’Œ tokenizer\n",
    "del teacher_model\n",
    "del teacher_tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"é‡Šæ”¾å GPU æ˜¾å­˜: {torch.cuda.memory_allocated() / 1024**3:.1f} GB\")\n",
    "print(\"\\nâœ“ Phase 5 å®Œæˆ - æ•™å¸ˆæ¨¡å‹æ˜¾å­˜å·²é‡Šæ”¾\")\n",
    "print(\"  è®­ç»ƒæ•°æ®å·²ä¿å­˜ï¼Œå¯ä»¥å¼€å§‹ Phase 6 å¾®è°ƒ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e510be2b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Phase 6: QLoRA å¾®è°ƒï¼ˆQLoRA Fine-tuningï¼‰\n",
    "\n",
    "> **âš ï¸ åœ¨ Google Colabï¼ˆå…è´¹ T4 GPUï¼‰ä¸Šè¿è¡Œ**\n",
    "\n",
    "## ç›®æ ‡\n",
    "\n",
    "ä½¿ç”¨ **QLoRA** å¾®è°ƒ **Qwen2.5-7B-Instruct**ï¼Œä½¿å…¶åœ¨å®¢æˆ·æµå¤±åˆ†æä»»åŠ¡ä¸Šç”Ÿæˆé«˜è´¨é‡çš„ç»“æ„åŒ– JSON å“åº”ã€‚\n",
    "\n",
    "## åŸºåº§æ¨¡å‹ï¼šQwen2.5-7B-Instruct\n",
    "\n",
    "| å±æ€§ | è¯´æ˜ |\n",
    "|------|------|\n",
    "| å‚æ•°é‡ | 7.6B |\n",
    "| é‡åŒ–åå¤§å° | ~4.5 GB (4-bit) |\n",
    "| è®­ç»ƒå³°å€¼æ˜¾å­˜ | ~12 GB |\n",
    "| T4 å®‰å…¨ | âœ… (15 GB é™åˆ¶å†…) |\n",
    "| è®¸å¯è¯ | Apache 2.0 |\n",
    "\n",
    "## ä»€ä¹ˆæ˜¯ QLoRAï¼Ÿ\n",
    "\n",
    "**QLoRA (Quantized Low-Rank Adaptation)** ç»“åˆäº†ä¸¤ä¸ªå…³é”®æŠ€æœ¯ï¼š\n",
    "\n",
    "1. **4-bit é‡åŒ–**ï¼šå°†æ¨¡å‹æƒé‡ä» FP16 (2å­—èŠ‚) å‹ç¼©åˆ° 4-bit (0.5å­—èŠ‚)ï¼Œæ˜¾å­˜å ç”¨é™ä½ 75%\n",
    "2. **LoRA ä½ç§©é€‚é…**ï¼šå†»ç»“åŸå§‹æƒé‡ï¼Œåªè®­ç»ƒå°‘é‡ä½ç§©çŸ©é˜µï¼ˆ~20M å‚æ•°ï¼Œå æ€»å‚æ•° 0.26%ï¼‰\n",
    "\n",
    "### LoRA å‚æ•°è§£é‡Š\n",
    "\n",
    "| å‚æ•° | å€¼ | è¯´æ˜ |\n",
    "|------|-----|------|\n",
    "| `r` | 16 | ä½ç§©çŸ©é˜µçš„ç§©ï¼Œè¶Šå¤§è¡¨è¾¾èƒ½åŠ›è¶Šå¼ºä½†æ˜¾å­˜è¶Šå¤š |\n",
    "| `lora_alpha` | 32 | ç¼©æ”¾å› å­ï¼Œé€šå¸¸è®¾ä¸º 2r |\n",
    "| `lora_dropout` | 0.05 | Dropout é˜²è¿‡æ‹Ÿåˆ |\n",
    "| ç›®æ ‡æ¨¡å— | q/k/v/o/gate/up/down_proj | è¦†ç›–æ‰€æœ‰ attention + FFN å±‚ |\n",
    "\n",
    "### è®­ç»ƒå‚æ•°è§£é‡Š\n",
    "\n",
    "| å‚æ•° | å€¼ | è¯´æ˜ |\n",
    "|------|-----|------|\n",
    "| Epochs | 3 | å°æ•°æ®é›†é€‚å½“å¤šè®­å‡ è½® |\n",
    "| Batch size | 1 | å—é™äºæ˜¾å­˜ |\n",
    "| Gradient accumulation | 8 | ç­‰æ•ˆ batch size = 8 |\n",
    "| Learning rate | 2e-4 | QLoRA æ ‡å‡†å­¦ä¹ ç‡ |\n",
    "| Optimizer | paged_adamw_8bit | 8-bit ä¼˜åŒ–å™¨èŠ‚çœæ˜¾å­˜ |\n",
    "| Scheduler | cosine | ä½™å¼¦é€€ç«ï¼Œè®­ç»ƒæ›´ç¨³å®š |\n",
    "| Max sequence length | 2048 | è¦†ç›– RAG ä¸Šä¸‹æ–‡ + å“åº” |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aebf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Phase 6: QLoRA å¾®è°ƒ\n",
    "# âš ï¸ åœ¨ Google Colab (T4 GPU) ä¸Šè¿è¡Œ\n",
    "# ============================================\n",
    "\n",
    "# --------------------------------------------\n",
    "# 6.1 åŠ è½½åŸºåº§æ¨¡å‹ + é…ç½® QLoRA\n",
    "# --------------------------------------------\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import torch\n",
    "\n",
    "# 4-bit é‡åŒ–é…ç½®\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # A100 åŸç”Ÿæ”¯æŒ bfloat16\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "BASE_MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "print(f\"åŠ è½½åŸºåº§æ¨¡å‹: {BASE_MODEL_NAME} (4-bit)...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.model_max_length = 2048       # é™åˆ¶æœ€å¤§åºåˆ—é•¿åº¦\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# å‡†å¤‡æ¨¡å‹è¿›è¡Œ k-bit è®­ç»ƒ\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# é…ç½® LoRA é€‚é…å™¨\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                           # ä½ç§©çŸ©é˜µçš„ç§©\n",
    "    lora_alpha=32,                  # ç¼©æ”¾å› å­ (é€šå¸¸ = 2r)\n",
    "    lora_dropout=0.05,              # Dropout é˜²è¿‡æ‹Ÿåˆ\n",
    "    bias=\"none\",                    # ä¸è®­ç»ƒ bias\n",
    "    task_type=\"CAUSAL_LM\",         # å› æœè¯­è¨€æ¨¡å‹ä»»åŠ¡\n",
    "    target_modules=[                # ç›®æ ‡æ¨¡å—ï¼šè¦†ç›–æ‰€æœ‰çº¿æ€§å±‚\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# åº”ç”¨ LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# æ‰“å°å¯è®­ç»ƒå‚æ•°\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(f\"\\nGPU æ˜¾å­˜ä½¿ç”¨: {torch.cuda.memory_allocated() / 1024**3:.1f} GB\")\n",
    "print(\"\\nâœ“ æ¨¡å‹åŠ è½½å’Œ LoRA é…ç½®å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdae808",
   "metadata": {},
   "source": [
    "### åŠ è½½è®­ç»ƒæ•°æ®å¹¶å¼€å§‹è®­ç»ƒ\n",
    "\n",
    "ä½¿ç”¨ `SFTTrainer`ï¼ˆSupervised Fine-Tuning Trainerï¼‰è¿›è¡Œè®­ç»ƒã€‚å®ƒå°è£…äº† Hugging Face Trainerï¼Œå¹¶é’ˆå¯¹æŒ‡ä»¤å¾®è°ƒåœºæ™¯åšäº†ä¼˜åŒ–ã€‚\n",
    "\n",
    "**è®­ç»ƒé¢„ä¼°**ï¼š\n",
    "- ~180-240 æ¡æ ·æœ¬ï¼Œ3 epochs\n",
    "- Batch size 1 Ã— Gradient accumulation 8 = æœ‰æ•ˆ batch 8\n",
    "- é¢„è®¡è®­ç»ƒæ—¶é—´ï¼š**~30-50 åˆ†é’Ÿ**ï¼ˆT4 GPUï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a45186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# 6.2 åŠ è½½è®­ç»ƒæ•°æ® + SFTTrainer è®­ç»ƒ\n",
    "# âš ï¸ åœ¨ Google Colab ä¸Šè¿è¡Œ\n",
    "# --------------------------------------------\n",
    "\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# åŠ è½½è®­ç»ƒæ•°æ®\n",
    "train_dataset = load_dataset('json', data_files='data/finetune_train_hf.jsonl', split='train')\n",
    "val_dataset = load_dataset('json', data_files='data/finetune_val_hf.jsonl', split='train')\n",
    "\n",
    "print(f\"è®­ç»ƒé›†: {len(train_dataset)} æ¡\")\n",
    "print(f\"éªŒè¯é›†: {len(val_dataset)} æ¡\")\n",
    "\n",
    "# è®­ç»ƒé…ç½®\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=\"./qwen2.5-7b-churn-lora\",      # è¾“å‡ºç›®å½•\n",
    "    num_train_epochs=3,                          # è®­ç»ƒè½®æ•°\n",
    "    per_device_train_batch_size=1,               # æ¯ GPU batch size\n",
    "    per_device_eval_batch_size=1,                # è¯„ä¼° batch size\n",
    "    gradient_accumulation_steps=8,               # æ¢¯åº¦ç´¯ç§¯ï¼ˆæœ‰æ•ˆ batch = 8ï¼‰\n",
    "    optim=\"paged_adamw_8bit\",                    # 8-bit ä¼˜åŒ–å™¨èŠ‚çœæ˜¾å­˜\n",
    "    learning_rate=2e-4,                          # QLoRA æ ‡å‡†å­¦ä¹ ç‡\n",
    "    lr_scheduler_type=\"cosine\",                  # ä½™å¼¦é€€ç«\n",
    "    warmup_steps=6,                              # é¢„çƒ­æ­¥æ•°\n",
    "    weight_decay=0.01,                           # æƒé‡è¡°å‡\n",
    "    fp16=False,                                   # A100 ä½¿ç”¨ BF16ï¼ˆæ›´ç¨³å®šï¼Œæ— éœ€ GradScalerï¼‰\n",
    "    bf16=True,\n",
    "    logging_steps=5,                             # æ¯ 5 æ­¥è®°å½•æ—¥å¿—\n",
    "    eval_strategy=\"steps\",                       # æŒ‰æ­¥æ•°è¯„ä¼°\n",
    "    eval_steps=20,                               # æ¯ 20 æ­¥è¯„ä¼°\n",
    "    save_strategy=\"steps\",                       # æŒ‰æ­¥æ•°ä¿å­˜\n",
    "    save_steps=50,                               # æ¯ 50 æ­¥ä¿å­˜\n",
    "    save_total_limit=3,                          # æœ€å¤šä¿ç•™ 3 ä¸ª checkpoint\n",
    "    gradient_checkpointing=True,                 # æ¢¯åº¦æ£€æŸ¥ç‚¹èŠ‚çœæ˜¾å­˜\n",
    "    dataset_text_field=\"text\",                   # æ•°æ®é›†ä¸­æ–‡æœ¬å­—æ®µå\n",
    "    report_to=\"none\",                            # ä¸ä¸Šä¼ åˆ° wandb\n",
    ")\n",
    "\n",
    "# åˆ›å»ºè®­ç»ƒå™¨\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "print(f\"\\nè®­ç»ƒé…ç½®:\")\n",
    "print(f\"  æœ‰æ•ˆ batch size: {sft_config.per_device_train_batch_size * sft_config.gradient_accumulation_steps}\")\n",
    "print(f\"  æ€»è®­ç»ƒæ­¥æ•°: {len(train_dataset) // (sft_config.per_device_train_batch_size * sft_config.gradient_accumulation_steps) * sft_config.num_train_epochs}\")\n",
    "\n",
    "# å¼€å§‹è®­ç»ƒ\n",
    "print(\"\\nå¼€å§‹è®­ç»ƒ...\")\n",
    "print(\"=\" * 50)\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\nè®­ç»ƒå®Œæˆ âœ“\")\n",
    "print(f\"  è®­ç»ƒæŸå¤±: {train_result.training_loss:.4f}\")\n",
    "print(f\"  è®­ç»ƒæ—¶é—´: {train_result.metrics['train_runtime']:.0f} ç§’\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66767f32",
   "metadata": {},
   "source": [
    "### ä¿å­˜ LoRA Adapter\n",
    "\n",
    "å¾®è°ƒåçš„ LoRA adapter ä»…çº¦ **40 MB**ï¼ˆç›¸æ¯”å®Œæ•´æ¨¡å‹ 14 GBï¼‰ï¼Œä¾¿äºå­˜å‚¨å’Œåˆ†äº«ã€‚\n",
    "\n",
    "ä¿å­˜ä½ç½®ï¼š\n",
    "1. Colab æœ¬åœ°ï¼š`./qwen2.5-7b-churn-lora/final`\n",
    "2. Google Driveï¼š`/content/drive/MyDrive/qwen2.5-7b-churn-lora`ï¼ˆæŒä¹…åŒ–ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f17398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# 6.3 ä¿å­˜ LoRA Adapter\n",
    "# âš ï¸ åœ¨ Google Colab ä¸Šè¿è¡Œ\n",
    "# --------------------------------------------\n",
    "\n",
    "# ä¿å­˜åˆ°æœ¬åœ°\n",
    "adapter_local_path = \"./qwen2.5-7b-churn-lora/final\"\n",
    "trainer.save_model(adapter_local_path)\n",
    "tokenizer.save_pretrained(adapter_local_path)\n",
    "print(f\"Adapter å·²ä¿å­˜åˆ°: {adapter_local_path}\")\n",
    "\n",
    "# ä¿å­˜åˆ° Google Driveï¼ˆæŒä¹…åŒ–ï¼‰\n",
    "adapter_drive_path = \"/content/drive/MyDrive/qwen2.5-7b-churn-lora\"\n",
    "os.makedirs(adapter_drive_path, exist_ok=True)\n",
    "trainer.save_model(adapter_drive_path)\n",
    "tokenizer.save_pretrained(adapter_drive_path)\n",
    "print(f\"Adapter å·²å¤‡ä»½åˆ° Google Drive: {adapter_drive_path}\")\n",
    "\n",
    "# æ˜¾ç¤º adapter å¤§å°\n",
    "import subprocess\n",
    "result = subprocess.run(['du', '-sh', adapter_local_path], capture_output=True, text=True)\n",
    "print(f\"\\nAdapter å¤§å°: {result.stdout.strip()}\")\n",
    "\n",
    "# é‡Šæ”¾è®­ç»ƒç›¸å…³æ˜¾å­˜\n",
    "del trainer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# å¯é€‰: æ¨é€åˆ° Hugging Face Hub\n",
    "# from huggingface_hub import login\n",
    "# login(token=\"your_hf_token\")\n",
    "# model.push_to_hub(\"your-username/qwen2.5-7b-churn-lora\")\n",
    "# tokenizer.push_to_hub(\"your-username/qwen2.5-7b-churn-lora\")\n",
    "\n",
    "print(f\"\\nGPU æ˜¾å­˜: {torch.cuda.memory_allocated() / 1024**3:.1f} GB\")\n",
    "print(\"\\nâœ“ Phase 6 å®Œæˆ - LoRA Adapter å·²ä¿å­˜\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8016082",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Phase 7: å¼€æºæ¨¡å‹é›†æˆåˆ° RAG Pipeline\n",
    "\n",
    "> **âš ï¸ åœ¨ Google Colabï¼ˆå…è´¹ T4 GPUï¼‰ä¸Šè¿è¡Œ**\n",
    "\n",
    "## ç›®æ ‡\n",
    "\n",
    "ç”¨ **QLoRA å¾®è°ƒåçš„ Qwen2.5-7B-Instruct** æ›¿ä»£ Phase 3 çš„ base æ¨¡å‹ï¼Œæå‡ RAG è¾“å‡ºè´¨é‡ã€‚\n",
    "\n",
    "## æ¶æ„å˜æ›´\n",
    "\n",
    "```\n",
    "åŸæ¶æ„ (Phase 3-4):\n",
    "  hybrid_search() â†’ build_context() â†’ Qwen2.5-7B base (é›¶æ ·æœ¬) â†’ JSON è§£æ\n",
    "\n",
    "æ–°æ¶æ„ (Phase 7):\n",
    "  hybrid_search() â†’ build_context() â†’ Qwen2.5-7B + LoRA (æœ¬åœ°) â†’ JSON è§£æ\n",
    "```\n",
    "\n",
    "## VRAM é¢„ä¼°\n",
    "\n",
    "| ç»„ä»¶ | æ˜¾å­˜ |\n",
    "|------|------|\n",
    "| BGE åµŒå…¥æ¨¡å‹ | ~0.5 GB |\n",
    "| Qwen2.5-7B + LoRA (4-bit) | ~4.5 GB |\n",
    "| **åˆè®¡** | **~5 GB** |\n",
    "\n",
    "æ¨ç†æ¨¡å¼ä¸‹æ˜¾å­˜å……è£•ï¼ŒT4 å®Œå…¨å¤Ÿç”¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f797af1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Phase 7: å¼€æºæ¨¡å‹é›†æˆåˆ° RAG Pipeline\n",
    "# âš ï¸ åœ¨ Google Colab (T4 GPU) ä¸Šè¿è¡Œ\n",
    "# ============================================\n",
    "\n",
    "# --------------------------------------------\n",
    "# 7.1 åŠ è½½å¾®è°ƒåçš„æ¨¡å‹\n",
    "# --------------------------------------------\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import json\n",
    "import re\n",
    "import gc\n",
    "\n",
    "# å¦‚æœä¹‹å‰çš„æ¨¡å‹è¿˜åœ¨æ˜¾å­˜ä¸­ï¼Œå…ˆæ¸…ç†\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 4-bit é‡åŒ–é…ç½®ï¼ˆæ¨ç†ç”¨ï¼‰\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "BASE_MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "# ä» Google Drive åŠ è½½ adapterï¼ˆæŒä¹…åŒ–è·¯å¾„ï¼‰\n",
    "ADAPTER_PATH = \"/content/drive/MyDrive/qwen2.5-7b-churn-lora\"\n",
    "# å¤‡é€‰ï¼šä»æœ¬åœ°åŠ è½½\n",
    "# ADAPTER_PATH = \"./qwen2.5-7b-churn-lora/final\"\n",
    "\n",
    "print(f\"åŠ è½½åŸºåº§æ¨¡å‹: {BASE_MODEL_NAME} (4-bit)...\")\n",
    "ft_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "ft_base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# åŠ è½½ LoRA adapter\n",
    "print(f\"åŠ è½½ LoRA adapter: {ADAPTER_PATH}...\")\n",
    "ft_model = PeftModel.from_pretrained(ft_base_model, ADAPTER_PATH)\n",
    "ft_model.eval()\n",
    "\n",
    "print(f\"\\nå¾®è°ƒæ¨¡å‹åŠ è½½å®Œæˆ âœ“\")\n",
    "print(f\"GPU æ˜¾å­˜: {torch.cuda.memory_allocated() / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96fc38f",
   "metadata": {},
   "source": [
    "## 7.2 å®šä¹‰æœ¬åœ° RAG æŸ¥è¯¢å‡½æ•°\n",
    "\n",
    "`rag_query_local()` ä¸åŸ `rag_query()` å…·æœ‰ç›¸åŒçš„æ¥å£ï¼Œä½†åŠ è½½äº† LoRA å¾®è°ƒæƒé‡ï¼š\n",
    "\n",
    "| æ­¥éª¤ | åŸæ–¹æ³• (Phase 3) | æ–°æ–¹æ³• (Phase 7) |\n",
    "|------|------------------|------------------|\n",
    "| æ£€ç´¢ | `hybrid_search()` | `hybrid_search()` (ä¸å˜) |\n",
    "| ä¸Šä¸‹æ–‡ | `build_context()` | `build_context()` (ä¸å˜) |\n",
    "| ç”Ÿæˆ | `model.generate()` (base) | `model.generate()` (base + LoRA) |\n",
    "| è§£æ | `json.loads()` | `json.loads()` + regex fallback |\n",
    "\n",
    "### JSON è§£æå¢å¼º\n",
    "\n",
    "å¼€æºæ¨¡å‹å¯èƒ½åœ¨ JSON å‰åæ·»åŠ é¢å¤–æ–‡æœ¬ï¼ˆå¦‚ \"Here is the analysis:\"ï¼‰ï¼Œå› æ­¤å¢åŠ äº† **regex fallback** æå– JSON å—ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c973da73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# 7.2 å®šä¹‰æœ¬åœ° RAG æŸ¥è¯¢å‡½æ•°\n",
    "# âš ï¸ åœ¨ Google Colab ä¸Šè¿è¡Œ\n",
    "# --------------------------------------------\n",
    "\n",
    "def rag_query_local(user_query, k=5, max_new_tokens=1024, use_finetuned=True):\n",
    "    \"\"\"\n",
    "    å¾®è°ƒæ¨¡å‹ RAG æŸ¥è¯¢ Pipelineï¼ˆbase + LoRA adapterï¼‰\n",
    "\n",
    "    Args:\n",
    "        user_query: ç”¨æˆ·æŸ¥è¯¢\n",
    "        k: æ£€ç´¢æ–‡æ¡£æ•°\n",
    "        max_new_tokens: æœ€å¤§ç”Ÿæˆ token æ•°\n",
    "        use_finetuned: True ä½¿ç”¨å¾®è°ƒæ¨¡å‹ï¼ŒFalse ä½¿ç”¨åŸºåº§æ¨¡å‹ï¼ˆç”¨äºå¯¹æ¯”ï¼‰\n",
    "\n",
    "    Returns:\n",
    "        dict: è§£æåçš„ JSON ç»“æœ\n",
    "    \"\"\"\n",
    "    # Step 1: æ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼ˆä¸åŸ rag_query å®Œå…¨ç›¸åŒï¼‰\n",
    "    search_results = hybrid_search(user_query, k=k)\n",
    "\n",
    "    # Step 2: æ„å»ºä¸Šä¸‹æ–‡ï¼ˆä¸åŸ rag_query å®Œå…¨ç›¸åŒï¼‰\n",
    "    context = build_context(search_results, max_docs=k)\n",
    "\n",
    "    # Step 3: æ„é€  prompt å¹¶ç”¨æœ¬åœ°æ¨¡å‹ç”Ÿæˆ\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": USER_TEMPLATE.format(query=user_query, context=context)},\n",
    "    ]\n",
    "\n",
    "    active_model = ft_model if use_finetuned else ft_base_model\n",
    "\n",
    "    text = ft_tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    inputs = ft_tokenizer(text, return_tensors=\"pt\").to(active_model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = active_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=ft_tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    generated_ids = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "    response_text = ft_tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    # Step 4: JSON è§£æ + regex fallback\n",
    "    result = None\n",
    "    try:\n",
    "        result = json.loads(response_text)\n",
    "    except json.JSONDecodeError:\n",
    "        # Fallback: æå– JSON å—\n",
    "        json_match = re.search(r'\\{[\\s\\S]*\\}', response_text)\n",
    "        if json_match:\n",
    "            try:\n",
    "                result = json.loads(json_match.group())\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "\n",
    "    if result is None:\n",
    "        result = {\"error\": \"Failed to parse JSON\", \"raw_response\": response_text}\n",
    "\n",
    "    # æ·»åŠ æ£€ç´¢å…ƒä¿¡æ¯\n",
    "    result[\"retrieved_docs\"] = len(search_results)\n",
    "    result[\"retrieved_customer_ids\"] = [customer_ids[idx] for idx, _ in search_results]\n",
    "\n",
    "    return result\n",
    "\n",
    "print(\"rag_query_local() å®šä¹‰å®Œæˆ âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c3507d",
   "metadata": {},
   "source": [
    "## 7.3 å®šä¹‰å®Œæ•´åˆ†æå‡½æ•°\n",
    "\n",
    "`analyze_churn_local()` å°è£…å®Œæ•´çš„ RAG æµç¨‹ï¼šæ£€ç´¢ â†’ ç”Ÿæˆ â†’ æ ¼å¼åŒ–è¾“å‡º â†’ å¼•ç”¨éªŒè¯ã€‚\n",
    "\n",
    "å¤ç”¨å·²æœ‰çš„ `display_analysis()` å’Œ `validate_citations()` å‡½æ•°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846d39a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# 7.3 å®šä¹‰å®Œæ•´åˆ†æå‡½æ•°\n",
    "# âš ï¸ åœ¨ Google Colab ä¸Šè¿è¡Œ\n",
    "# --------------------------------------------\n",
    "\n",
    "# å¤ç”¨ Phase 4 çš„è¾…åŠ©å‡½æ•°\n",
    "def display_analysis(result):\n",
    "    \"\"\"ç¾åŒ–æ˜¾ç¤ºåˆ†æç»“æœ\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"CUSTOMER CHURN ANALYSIS REPORT\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if \"error\" in result:\n",
    "        print(f\"Error: {result['error']}\")\n",
    "        if \"raw_response\" in result:\n",
    "            print(f\"Raw response: {result['raw_response'][:500]}\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nSummary:\")\n",
    "    print(f\"   {result.get('summary', 'N/A')}\")\n",
    "\n",
    "    print(f\"\\nTop Reasons for Churn:\")\n",
    "    for i, reason in enumerate(result.get('top_reasons', []), 1):\n",
    "        print(f\"   {i}. {reason}\")\n",
    "\n",
    "    print(f\"\\nRisk Level: {result.get('risk_level', 'N/A').upper()}\")\n",
    "\n",
    "    print(f\"\\nRecommended Actions:\")\n",
    "    for i, action in enumerate(result.get('actions', []), 1):\n",
    "        print(f\"   {i}. {action}\")\n",
    "\n",
    "    print(f\"\\nCitations (Customer IDs):\")\n",
    "    print(f\"   {', '.join(result.get('citations', []))}\")\n",
    "\n",
    "    print(f\"\\nRetrieved Documents: {result.get('retrieved_docs', 0)}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "def normalize_customer_id(cid):\n",
    "    \"\"\"æ ‡å‡†åŒ–å®¢æˆ· ID\"\"\"\n",
    "    cid = cid.strip().lower()\n",
    "    prefixes = ['customerid_', 'customer_id_', 'cid_', 'id_']\n",
    "    for prefix in prefixes:\n",
    "        if cid.startswith(prefix):\n",
    "            cid = cid[len(prefix):]\n",
    "    return cid\n",
    "\n",
    "def validate_citations(result):\n",
    "    \"\"\"éªŒè¯å¼•ç”¨æ˜¯å¦æ¥è‡ªæ£€ç´¢ç»“æœ\"\"\"\n",
    "    citations = result.get('citations', [])\n",
    "    retrieved_ids = result.get('retrieved_customer_ids', [])\n",
    "\n",
    "    retrieved_normalized = {normalize_customer_id(rid): rid for rid in retrieved_ids}\n",
    "\n",
    "    valid_citations = []\n",
    "    invalid_citations = []\n",
    "\n",
    "    for cid in citations:\n",
    "        normalized = normalize_customer_id(cid)\n",
    "        if normalized in retrieved_normalized:\n",
    "            valid_citations.append(cid)\n",
    "        else:\n",
    "            invalid_citations.append(cid)\n",
    "\n",
    "    accuracy = len(valid_citations) / len(citations) if citations else 0\n",
    "\n",
    "    return {\n",
    "        \"total_citations\": len(citations),\n",
    "        \"valid_citations\": valid_citations,\n",
    "        \"invalid_citations\": invalid_citations,\n",
    "        \"accuracy\": accuracy,\n",
    "    }\n",
    "\n",
    "def analyze_churn_local(query, use_finetuned=True):\n",
    "    \"\"\"\n",
    "    å®Œæ•´çš„æœ¬åœ°æµå¤±åˆ†æå‡½æ•°\n",
    "\n",
    "    Args:\n",
    "        query: åˆ†ææŸ¥è¯¢\n",
    "        use_finetuned: True ä½¿ç”¨å¾®è°ƒæ¨¡å‹ï¼ŒFalse ä½¿ç”¨åŸºåº§æ¨¡å‹\n",
    "\n",
    "    Returns:\n",
    "        åˆ†æç»“æœ dict\n",
    "    \"\"\"\n",
    "    model_type = \"Fine-tuned\" if use_finetuned else \"Base\"\n",
    "    print(f\"Analyzing ({model_type}): {query}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    result = rag_query_local(query, k=5, use_finetuned=use_finetuned)\n",
    "    display_analysis(result)\n",
    "\n",
    "    validation = validate_citations(result)\n",
    "    print(f\"\\nCitation Accuracy: {validation['accuracy']*100:.1f}%\")\n",
    "\n",
    "    return result\n",
    "\n",
    "print(\"analyze_churn_local() å®šä¹‰å®Œæˆ âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed43afd9",
   "metadata": {},
   "source": [
    "## 7.4 ç«¯åˆ°ç«¯æµ‹è¯•\n",
    "\n",
    "ç”¨ 3 æ¡ä¸åŒç±»å‹çš„æŸ¥è¯¢éªŒè¯å®Œæ•´ RAG Pipeline åœ¨æœ¬åœ°æ¨¡å‹ä¸‹çš„å·¥ä½œæƒ…å†µã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a837672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# 7.4 ç«¯åˆ°ç«¯æµ‹è¯•\n",
    "# âš ï¸ åœ¨ Google Colab ä¸Šè¿è¡Œ\n",
    "# --------------------------------------------\n",
    "\n",
    "test_queries = [\n",
    "    \"Why are customers with fiber optic internet churning?\",\n",
    "    \"What issues do senior citizens face with our services?\",\n",
    "    \"What should we do to reduce churn among month-to-month customers?\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Phase 7: ç«¯åˆ°ç«¯æµ‹è¯• - å¾®è°ƒæ¨¡å‹ RAG Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_results = []\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"æµ‹è¯• {i}/{len(test_queries)}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    result = analyze_churn_local(query, use_finetuned=True)\n",
    "    test_results.append(result)\n",
    "\n",
    "# æ±‡æ€»æµ‹è¯•ç»“æœ\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ç«¯åˆ°ç«¯æµ‹è¯•æ±‡æ€»\")\n",
    "print(\"=\" * 60)\n",
    "for i, (query, result) in enumerate(zip(test_queries, test_results), 1):\n",
    "    has_error = \"error\" in result\n",
    "    json_valid = not has_error\n",
    "    fields_ok = all(f in result for f in ['summary', 'top_reasons', 'risk_level', 'actions', 'citations']) if not has_error else False\n",
    "    print(f\"  æµ‹è¯• {i}: JSON={'OK' if json_valid else 'FAIL'}  å­—æ®µ={'å®Œæ•´' if fields_ok else 'ç¼ºå¤±'}  æŸ¥è¯¢={query[:50]}...\")\n",
    "\n",
    "print(\"\\nâœ“ Phase 7 å®Œæˆ - å¼€æºæ¨¡å‹ RAG Pipeline éªŒè¯é€šè¿‡\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9b66db",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Phase 8: è¯„ä¼°ä¸å¯¹æ¯”ï¼ˆEvaluation & Comparisonï¼‰\n",
    "\n",
    "> **âš ï¸ åœ¨ Google Colabï¼ˆå…è´¹ T4 GPUï¼‰ä¸Šè¿è¡Œ**\n",
    "\n",
    "## ç›®æ ‡\n",
    "\n",
    "å¯¹æ¯” **Base æ¨¡å‹ï¼ˆé›¶æ ·æœ¬ï¼‰** vs **QLoRA å¾®è°ƒåæ¨¡å‹** åœ¨ RAG ä»»åŠ¡ä¸Šçš„è¡¨ç°å·®å¼‚ã€‚\n",
    "\n",
    "## åŒæ¨¡å‹å¯¹æ¯”æ–¹æ¡ˆ\n",
    "\n",
    "| æ¨¡å‹ | è¯´æ˜ |\n",
    "|------|------|\n",
    "| Qwen2.5-7B-Instruct (base) | åŸå§‹æ¨¡å‹ï¼Œé›¶æ ·æœ¬ |\n",
    "| Qwen2.5-7B-Instruct + LoRA | å¾®è°ƒåæ¨¡å‹ |\n",
    "\n",
    "## è¯„ä¼°æŒ‡æ ‡\n",
    "\n",
    "| æŒ‡æ ‡ | è¯´æ˜ |\n",
    "|------|------|\n",
    "| JSON æ ¼å¼åˆè§„ç‡ | è¿”å›æ˜¯å¦ä¸ºåˆæ³• JSON |\n",
    "| å­—æ®µå®Œæ•´åº¦ | 5 ä¸ªå¿…éœ€å­—æ®µæ˜¯å¦é½å…¨ |\n",
    "| ç±»å‹æ­£ç¡®æ€§ | å­—æ®µå€¼å’Œç±»å‹æ˜¯å¦ç¬¦åˆ schema |\n",
    "| å¼•ç”¨å‡†ç¡®ç‡ | å¼•ç”¨çš„å®¢æˆ· ID æ˜¯å¦æ¥è‡ªæ£€ç´¢ç»“æœ |\n",
    "| é£é™©ç­‰çº§ä¸€è‡´æ€§ | risk_level ä¸å®é™…æµå¤±æ¯”ä¾‹æ˜¯å¦å¯¹é½ |\n",
    "\n",
    "## æµ‹è¯•é›†\n",
    "\n",
    "10 æ¡æœªå‚ä¸è®­ç»ƒçš„å¤šæ ·åŒ–æŸ¥è¯¢ï¼Œè¦†ç›–ä¸åŒåˆ†æç»´åº¦ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cd2dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Phase 8: è¯„ä¼°ä¸å¯¹æ¯”\n",
    "# âš ï¸ åœ¨ Google Colab (T4 GPU) ä¸Šè¿è¡Œ\n",
    "# ============================================\n",
    "\n",
    "# --------------------------------------------\n",
    "# 8.1 å®šä¹‰æµ‹è¯•é›†å’Œè¯„ä¼°å‡½æ•°\n",
    "# --------------------------------------------\n",
    "\n",
    "# 10 æ¡æœªå‚ä¸è®­ç»ƒçš„æµ‹è¯•æŸ¥è¯¢\n",
    "eval_queries = [\n",
    "    {\"query\": \"What are the top reasons for churn among customers with multiple services?\", \"category\": \"service\"},\n",
    "    {\"query\": \"How do payment methods influence customer retention?\", \"category\": \"billing\"},\n",
    "    {\"query\": \"What patterns exist in feedback from customers who stayed?\", \"category\": \"sentiment\"},\n",
    "    {\"query\": \"Why do customers without online security churn more?\", \"category\": \"service\"},\n",
    "    {\"query\": \"What is the relationship between tenure and customer satisfaction?\", \"category\": \"tenure\"},\n",
    "    {\"query\": \"How effective is tech support in preventing churn?\", \"category\": \"service\"},\n",
    "    {\"query\": \"What demographic factors contribute most to churn risk?\", \"category\": \"demographics\"},\n",
    "    {\"query\": \"Compare customer satisfaction between paperless and non-paperless billing\", \"category\": \"comparison\"},\n",
    "    {\"query\": \"What proactive measures can reduce churn for high-value customers?\", \"category\": \"action\"},\n",
    "    {\"query\": \"What role does contract length play in customer loyalty?\", \"category\": \"contract\"},\n",
    "]\n",
    "\n",
    "def evaluate_response(result, query_info):\n",
    "    \"\"\"\n",
    "    è¯„ä¼°å•æ¡å“åº”çš„è´¨é‡\n",
    "\n",
    "    è¿”å›å„ç»´åº¦å¾—åˆ† (0 æˆ– 1)\n",
    "    \"\"\"\n",
    "    scores = {}\n",
    "\n",
    "    # 1. JSON æ ¼å¼åˆè§„ç‡\n",
    "    scores[\"json_valid\"] = 0 if \"error\" in result else 1\n",
    "\n",
    "    if scores[\"json_valid\"] == 0:\n",
    "        # JSON æ— æ•ˆï¼Œå…¶ä»–æŒ‡æ ‡éƒ½ä¸º 0\n",
    "        scores[\"fields_complete\"] = 0\n",
    "        scores[\"types_correct\"] = 0\n",
    "        scores[\"citation_accuracy\"] = 0.0\n",
    "        scores[\"risk_aligned\"] = 0\n",
    "        scores[\"response_length\"] = 0\n",
    "        return scores\n",
    "\n",
    "    # 2. å­—æ®µå®Œæ•´åº¦\n",
    "    required_fields = ['summary', 'top_reasons', 'risk_level', 'actions', 'citations']\n",
    "    scores[\"fields_complete\"] = 1 if all(f in result for f in required_fields) else 0\n",
    "\n",
    "    # 3. ç±»å‹æ­£ç¡®æ€§\n",
    "    type_checks = [\n",
    "        isinstance(result.get('summary'), str) and len(result.get('summary', '')) > 10,\n",
    "        isinstance(result.get('top_reasons'), list) and len(result.get('top_reasons', [])) > 0,\n",
    "        result.get('risk_level') in ['high', 'medium', 'low'],\n",
    "        isinstance(result.get('actions'), list) and len(result.get('actions', [])) > 0,\n",
    "        isinstance(result.get('citations'), list),\n",
    "    ]\n",
    "    scores[\"types_correct\"] = 1 if all(type_checks) else 0\n",
    "\n",
    "    # 4. å¼•ç”¨å‡†ç¡®ç‡\n",
    "    validation = validate_citations(result)\n",
    "    scores[\"citation_accuracy\"] = validation[\"accuracy\"]\n",
    "\n",
    "    # 5. é£é™©ç­‰çº§ä¸€è‡´æ€§ï¼ˆåŸºäºæ£€ç´¢åˆ°çš„å®¢æˆ·çš„å®é™…æµå¤±æ¯”ä¾‹ï¼‰\n",
    "    retrieved_ids = result.get(\"retrieved_customer_ids\", [])\n",
    "    if retrieved_ids:\n",
    "        churned = sum(1 for cid in retrieved_ids\n",
    "                      if cid in customer_ids and\n",
    "                      df_with_feedback[df_with_feedback['customerID'] == cid]['Churn'].values[0] == 'Yes'\n",
    "                      if len(df_with_feedback[df_with_feedback['customerID'] == cid]) > 0)\n",
    "        churn_rate = churned / len(retrieved_ids)\n",
    "        actual_risk = \"high\" if churn_rate > 0.5 else (\"medium\" if churn_rate > 0.25 else \"low\")\n",
    "        scores[\"risk_aligned\"] = 1 if result.get(\"risk_level\") == actual_risk else 0\n",
    "    else:\n",
    "        scores[\"risk_aligned\"] = 0\n",
    "\n",
    "    # 6. å“åº”è¯¦ç»†åº¦ï¼ˆsummary + reasons + actions çš„æ€»å­—ç¬¦æ•°ï¼‰\n",
    "    detail_len = len(result.get('summary', ''))\n",
    "    detail_len += sum(len(r) for r in result.get('top_reasons', []))\n",
    "    detail_len += sum(len(a) for a in result.get('actions', []))\n",
    "    scores[\"response_length\"] = detail_len\n",
    "\n",
    "    return scores\n",
    "\n",
    "print(f\"æµ‹è¯•é›†: {len(eval_queries)} æ¡æŸ¥è¯¢\")\n",
    "print(\"è¯„ä¼°å‡½æ•°å®šä¹‰å®Œæˆ âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b13207",
   "metadata": {},
   "source": [
    "### è¿è¡ŒåŒæ¨¡å‹è¯„ä¼°\n",
    "\n",
    "å¯¹åŒä¸€ç»„æµ‹è¯•æŸ¥è¯¢ï¼Œåˆ†åˆ«ç”¨ **Base æ¨¡å‹** å’Œ **Fine-tuned æ¨¡å‹** ç”Ÿæˆå“åº”å¹¶è¯„ä¼°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498cc4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# 8.2 è¿è¡ŒåŒæ¨¡å‹è¯„ä¼°\n",
    "# âš ï¸ åœ¨ Google Colab ä¸Šè¿è¡Œ\n",
    "# --------------------------------------------\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# å­˜å‚¨è¯„ä¼°ç»“æœ\n",
    "eval_results = {\"base\": [], \"finetuned\": []}\n",
    "eval_scores = {\"base\": [], \"finetuned\": []}\n",
    "\n",
    "for model_type, use_ft in [(\"finetuned\", True), (\"base\", False)]:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"è¯„ä¼°æ¨¡å‹: {'Fine-tuned (LoRA)' if use_ft else 'Base (é›¶æ ·æœ¬)'}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    for q_info in tqdm(eval_queries, desc=model_type):\n",
    "        result = rag_query_local(q_info[\"query\"], k=5, use_finetuned=use_ft)\n",
    "        scores = evaluate_response(result, q_info)\n",
    "\n",
    "        eval_results[model_type].append(result)\n",
    "        eval_scores[model_type].append(scores)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# æ±‡æ€»è¯„ä¼°ç»“æœ\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"è¯„ä¼°ç»“æœæ±‡æ€»\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "metrics = [\"json_valid\", \"fields_complete\", \"types_correct\", \"citation_accuracy\", \"risk_aligned\"]\n",
    "metric_names = {\n",
    "    \"json_valid\": \"JSON æ ¼å¼åˆè§„ç‡\",\n",
    "    \"fields_complete\": \"å­—æ®µå®Œæ•´åº¦\",\n",
    "    \"types_correct\": \"ç±»å‹æ­£ç¡®æ€§\",\n",
    "    \"citation_accuracy\": \"å¼•ç”¨å‡†ç¡®ç‡\",\n",
    "    \"risk_aligned\": \"é£é™©ç­‰çº§ä¸€è‡´æ€§\",\n",
    "}\n",
    "\n",
    "print(f\"\\n{'æŒ‡æ ‡':<20} {'Base æ¨¡å‹':>12} {'å¾®è°ƒæ¨¡å‹':>12} {'æå‡':>10}\")\n",
    "print(\"-\" * 56)\n",
    "\n",
    "for metric in metrics:\n",
    "    base_avg = np.mean([s[metric] for s in eval_scores[\"base\"]])\n",
    "    ft_avg = np.mean([s[metric] for s in eval_scores[\"finetuned\"]])\n",
    "    diff = ft_avg - base_avg\n",
    "    print(f\"{metric_names[metric]:<20} {base_avg:>11.1%} {ft_avg:>11.1%} {diff:>+9.1%}\")\n",
    "\n",
    "# å“åº”è¯¦ç»†åº¦\n",
    "base_len = np.mean([s[\"response_length\"] for s in eval_scores[\"base\"]])\n",
    "ft_len = np.mean([s[\"response_length\"] for s in eval_scores[\"finetuned\"]])\n",
    "print(f\"{'å¹³å‡å“åº”é•¿åº¦':<20} {base_len:>11.0f} {ft_len:>11.0f} {ft_len-base_len:>+9.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4723e610",
   "metadata": {},
   "source": [
    "### å¯è§†åŒ–å¯¹æ¯”\n",
    "\n",
    "ç”Ÿæˆä¸¤ç»„å¯è§†åŒ–å›¾è¡¨ï¼š\n",
    "1. **è´¨é‡æŒ‡æ ‡æŸ±çŠ¶å›¾**ï¼šå¯¹æ¯” 5 ä¸ªè¯„ä¼°ç»´åº¦\n",
    "2. **å“åº”è¯¦ç»†åº¦æŸ±çŠ¶å›¾**ï¼šå¯¹æ¯”å¹³å‡å“åº”é•¿åº¦\n",
    "3. **é€æŸ¥è¯¢å¯¹æ¯”è¡¨æ ¼**ï¼šæ¯æ¡æŸ¥è¯¢çš„è¯¦ç»†è¯„åˆ†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabda3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# 8.3 å¯è§†åŒ–å¯¹æ¯”\n",
    "# âš ï¸ åœ¨ Google Colab ä¸Šè¿è¡Œ\n",
    "# --------------------------------------------\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.dpi'] = 120\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# === å›¾1: è´¨é‡æŒ‡æ ‡å¯¹æ¯” ===\n",
    "ax1 = axes[0]\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "base_scores_avg = [np.mean([s[m] for s in eval_scores[\"base\"]]) for m in metrics]\n",
    "ft_scores_avg = [np.mean([s[m] for s in eval_scores[\"finetuned\"]]) for m in metrics]\n",
    "\n",
    "bars1 = ax1.bar(x - width/2, base_scores_avg, width, label='Base (Zero-shot)', color='#FF6B6B', alpha=0.8)\n",
    "bars2 = ax1.bar(x + width/2, ft_scores_avg, width, label='Fine-tuned (QLoRA)', color='#4ECDC4', alpha=0.8)\n",
    "\n",
    "ax1.set_xlabel('Evaluation Metrics')\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_title('Model Quality Comparison')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(['JSON\\nValid', 'Fields\\nComplete', 'Types\\nCorrect', 'Citation\\nAccuracy', 'Risk\\nAligned'],\n",
    "                     fontsize=8)\n",
    "ax1.set_ylim(0, 1.15)\n",
    "ax1.legend(loc='upper right', fontsize=8)\n",
    "\n",
    "# æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax1.annotate(f'{height:.0%}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom', fontsize=7)\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax1.annotate(f'{height:.0%}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom', fontsize=7)\n",
    "\n",
    "# === å›¾2: å“åº”è¯¦ç»†åº¦å¯¹æ¯” ===\n",
    "ax2 = axes[1]\n",
    "base_lengths = [s[\"response_length\"] for s in eval_scores[\"base\"]]\n",
    "ft_lengths = [s[\"response_length\"] for s in eval_scores[\"finetuned\"]]\n",
    "\n",
    "bp = ax2.boxplot([base_lengths, ft_lengths],\n",
    "                  labels=['Base (Zero-shot)', 'Fine-tuned (QLoRA)'],\n",
    "                  patch_artist=True,\n",
    "                  boxprops=dict(alpha=0.8))\n",
    "bp['boxes'][0].set_facecolor('#FF6B6B')\n",
    "bp['boxes'][1].set_facecolor('#4ECDC4')\n",
    "\n",
    "ax2.set_ylabel('Response Length (chars)')\n",
    "ax2.set_title('Response Detail Comparison')\n",
    "\n",
    "# æ·»åŠ å‡å€¼æ ‡è®°\n",
    "ax2.scatter([1, 2], [np.mean(base_lengths), np.mean(ft_lengths)],\n",
    "            color='black', marker='D', s=50, zorder=5, label='Mean')\n",
    "ax2.legend(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# ä¿å­˜å›¾è¡¨\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "plt.savefig(\"data/model_comparison.png\", dpi=150, bbox_inches='tight')\n",
    "\n",
    "# åŒæ—¶ä¿å­˜åˆ° Google Drive\n",
    "drive_data_dir = '/content/drive/MyDrive/lora_finetune_data'\n",
    "os.makedirs(drive_data_dir, exist_ok=True)\n",
    "plt.savefig(os.path.join(drive_data_dir, 'model_comparison.png'), dpi=150, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n",
    "print(\"\\nå›¾è¡¨å·²ä¿å­˜åˆ° data/model_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bb511d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# 8.4 é€æŸ¥è¯¢å¯¹æ¯”è¡¨æ ¼\n",
    "# âš ï¸ åœ¨ Google Colab ä¸Šè¿è¡Œ\n",
    "# --------------------------------------------\n",
    "\n",
    "print(\"=\" * 90)\n",
    "print(\"é€æŸ¥è¯¢å¯¹æ¯”è¡¨æ ¼\")\n",
    "print(\"=\" * 90)\n",
    "print(f\"{'#':<3} {'æŸ¥è¯¢':<50} {'æ¨¡å‹':<12} {'JSON':>5} {'å­—æ®µ':>5} {'ç±»å‹':>5} {'å¼•ç”¨':>6} {'é£é™©':>5}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for i, q_info in enumerate(eval_queries):\n",
    "    query_short = q_info[\"query\"][:48] + \"..\" if len(q_info[\"query\"]) > 48 else q_info[\"query\"]\n",
    "\n",
    "    base_s = eval_scores[\"base\"][i]\n",
    "    ft_s = eval_scores[\"finetuned\"][i]\n",
    "\n",
    "    print(f\"{i+1:<3} {query_short:<50} {'Base':<12} \"\n",
    "          f\"{'OK' if base_s['json_valid'] else 'FAIL':>5} \"\n",
    "          f\"{'OK' if base_s['fields_complete'] else 'FAIL':>5} \"\n",
    "          f\"{'OK' if base_s['types_correct'] else 'FAIL':>5} \"\n",
    "          f\"{base_s['citation_accuracy']:>5.0%} \"\n",
    "          f\"{'OK' if base_s['risk_aligned'] else 'FAIL':>5}\")\n",
    "\n",
    "    print(f\"{'':3} {'':50} {'LoRA':<12} \"\n",
    "          f\"{'OK' if ft_s['json_valid'] else 'FAIL':>5} \"\n",
    "          f\"{'OK' if ft_s['fields_complete'] else 'FAIL':>5} \"\n",
    "          f\"{'OK' if ft_s['types_correct'] else 'FAIL':>5} \"\n",
    "          f\"{ft_s['citation_accuracy']:>5.0%} \"\n",
    "          f\"{'OK' if ft_s['risk_aligned'] else 'FAIL':>5}\")\n",
    "    print()\n",
    "\n",
    "# æœ€ç»ˆæ€»ç»“\n",
    "print(\"=\" * 60)\n",
    "print(\"Phase 8 è¯„ä¼°å®Œæˆ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "base_overall = np.mean([np.mean([s[m] for m in metrics]) for s in eval_scores[\"base\"]])\n",
    "ft_overall = np.mean([np.mean([s[m] for m in metrics]) for s in eval_scores[\"finetuned\"]])\n",
    "\n",
    "print(f\"\\n  Base æ¨¡å‹ç»¼åˆå¾—åˆ†:    {base_overall:.1%}\")\n",
    "print(f\"  å¾®è°ƒæ¨¡å‹ç»¼åˆå¾—åˆ†:    {ft_overall:.1%}\")\n",
    "print(f\"  ç»¼åˆæå‡:            {ft_overall - base_overall:+.1%}\")\n",
    "\n",
    "print(f\"\\né¡¹ç›®å®Œæˆ:\")\n",
    "print(f\"  Phase 1-4: RAG ç³»ç»Ÿ (Qwen2.5-7B-Instruct, å¼€æº)\")\n",
    "print(f\"  Phase 5:   è®­ç»ƒæ•°æ®å‡†å¤‡ (Colab, å…è´¹)\")\n",
    "print(f\"  Phase 6:   QLoRA å¾®è°ƒ (Colab, å…è´¹)\")\n",
    "print(f\"  Phase 7:   å¼€æºæ¨¡å‹é›†æˆ (Colab, å…è´¹)\")\n",
    "print(f\"  Phase 8:   è¯„ä¼°ä¸å¯¹æ¯” (Colab, å…è´¹)\")\n",
    "print(f\"\\nå…¨éƒ¨é˜¶æ®µå‡å·²å®Œæˆï¼Œå…¨æµç¨‹ä½¿ç”¨å¼€æºæ¨¡å‹ï¼Œé›¶æˆæœ¬è¿è¡Œ âœ“\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}